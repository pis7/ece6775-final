
****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019
  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019
    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.

source /opt/xilinx/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace
INFO: [HLS 200-10] Running '/opt/xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'
INFO: [HLS 200-10] For user 'pis7' on host 'en-ec-zhang-22.coecis.cornell.edu' (Linux_x86_64 version 4.18.0-553.27.1.el8_10.x86_64) on Thu Nov 28 07:54:15 EST 2024
INFO: [HLS 200-10] On os "Red Hat Enterprise Linux release 8.10 (Ootpa)"
INFO: [HLS 200-10] In directory '/home/pis7/ece6775-final/ecelinux'
Sourcing Tcl script 'run.tcl'
INFO: [HLS 200-10] Opening and resetting project '/home/pis7/ece6775-final/ecelinux/attention.prj'.
INFO: [HLS 200-10] Adding design file 'attention.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'attention_test.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'data_long' to the project
INFO: [HLS 200-10] Opening and resetting solution '/home/pis7/ece6775-final/ecelinux/attention.prj/solution1'.
INFO: [HLS 200-10] Cleaning up the solution database.
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294317 ; free virtual = 370702
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294317 ; free virtual = 370702
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:54 ; elapsed = 00:00:59 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294235 ; free virtual = 370633
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:55 ; elapsed = 00:00:59 . Memory (MB): peak = 1235.207 ; gain = 597.184 ; free physical = 294186 ; free virtual = 370591
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:290) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:238) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:308) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:314) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:239) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:306:10) to (./layer.h:305:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:132:28) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:238:8) to (./layer.h:237:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:59 ; elapsed = 00:01:05 . Memory (MB): peak = 1299.219 ; gain = 661.195 ; free physical = 294104 ; free virtual = 370518
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:251:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:302:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:237:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:250:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:271:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:271:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:253:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:02 ; elapsed = 00:01:08 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293981 ; free virtual = 370397
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 68.27 seconds; current allocated memory: 442.053 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 444.647 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 445.571 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 446.096 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 447.595 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 449.234 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 449.604 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 449.669 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 450.455 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 451.636 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 452.094 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 452.466 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 453.694 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 455.441 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 455.875 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 456.258 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 456.463 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 456.757 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 457.210 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 457.853 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 458.393 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 459.293 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 459.862 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 460.537 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 461.395 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.43 seconds; current allocated memory: 465.439 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.4 seconds; current allocated memory: 466.762 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 467.369 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 472.747 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.96 seconds; current allocated memory: 483.884 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 488.930 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 495.824 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 497.927 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 502.777 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 509.199 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.22 seconds; current allocated memory: 517.184 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 518.863 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 521.233 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 525.486 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 529.371 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.91 seconds; current allocated memory: 539.222 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.48 seconds; current allocated memory: 546.697 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40ndEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40shbi_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigibs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigibs_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighmb6_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cachevdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cachevdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cachewdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cachewdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cachexdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cachexdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cacheyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cacheyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cachezec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cachezec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cacheAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cacheAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cacheBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cacheBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cacheCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cacheCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighFfa_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighGfk_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighHfu_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_IfE_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheLf8_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cachePgM_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_proj_Thq_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_output_0_ram (RAM)' using distributed RAMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:01:28 ; elapsed = 00:01:50 . Memory (MB): peak = 1616.148 ; gain = 978.125 ; free physical = 293798 ; free virtual = 370262
INFO: [VHDL 208-304] Generating VHDL RTL for dut.
INFO: [VLOG 209-307] Generating Verilog RTL for dut.
INFO: [HLS 200-112] Total elapsed time: 110.4 seconds; peak allocated memory: 546.697 MB.
INFO: [Common 17-206] Exiting vivado_hls at Thu Nov 28 07:56:05 2024...
