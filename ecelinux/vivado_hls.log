
****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019
  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019
    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.

source /opt/xilinx/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace
INFO: [HLS 200-10] Running '/opt/xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'
INFO: [HLS 200-10] For user 'pis7' on host 'en-ec-zhang-22.coecis.cornell.edu' (Linux_x86_64 version 4.18.0-553.27.1.el8_10.x86_64) on Sun Nov 24 10:24:39 EST 2024
INFO: [HLS 200-10] On os "Red Hat Enterprise Linux release 8.10 (Ootpa)"
INFO: [HLS 200-10] In directory '/home/pis7/ece6775-final/ecelinux'
Sourcing Tcl script 'run.tcl'
INFO: [HLS 200-10] Opening and resetting project '/home/pis7/ece6775-final/ecelinux/attention.prj'.
INFO: [HLS 200-10] Adding design file 'attention.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'attention_test.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'data_short' to the project
INFO: [HLS 200-10] Opening and resetting solution '/home/pis7/ece6775-final/ecelinux/attention.prj/solution1'.
INFO: [HLS 200-10] Cleaning up the solution database.
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
   Compiling ../../../../attention_test.cpp in debug mode
   Compiling ../../../../attention.cpp in debug mode
   Generating csim.exe
0.0261049 -0.0298078 0.0107381 -0.0261049 -0.0362875 -0.00888658 -0.000555277 -0.0318441 0.0605412 -0.0396202 0.00907183 0.0127747 0.0127747 0.00370264 -0.0348065 0.0179586 
attention           :      1 calls;   1.750 msecs total time
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:21 ; elapsed = 00:00:25 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308732 ; free virtual = 381958
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:21 ; elapsed = 00:00:25 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308732 ; free virtual = 381958
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:24) in function 'void init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:22 ; elapsed = 00:00:26 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308703 ; free virtual = 381932
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>122' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_exp' into 'softmax<1, 4, 3>' (./layer.h:323) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
WARNING: [SYNCHK 200-120] ./layer.h:87: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 1 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:22 ; elapsed = 00:00:27 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308701 ; free virtual = 381931
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:286) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:200) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:212) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'attention_exp' (./layer.h:300:61).
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:222) in function 'attention<2, 1, 16, 16, 4, 4, 2>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:248) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:154) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTENTION_EXP_LOOP_1' (./layer.h:300) in function 'attention_exp' completely with a factor of 5.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:223) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:320) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:327) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:249) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:155) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:128) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:101) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:102) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:193) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:235) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:236) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:193) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:147) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:155) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:160) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:169) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:173) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:196) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:197) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:117) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:218) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:134:64) to (./layer.h:134:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:200:69) to (./layer.h:200:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:25 ; elapsed = 00:00:30 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308664 ; free virtual = 381897
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:247:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:316:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:179:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:246:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:178:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:58:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:153:41)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:231:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4, 2>' to 'attention' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:198:78)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:249:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:92:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:137:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:160:36)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:161:41)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:21)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:235:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:234:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:223:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:27 ; elapsed = 00:00:32 . Memory (MB): peak = 1296.152 ; gain = 658.125 ; free physical = 308544 ; free virtual = 381777
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 32.23 seconds; current allocated memory: 303.487 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 304.282 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 305.001 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 305.836 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 305.963 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 306.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('output_0_V_load_3', ./layer.h:161) on array 'output_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 307.464 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 309.054 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 309.496 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 309.871 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 311.114 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 312.871 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 313.292 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 313.664 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('output_1_V_addr_2_write_ln249', ./layer.h:249) of variable 'input_2_V_load', ./layer.h:249 on array 'output_1_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_1_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 313.928 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 314.217 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 314.677 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 315.316 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'attention_exp'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 11.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 315.626 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 316.049 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 12, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 13, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 316.785 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 318.339 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 318.954 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 319.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('attn_output_2D_0_V_2_write_ln223', attention.cpp:223) of variable 'tmp_6', attention.cpp:223 on array 'attn_output_2D[0].V', attention.cpp:218 due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'attn_output_2D_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 320.520 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 323.099 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 323.738 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 324.064 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_53s_32s_53_57_seq_1' to 'dut_sdiv_53s_32s_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_46ns_32s_32_50_seq_1' to 'dut_sdiv_46ns_32scud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_64s_32s_76_5_1' to 'dut_mul_64s_32s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_64s_32s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_46ns_32scud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_53s_32s_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 326.315 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_51s_31ns_32_55_seq_1' to 'dut_udiv_51s_31nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_51s_31nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 329.912 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 333.185 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_76ns_59s_32_80_1' to 'dut_sdiv_76ns_59sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_76ns_59sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 336.231 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 344.004 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 350.469 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 358.512 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 360.304 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_32_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 362.682 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention_exp'.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 365.880 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_54ns_32s_32_58_seq_1' to 'dut_sdiv_54ns_32sibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_54ns_32sibs': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 370.047 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 375.478 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_0' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_1' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_2' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_3' to 'attention_q_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_0' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_1' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_2' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_3' to 'attention_k_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_0' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_1' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_2' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_3' to 'attention_v_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_0' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_1' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_2' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_3' to 'attention_o_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_OgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_PgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizRg6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.17 seconds; current allocated memory: 384.406 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 391.135 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_53s_32s_bkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_46ns_32scud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_64s_32s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_51s_31nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_76ns_59sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_54ns_32sibs_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighmb6_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighAem_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_quantizBew_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_CeG_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheFfa_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cacheJfO_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:42 ; elapsed = 00:00:57 . Memory (MB): peak = 1424.152 ; gain = 786.125 ; free physical = 308396 ; free virtual = 381671
INFO: [VHDL 208-304] Generating VHDL RTL for dut.
INFO: [VLOG 209-307] Generating Verilog RTL for dut.
INFO: [COSIM 212-47] Using XSIM for RTL simulation.
INFO: [COSIM 212-14] Instrumenting C test bench ...
   Build using "/opt/xilinx/Vivado/2019.2/tps/lnx64/gcc-6.2.0/bin/g++"
   Compiling attention.cpp_pre.cpp.tb.cpp
   Compiling attention_test.cpp_pre.cpp.tb.cpp
   Compiling apatb_dut.cpp
   Generating cosim.tv.exe
INFO: [COSIM 212-302] Starting C TB testing ... 
0.0261049 -0.0298078 0.0107381 -0.0261049 -0.0362875 -0.00888658 -0.000555277 -0.0318441 0.0605412 -0.0396202 0.00907183 0.0127747 0.0127747 0.00370264 -0.0348065 0.0179586 
attention           :      1 calls;   2.769 msecs total time
INFO: [COSIM 212-333] Generating C post check test bench ...
INFO: [COSIM 212-12] Generating RTL test bench ...
INFO: [COSIM 212-1] *** C/RTL co-simulation file generation completed. ***
INFO: [COSIM 212-323] Starting verilog simulation. 
INFO: [COSIM 212-15] Starting XSIM ...
INFO: [XSIM 43-3496] Using init file passed via -initfile option "/opt/xilinx/Vivado/2019.2/data/xsim/ip/xsim_ip.ini".
Vivado Simulator 2019.2
Copyright 1986-1999, 2001-2019 Xilinx, Inc. All Rights Reserved.
Running: /opt/xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/xelab xil_defaultlib.apatb_dut_top glbl -prj dut.prj -L smartconnect_v1_0 -L axi_protocol_checker_v1_1_12 -L axi_protocol_checker_v1_1_13 -L axis_protocol_checker_v1_1_11 -L axis_protocol_checker_v1_1_12 -L xil_defaultlib -L unisims_ver -L xpm --initfile /opt/xilinx/Vivado/2019.2/data/xsim/ip/xsim_ip.ini --lib ieee_proposed=./ieee_proposed -s dut 
Multi-threading is on. Using 62 slave threads.
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/glbl.v" into library work
INFO: [VRFC 10-311] analyzing module glbl
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut.autotb.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module apatb_dut_top
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/AESL_autofifo_strm_in_V_V.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module AESL_autofifo_strm_in_V_V
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/AESL_autofifo_strm_out_V_V.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module AESL_autofifo_strm_out_V_V
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/rms_norm_16_s.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module rms_norm_16_s
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/quantize_activation.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module quantize_activation
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/init_2d_mem.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module init_2d_mem
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/linear_forward_no_mu.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module linear_forward_no_mu
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/reshape_2D_to_3D.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module reshape_2D_to_3D
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/apply_rotary_pos_emb.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module apply_rotary_pos_emb
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/cache_update.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module cache_update
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/transpose_last_two_d.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module transpose_last_two_d
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/GEMM_3D_float.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module GEMM_3D_float
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_exp.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_exp
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/softmax_1_4_3_s.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module softmax_1_4_3_s
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/GEMM_3D_float_1.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module GEMM_3D_float_1
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_sdiv_53s_32s_bkb.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_sdiv_53s_32s_bkb_div_u
INFO: [VRFC 10-311] analyzing module dut_sdiv_53s_32s_bkb_div
INFO: [VRFC 10-311] analyzing module dut_sdiv_53s_32s_bkb
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_sdiv_46ns_32scud.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_sdiv_46ns_32scud_div_u
INFO: [VRFC 10-311] analyzing module dut_sdiv_46ns_32scud_div
INFO: [VRFC 10-311] analyzing module dut_sdiv_46ns_32scud
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_mul_64s_32s_7dEe.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_mul_64s_32s_7dEe_MulnS_0
INFO: [VRFC 10-311] analyzing module dut_mul_64s_32s_7dEe
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_udiv_51s_31nseOg.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_udiv_51s_31nseOg_div_u
INFO: [VRFC 10-311] analyzing module dut_udiv_51s_31nseOg_div
INFO: [VRFC 10-311] analyzing module dut_udiv_51s_31nseOg
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_sdiv_76ns_59sfYi.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_sdiv_76ns_59sfYi_div_u
INFO: [VRFC 10-311] analyzing module dut_sdiv_76ns_59sfYi_div
INFO: [VRFC 10-311] analyzing module dut_sdiv_76ns_59sfYi
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_mux_42_32_1_1.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_mux_42_32_1_1
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_mux_165_32_1_1.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_mux_165_32_1_1
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/apply_rotary_pos_g8j.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module apply_rotary_pos_g8j_rom
INFO: [VRFC 10-311] analyzing module apply_rotary_pos_g8j
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/apply_rotary_pos_hbi.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module apply_rotary_pos_hbi_rom
INFO: [VRFC 10-311] analyzing module apply_rotary_pos_hbi
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_mux_32_32_1_1.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_mux_32_32_1_1
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_mux_124_32_1_1.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_mux_124_32_1_1
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_sdiv_54ns_32sibs.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_sdiv_54ns_32sibs_div_u
INFO: [VRFC 10-311] analyzing module dut_sdiv_54ns_32sibs_div
INFO: [VRFC 10-311] analyzing module dut_sdiv_54ns_32sibs
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_ln_weigjbC.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_ln_weigjbC_rom
INFO: [VRFC 10-311] analyzing module attention_ln_weigjbC
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_q_weighkbM.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_q_weighkbM_rom
INFO: [VRFC 10-311] analyzing module attention_q_weighkbM
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_q_weighlbW.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_q_weighlbW_rom
INFO: [VRFC 10-311] analyzing module attention_q_weighlbW
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_q_weighmb6.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_q_weighmb6_rom
INFO: [VRFC 10-311] analyzing module attention_q_weighmb6
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_q_weighncg.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_q_weighncg_rom
INFO: [VRFC 10-311] analyzing module attention_q_weighncg
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_k_weighocq.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_k_weighocq_rom
INFO: [VRFC 10-311] analyzing module attention_k_weighocq
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_k_weighpcA.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_k_weighpcA_rom
INFO: [VRFC 10-311] analyzing module attention_k_weighpcA
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_k_weighqcK.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_k_weighqcK_rom
INFO: [VRFC 10-311] analyzing module attention_k_weighqcK
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_k_weighrcU.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_k_weighrcU_rom
INFO: [VRFC 10-311] analyzing module attention_k_weighrcU
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_v_weighsc4.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_v_weighsc4_rom
INFO: [VRFC 10-311] analyzing module attention_v_weighsc4
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_v_weightde.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_v_weightde_rom
INFO: [VRFC 10-311] analyzing module attention_v_weightde
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_v_weighudo.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_v_weighudo_rom
INFO: [VRFC 10-311] analyzing module attention_v_weighudo
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_v_weighvdy.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_v_weighvdy_rom
INFO: [VRFC 10-311] analyzing module attention_v_weighvdy
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_k_cache_V.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_k_cache_V_rom
INFO: [VRFC 10-311] analyzing module attention_k_cache_V
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_v_cache_V.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_v_cache_V_rom
INFO: [VRFC 10-311] analyzing module attention_v_cache_V
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_ln_weigwdI.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_ln_weigwdI_rom
INFO: [VRFC 10-311] analyzing module attention_ln_weigwdI
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_o_weighxdS.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_o_weighxdS_rom
INFO: [VRFC 10-311] analyzing module attention_o_weighxdS
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_o_weighyd2.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_o_weighyd2_rom
INFO: [VRFC 10-311] analyzing module attention_o_weighyd2
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_o_weighzec.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_o_weighzec_rom
INFO: [VRFC 10-311] analyzing module attention_o_weighzec
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_o_weighAem.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_o_weighAem_rom
INFO: [VRFC 10-311] analyzing module attention_o_weighAem
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_quantizBew.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_quantizBew_ram
INFO: [VRFC 10-311] analyzing module attention_quantizBew
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_q_proj_CeG.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_q_proj_CeG_ram
INFO: [VRFC 10-311] analyzing module attention_q_proj_CeG
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_k_cacheFfa.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_k_cacheFfa_ram
INFO: [VRFC 10-311] analyzing module attention_k_cacheFfa
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/attention_v_cacheJfO.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module attention_v_cacheJfO_ram
INFO: [VRFC 10-311] analyzing module attention_v_cacheJfO
INFO: [VRFC 10-2263] Analyzing SystemVerilog file "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut_input_0_V.v" into library xil_defaultlib
INFO: [VRFC 10-311] analyzing module dut_input_0_V_ram
INFO: [VRFC 10-311] analyzing module dut_input_0_V
Starting static elaboration
Pass Through NonSizing Optimizer
Completed static elaboration
Starting simulation data flow analysis
Completed simulation data flow analysis
Time Resolution for simulation is 1ps
Compiling module xil_defaultlib.dut_input_0_V_ram
Compiling module xil_defaultlib.dut_input_0_V(DataWidth=32,Addre...
Compiling module xil_defaultlib.attention_ln_weigjbC_rom
Compiling module xil_defaultlib.attention_ln_weigjbC(DataWidth=3...
Compiling module xil_defaultlib.attention_q_weighkbM_rom
Compiling module xil_defaultlib.attention_q_weighkbM(DataWidth=8...
Compiling module xil_defaultlib.attention_q_weighlbW_rom
Compiling module xil_defaultlib.attention_q_weighlbW(DataWidth=8...
Compiling module xil_defaultlib.attention_q_weighmb6_rom
Compiling module xil_defaultlib.attention_q_weighmb6(DataWidth=8...
Compiling module xil_defaultlib.attention_q_weighncg_rom
Compiling module xil_defaultlib.attention_q_weighncg(DataWidth=8...
Compiling module xil_defaultlib.attention_k_weighocq_rom
Compiling module xil_defaultlib.attention_k_weighocq(DataWidth=8...
Compiling module xil_defaultlib.attention_k_weighpcA_rom
Compiling module xil_defaultlib.attention_k_weighpcA(DataWidth=8...
Compiling module xil_defaultlib.attention_k_weighqcK_rom
Compiling module xil_defaultlib.attention_k_weighqcK(DataWidth=8...
Compiling module xil_defaultlib.attention_k_weighrcU_rom
Compiling module xil_defaultlib.attention_k_weighrcU(DataWidth=8...
Compiling module xil_defaultlib.attention_v_weighsc4_rom
Compiling module xil_defaultlib.attention_v_weighsc4(DataWidth=8...
Compiling module xil_defaultlib.attention_v_weightde_rom
Compiling module xil_defaultlib.attention_v_weightde(DataWidth=8...
Compiling module xil_defaultlib.attention_v_weighudo_rom
Compiling module xil_defaultlib.attention_v_weighudo(DataWidth=8...
Compiling module xil_defaultlib.attention_v_weighvdy_rom
Compiling module xil_defaultlib.attention_v_weighvdy(DataWidth=8...
Compiling module xil_defaultlib.attention_k_cache_V_rom
Compiling module xil_defaultlib.attention_k_cache_V(DataWidth=32...
Compiling module xil_defaultlib.attention_v_cache_V_rom
Compiling module xil_defaultlib.attention_v_cache_V(DataWidth=32...
Compiling module xil_defaultlib.attention_ln_weigwdI_rom
Compiling module xil_defaultlib.attention_ln_weigwdI(DataWidth=3...
Compiling module xil_defaultlib.attention_o_weighxdS_rom
Compiling module xil_defaultlib.attention_o_weighxdS(DataWidth=8...
Compiling module xil_defaultlib.attention_o_weighyd2_rom
Compiling module xil_defaultlib.attention_o_weighyd2(DataWidth=8...
Compiling module xil_defaultlib.attention_o_weighzec_rom
Compiling module xil_defaultlib.attention_o_weighzec(DataWidth=8...
Compiling module xil_defaultlib.attention_o_weighAem_rom
Compiling module xil_defaultlib.attention_o_weighAem(DataWidth=8...
Compiling module xil_defaultlib.attention_quantizBew_ram
Compiling module xil_defaultlib.attention_quantizBew(DataWidth=8...
Compiling module xil_defaultlib.attention_q_proj_CeG_ram
Compiling module xil_defaultlib.attention_q_proj_CeG(DataWidth=3...
Compiling module xil_defaultlib.attention_k_cacheFfa_ram
Compiling module xil_defaultlib.attention_k_cacheFfa(DataWidth=3...
Compiling module xil_defaultlib.attention_v_cacheJfO_ram
Compiling module xil_defaultlib.attention_v_cacheJfO(DataWidth=3...
Compiling module xil_defaultlib.dut_sdiv_76ns_59sfYi_div_u(in0_W...
Compiling module xil_defaultlib.dut_sdiv_76ns_59sfYi_div(in0_WID...
Compiling module xil_defaultlib.dut_sdiv_76ns_59sfYi(ID=1,NUM_ST...
Compiling module xil_defaultlib.linear_forward_no_mu
Compiling module xil_defaultlib.attention_exp
Compiling module xil_defaultlib.dut_mux_42_32_1_1(ID=1,din4_WIDT...
Compiling module xil_defaultlib.dut_mux_124_32_1_1(ID=1,din12_WI...
Compiling module xil_defaultlib.dut_sdiv_54ns_32sibs_div_u(in0_W...
Compiling module xil_defaultlib.dut_sdiv_54ns_32sibs_div(in0_WID...
Compiling module xil_defaultlib.dut_sdiv_54ns_32sibs(ID=1,NUM_ST...
Compiling module xil_defaultlib.softmax_1_4_3_s
Compiling module xil_defaultlib.apply_rotary_pos_g8j_rom
Compiling module xil_defaultlib.apply_rotary_pos_g8j(DataWidth=2...
Compiling module xil_defaultlib.apply_rotary_pos_hbi_rom
Compiling module xil_defaultlib.apply_rotary_pos_hbi(DataWidth=2...
Compiling module xil_defaultlib.dut_mux_165_32_1_1(ID=1,din16_WI...
Compiling module xil_defaultlib.apply_rotary_pos_emb
Compiling module xil_defaultlib.dut_sdiv_53s_32s_bkb_div_u(in0_W...
Compiling module xil_defaultlib.dut_sdiv_53s_32s_bkb_div(in0_WID...
Compiling module xil_defaultlib.dut_sdiv_53s_32s_bkb(ID=1,NUM_ST...
Compiling module xil_defaultlib.dut_sdiv_46ns_32scud_div_u(in0_W...
Compiling module xil_defaultlib.dut_sdiv_46ns_32scud_div(in0_WID...
Compiling module xil_defaultlib.dut_sdiv_46ns_32scud(ID=1,NUM_ST...
Compiling module xil_defaultlib.dut_mul_64s_32s_7dEe_MulnS_0
Compiling module xil_defaultlib.dut_mul_64s_32s_7dEe(ID=1,NUM_ST...
Compiling module xil_defaultlib.rms_norm_16_s
Compiling module xil_defaultlib.dut_udiv_51s_31nseOg_div_u(in0_W...
Compiling module xil_defaultlib.dut_udiv_51s_31nseOg_div(in0_WID...
Compiling module xil_defaultlib.dut_udiv_51s_31nseOg(ID=1,NUM_ST...
Compiling module xil_defaultlib.quantize_activation
Compiling module xil_defaultlib.dut_mux_32_32_1_1(ID=1,din3_WIDT...
Compiling module xil_defaultlib.GEMM_3D_float
Compiling module xil_defaultlib.GEMM_3D_float_1
Compiling module xil_defaultlib.cache_update
Compiling module xil_defaultlib.reshape_2D_to_3D
Compiling module xil_defaultlib.transpose_last_two_d
Compiling module xil_defaultlib.init_2d_mem
Compiling module xil_defaultlib.attention
Compiling module xil_defaultlib.dut
Compiling module xil_defaultlib.AESL_autofifo_strm_in_V_V
Compiling module xil_defaultlib.AESL_autofifo_strm_out_V_V
Compiling module xil_defaultlib.apatb_dut_top
Compiling module work.glbl
Built simulation snapshot dut

****** Webtalk v2019.2 (64-bit)
  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019
  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019
    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.

source /home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/xsim.dir/dut/webtalk/xsim_webtalk.tcl -notrace
INFO: [Common 17-206] Exiting Webtalk at Sun Nov 24 10:26:01 2024...

****** xsim v2019.2 (64-bit)
  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019
  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019
    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.

source xsim.dir/dut/xsim_script.tcl
# xsim {dut} -autoloadwcfg -tclbatch {dut.tcl}
Vivado Simulator 2019.2
Time resolution is 1 ps
source dut.tcl
## run all
////////////////////////////////////////////////////////////////////////////////////
// Inter-Transaction Progress: Completed Transaction / Total Transaction
// Intra-Transaction Progress: Measured Latency / Latency Estimation * 100%
//
// RTL Simulation : "Inter-Transaction Progress" ["Intra-Transaction Progress"] @ "Simulation Time"
////////////////////////////////////////////////////////////////////////////////////
// RTL Simulation : 0 / 1 [0.00%] @ "125000"
// RTL Simulation : 1 / 1 [100.00%] @ "35485000"
////////////////////////////////////////////////////////////////////////////////////
$finish called at time : 35525 ns : File "/home/pis7/ece6775-final/ecelinux/attention.prj/solution1/sim/verilog/dut.autotb.v" Line 271
## quit
INFO: [Common 17-206] Exiting xsim at Sun Nov 24 10:26:09 2024...
INFO: [COSIM 212-316] Starting C post checking ...
0.0261049 -0.0298078 0.0107381 -0.0261049 -0.0362875 -0.00888658 -0.000555277 -0.0318441 0.0605412 -0.0396202 0.00907183 0.0127747 0.0127747 0.00370264 -0.0348065 0.0179586 
attention           :      1 calls;   1.648 msecs total time
INFO: [COSIM 212-1000] *** C/RTL co-simulation finished: PASS ***
INFO: [COSIM 212-211] II is measurable only when transaction number is greater than 1 in RTL simulation. Otherwise, they will be marked as all NA. If user wants to calculate them, please make sure there are at least 2 transactions in RTL simulation.
INFO: [HLS 200-112] Total elapsed time: 90.67 seconds; peak allocated memory: 391.135 MB.
INFO: [Common 17-206] Exiting vivado_hls at Sun Nov 24 10:26:09 2024...
