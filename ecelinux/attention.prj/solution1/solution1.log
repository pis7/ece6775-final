==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:03:14 ; elapsed = 00:03:24 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 306405 ; free virtual = 380934
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:03:14 ; elapsed = 00:03:24 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 306405 ; free virtual = 380934
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:03:19 ; elapsed = 00:03:29 . Memory (MB): peak = 1245.090 ; gain = 607.066 ; free physical = 306183 ; free virtual = 380762
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<1536>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 1536>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 1536>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 1536>113' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 16, 6>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 16, 6>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 1536, 1536, 16, 96>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 1536, 1536, 16, 96>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:03:23 ; elapsed = 00:03:33 . Memory (MB): peak = 1509.180 ; gain = 871.156 ; free physical = 306001 ; free virtual = 380601
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<5, 1, 1536, 1536, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 1536, 1536, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<5, 1, 1536, 1536, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<5, 1, 1536, 1536, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<5, 1, 1536, 1536, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<5, 1, 1536, 1536, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<16, 1, 6, 16, 6, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 16, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<16, 1, 96, 16, 96, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 16, 96>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 1536, 1536>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 1536, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 1536>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 1536, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<5, 1, 1536, 1536, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 1536, 1536, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<5, 1, 1536, 1536, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<5, 1, 1536, 1536, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<5, 1, 1536, 1536, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<5, 1, 1536, 1536, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<16, 1, 6, 16, 6, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 16, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<16, 1, 96, 16, 96, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 16, 96>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 1536, 1536>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 1536, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 1536>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 1536, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:240) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:241) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<1536>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 1536>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 1536>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 1536>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 16, 6>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 16, 6>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 1536, ap_int<8> >' into 'attention<5, 1, 1536, 1536, 16, 96>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 1536, 1536, 16, 96>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 1536, 1536, 16, 96>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:94:5) to (./layer.h:93:43) in function 'rms_norm<1536>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 1536>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:237:21) to (./layer.h:237:21) in function 'cache_update<16, 5, 96>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:215:65) to (./layer.h:215:59) in function 'apply_rotary_pos_emb<1, 16, 96>'... converting 37 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:03:28 ; elapsed = 00:03:39 . Memory (MB): peak = 1520.441 ; gain = 882.418 ; free physical = 305871 ; free virtual = 380504
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 16, 96>' to 'transpose_last_two_d' (./layer.h:249:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 16, 96>' to 'reshape_2D_to_3D' (./layer.h:181:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 1536>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 1536, 1536>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 1536, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<16, 5, 96>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 1536, 1536, 16, 96>' to 'attention' (attention.cpp:38:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 16, 96>' to 'apply_rotary_pos_emb' (./layer.h:199:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<16, 1, 96, 16, 96, 6>' to 'GEMM_3D_float' (./layer.h:270:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<16, 1, 6, 16, 6, 96>' to 'GEMM_3D_float.1' (./layer.h:270:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:327:35)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:331:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:183:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:188:9)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:204:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:205:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:206:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:207:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:217:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:219:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:275:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:275:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:03:36 ; elapsed = 00:03:47 . Memory (MB): peak = 1893.180 ; gain = 1255.156 ; free physical = 305562 ; free virtual = 380207
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<1536>' to 'rms_norm_1536_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 16, 6>' to 'softmax_1_16_6_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 227.76 seconds; current allocated memory: 867.215 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 869.877 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_1536_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 870.980 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 871.680 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 872.404 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 872.724 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 872.835 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 872.904 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 873.278 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 873.731 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 873.906 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 874.059 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 874.616 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 875.404 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 875.824 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 876.274 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 876.498 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 876.718 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 876.938 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 877.184 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 877.660 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 878.222 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_16_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 878.599 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 879.081 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 879.369 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 879.617 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 880.077 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.28 seconds; current allocated memory: 882.838 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 883.765 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 884.225 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 889.258 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_1536_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_fpext_32ns_64_2_1' to 'dut_fpext_32ns_64bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_cud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsdEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7eOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_1536_s'.
INFO: [HLS 200-111]  Elapsed time: 2.07 seconds; current allocated memory: 900.683 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40nfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40nfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 903.675 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 904.906 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61sg8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sg8j': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 906.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 907.960 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 2 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 909.730 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 913.778 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 916.041 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 917.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nsocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nspcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nspcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 919.396 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_16_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40sqcK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40sqcK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_16_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 922.606 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 924.413 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in' to 'attention_ln_weigrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_tde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_udo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_vdy' due to the length limit 20
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:26 ; elapsed = 00:01:32 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305001 ; free virtual = 381499
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:26 ; elapsed = 00:01:32 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305000 ; free virtual = 381499
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:31 ; elapsed = 00:01:37 . Memory (MB): peak = 1240.953 ; gain = 602.930 ; free physical = 304788 ; free virtual = 381320
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>113' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:32 ; elapsed = 00:01:38 . Memory (MB): peak = 1366.527 ; gain = 728.504 ; free physical = 304695 ; free virtual = 381239
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:94:6) to (./layer.h:93:43) in function 'rms_norm<16>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:237:21) to (./layer.h:237:21) in function 'cache_update<4, 2, 4>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:215:65) to (./layer.h:219:52) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 36 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:219:52) to (./layer.h:215:59) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 27 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:37 ; elapsed = 00:01:44 . Memory (MB): peak = 1430.965 ; gain = 792.941 ; free physical = 304536 ; free virtual = 381101
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:40 ; elapsed = 00:01:48 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 304374 ; free virtual = 380945
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 108.07 seconds; current allocated memory: 669.257 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 671.849 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 12.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 672.937 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 673.667 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 674.710 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 675.569 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 675.711 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 675.779 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 677.141 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 678.686 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 679.113 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 679.486 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 681.097 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 683.416 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 684.041 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 684.582 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 684.845 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 685.137 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 685.631 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 686.268 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 686.720 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 687.276 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 13, Final II = 13, Depth = 16.
WARNING: [SCHED 204-21] Estimated clock period (19.76ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
WARNING: [SCHED 204-21] The critical path in module 'softmax_1_4_3_s' consists of the following:
	'call' operation ('agg_result_V_i', /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_math.h:1137->./layer.h:327) to 'exp<41, 25>' [102]  (7.9 ns)
	multiplexor before 'phi' operation ('empty_93', ./layer.h:327) with incoming values : ('p_read_1', ./layer.h:320) ('trunc_ln703_3', ./layer.h:327) ('input_V211_1', ./layer.h:320) [142]  (1.77 ns)
	'phi' operation ('empty_93', ./layer.h:327) with incoming values : ('p_read_1', ./layer.h:320) ('trunc_ln703_3', ./layer.h:327) ('input_V211_1', ./layer.h:320) [142]  (0 ns)
	'phi' operation ('empty_80', ./layer.h:320) with incoming values : ('p_read_1', ./layer.h:320) ('trunc_ln703_3', ./layer.h:327) ('input_V211_1', ./layer.h:320) [76]  (0 ns)
	'mux' operation ('tmp_7', ./layer.h:327) [99]  (2.78 ns)
	'sub' operation ('sub_ln1193', ./layer.h:327) [101]  (2.88 ns)
	'call' operation ('agg_result_V_i', /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_math.h:1137->./layer.h:327) to 'exp<41, 25>' [102]  (4.43 ns)
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 688.043 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 689.678 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 690.274 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 690.985 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 691.840 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.37 seconds; current allocated memory: 695.435 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 696.565 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 697.070 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 702.355 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_fpext_32ns_64_2_1' to 'dut_fpext_32ns_64bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.84 seconds; current allocated memory: 713.826 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 717.521 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 720.649 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 723.681 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.95 seconds; current allocated memory: 731.361 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 738.537 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 748.999 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 751.620 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 753.991 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nslbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 757.551 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40sncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40sncg': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 762.372 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 767.955 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in' to 'attention_ln_weigocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.42 seconds; current allocated memory: 776.861 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:25 ; elapsed = 00:01:32 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305033 ; free virtual = 381531
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:25 ; elapsed = 00:01:32 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305033 ; free virtual = 381531
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:29 ; elapsed = 00:01:37 . Memory (MB): peak = 1240.953 ; gain = 602.930 ; free physical = 304814 ; free virtual = 381346
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>113' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:31 ; elapsed = 00:01:38 . Memory (MB): peak = 1379.559 ; gain = 741.535 ; free physical = 304704 ; free virtual = 381249
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:94:6) to (./layer.h:93:43) in function 'rms_norm<16>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:237:21) to (./layer.h:237:21) in function 'cache_update<4, 2, 4>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:215:65) to (./layer.h:219:52) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 36 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:219:52) to (./layer.h:215:59) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 27 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:36 ; elapsed = 00:01:44 . Memory (MB): peak = 1443.996 ; gain = 805.973 ; free physical = 304550 ; free virtual = 381115
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:39 ; elapsed = 00:01:47 . Memory (MB): peak = 1693.180 ; gain = 1055.156 ; free physical = 304389 ; free virtual = 380961
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 107.98 seconds; current allocated memory: 669.206 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 671.798 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 12.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 672.842 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 673.556 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 674.597 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 675.420 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 675.583 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 675.651 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 677.031 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 678.578 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 679.004 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 679.376 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 680.974 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 683.294 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 683.935 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 684.476 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 684.767 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 685.059 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 685.551 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 686.193 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 686.644 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 687.201 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 687.936 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 689.498 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 690.100 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 690.810 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 691.665 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.31 seconds; current allocated memory: 695.289 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 696.404 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 696.922 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.19 seconds; current allocated memory: 702.180 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_fpext_32ns_64_2_1' to 'dut_fpext_32ns_64bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.82 seconds; current allocated memory: 713.662 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 717.331 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 720.475 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 723.508 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 731.219 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 738.366 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 748.781 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 751.430 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 753.769 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nslbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 757.358 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40sncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40sncg': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 761.907 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 767.424 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in' to 'attention_ln_weigocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.59 seconds; current allocated memory: 776.360 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.9 seconds; current allocated memory: 783.622 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nslbW_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nsmb6_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mjbC_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xkbM_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40sncg_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_rom' using block ROMs.
WARNING: [RTMG 210-274]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:26 ; elapsed = 00:01:33 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305035 ; free virtual = 381533
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:26 ; elapsed = 00:01:33 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305035 ; free virtual = 381533
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:30 ; elapsed = 00:01:38 . Memory (MB): peak = 1240.953 ; gain = 602.930 ; free physical = 304812 ; free virtual = 381344
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>113' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:32 ; elapsed = 00:01:39 . Memory (MB): peak = 1366.527 ; gain = 728.504 ; free physical = 304717 ; free virtual = 381262
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:94:6) to (./layer.h:93:43) in function 'rms_norm<16>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:237:21) to (./layer.h:237:21) in function 'cache_update<4, 2, 4>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:215:65) to (./layer.h:219:52) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 36 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:219:52) to (./layer.h:215:59) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 27 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:37 ; elapsed = 00:01:45 . Memory (MB): peak = 1430.965 ; gain = 792.941 ; free physical = 304562 ; free virtual = 381127
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:40 ; elapsed = 00:01:49 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 304403 ; free virtual = 380975
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 109.38 seconds; current allocated memory: 669.236 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 671.827 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 12.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 672.933 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 673.646 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 674.687 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 675.495 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 675.676 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 675.744 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 677.124 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 678.673 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 679.099 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 679.473 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 681.069 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 683.390 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 684.017 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 684.557 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 684.822 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 685.111 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 685.604 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 686.247 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 686.697 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 687.254 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 13, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 14, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 688.036 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 689.657 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 690.271 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 690.981 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 691.836 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.3 seconds; current allocated memory: 695.451 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 696.565 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 697.074 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.12 seconds; current allocated memory: 702.344 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_fpext_32ns_64_2_1' to 'dut_fpext_32ns_64bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.78 seconds; current allocated memory: 713.862 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 717.523 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 720.652 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 723.702 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 731.364 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 738.541 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 748.990 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 751.609 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 753.981 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nslbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 757.543 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40sncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40sncg': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 762.367 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 767.939 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in' to 'attention_ln_weigocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 776.842 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.93 seconds; current allocated memory: 784.077 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nslbW_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nsmb6_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mjbC_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xkbM_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40sncg_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: ==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:24 ; elapsed = 00:01:30 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305012 ; free virtual = 381510
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:24 ; elapsed = 00:01:30 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305012 ; free virtual = 381510
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:28 ; elapsed = 00:01:35 . Memory (MB): peak = 1296.148 ; gain = 658.125 ; free physical = 304798 ; free virtual = 381329
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>113' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:29 ; elapsed = 00:01:36 . Memory (MB): peak = 1366.527 ; gain = 728.504 ; free physical = 304701 ; free virtual = 381245
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:94:5) to (./layer.h:93:43) in function 'rms_norm<16>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:237:21) to (./layer.h:237:21) in function 'cache_update<4, 2, 4>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:215:65) to (./layer.h:215:59) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 37 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:32 ; elapsed = 00:01:40 . Memory (MB): peak = 1430.965 ; gain = 792.941 ; free physical = 304557 ; free virtual = 381122
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:38:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:199:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:327:35)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:331:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:183:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:188:9)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:204:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:205:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:206:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:207:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:217:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:219:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:275:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:275:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:34 ; elapsed = 00:01:42 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 304405 ; free virtual = 380976
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 102.72 seconds; current allocated memory: 652.523 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 655.164 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 656.216 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 656.861 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 657.570 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 657.886 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 657.996 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 658.064 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 658.428 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 658.905 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 659.034 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 659.165 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 659.690 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 660.474 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 660.863 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 661.261 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 661.462 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 661.644 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 661.818 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 662.072 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 662.524 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 663.063 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 663.439 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 663.933 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 664.205 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 664.416 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 664.809 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.21 seconds; current allocated memory: 667.473 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 668.388 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 668.812 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 673.829 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_fpext_32ns_64_2_1' to 'dut_fpext_32ns_64bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.79 seconds; current allocated memory: 685.165 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 688.147 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 689.349 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 690.651 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 692.248 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 2 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 693.943 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 697.746 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 699.714 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 700.813 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 702.902 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 706.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 707.769 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_sc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_tde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_udo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_vdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_wdI' due to the length limit 20
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:25 ; elapsed = 00:01:32 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305018 ; free virtual = 381517
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:25 ; elapsed = 00:01:32 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 305018 ; free virtual = 381516
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:29 ; elapsed = 00:01:36 . Memory (MB): peak = 1240.957 ; gain = 602.934 ; free physical = 304801 ; free virtual = 381333
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>113' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>113' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:30 ; elapsed = 00:01:37 . Memory (MB): peak = 1366.531 ; gain = 728.508 ; free physical = 304707 ; free virtual = 381251
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:94:6) to (./layer.h:93:43) in function 'rms_norm<16>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:237:21) to (./layer.h:237:21) in function 'cache_update<4, 2, 4>'... converting 10 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:215:65) to (./layer.h:219:52) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 36 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:219:52) to (./layer.h:215:59) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 27 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:36 ; elapsed = 00:01:44 . Memory (MB): peak = 1430.969 ; gain = 792.945 ; free physical = 304560 ; free virtual = 381125
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:39 ; elapsed = 00:01:47 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 304391 ; free virtual = 380963
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 107.66 seconds; current allocated memory: 669.258 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 671.849 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 12.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 672.938 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 673.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 674.709 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 675.569 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 675.712 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 675.780 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 677.144 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 678.691 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 679.117 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 679.489 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 681.120 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 683.424 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 684.048 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 684.589 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 684.850 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 685.143 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 685.636 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 686.275 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 686.726 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 687.282 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 13, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 14, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.95 seconds; current allocated memory: 688.068 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 689.688 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 690.284 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 690.997 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 691.853 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.32 seconds; current allocated memory: 695.444 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 696.574 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 697.082 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 702.338 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_fpext_32ns_64_2_1' to 'dut_fpext_32ns_64bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.82 seconds; current allocated memory: 713.839 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 717.517 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 720.646 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 723.696 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 731.375 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 738.551 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_fpext_32ns_64bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.22 seconds; current allocated memory: 748.983 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 751.603 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 753.988 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nslbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 757.550 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40sncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40sncg': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 762.375 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 767.914 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in' to 'attention_ln_weigocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.41 seconds; current allocated memory: 776.836 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.92 seconds; current allocated memory: 784.056 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nslbW_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nsmb6_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mjbC_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xkbM_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40sncg_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:29 ; elapsed = 00:01:35 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 295040 ; free virtual = 371157
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:29 ; elapsed = 00:01:35 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 295040 ; free virtual = 371157
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:34 ; elapsed = 00:01:40 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 294819 ; free virtual = 370969
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:35 ; elapsed = 00:01:41 . Memory (MB): peak = 1360.148 ; gain = 722.125 ; free physical = 294724 ; free virtual = 370888
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:40 ; elapsed = 00:01:47 . Memory (MB): peak = 1431.344 ; gain = 793.320 ; free physical = 294579 ; free virtual = 370764
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:43 ; elapsed = 00:01:50 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 294417 ; free virtual = 370609
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 111.02 seconds; current allocated memory: 669.836 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 672.429 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 673.365 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 673.866 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 674.897 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 675.704 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 675.882 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 675.950 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 677.293 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 678.877 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 679.303 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 679.681 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 680.928 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 682.670 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 683.089 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 683.462 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 683.708 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 684.004 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 684.457 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 685.097 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 685.547 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 686.140 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 13, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 14, distance = 1, offset = 1)
   between 'switch' operation ('_ln327', ./layer.h:327) and 'sub' operation ('sub_ln1193', ./layer.h:327).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 686.888 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 688.505 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 689.143 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 689.813 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 690.720 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.19 seconds; current allocated memory: 694.133 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 695.167 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 695.633 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 700.787 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.96 seconds; current allocated memory: 711.923 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 714.807 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 717.903 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 720.909 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.18 seconds; current allocated memory: 728.651 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 735.111 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.19 seconds; current allocated memory: 743.150 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 744.943 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 747.323 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 750.839 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 755.720 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.1 seconds; current allocated memory: 761.219 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.65 seconds; current allocated memory: 770.117 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.14 seconds; current allocated memory: 777.301 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:28 ; elapsed = 00:01:35 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 295014 ; free virtual = 371129
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:28 ; elapsed = 00:01:35 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 295014 ; free virtual = 371129
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:32 ; elapsed = 00:01:40 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 294794 ; free virtual = 370943
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:33 ; elapsed = 00:01:41 . Memory (MB): peak = 1366.547 ; gain = 728.523 ; free physical = 294701 ; free virtual = 370863
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:38 ; elapsed = 00:01:47 . Memory (MB): peak = 1431.344 ; gain = 793.320 ; free physical = 294554 ; free virtual = 370737
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:41 ; elapsed = 00:01:50 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 294394 ; free virtual = 370584
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 110.71 seconds; current allocated memory: 669.871 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 672.461 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.08 seconds; current allocated memory: 673.402 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 673.888 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 674.917 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 675.729 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 675.906 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 675.973 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 677.300 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 678.885 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 679.313 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 679.688 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 680.949 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 682.694 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 683.114 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 683.487 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 683.737 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 684.028 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 684.483 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 685.122 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 685.574 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 686.151 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 686.890 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 688.490 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 689.145 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 689.819 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 690.738 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.21 seconds; current allocated memory: 694.155 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 695.184 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 695.651 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 700.802 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.1 seconds; current allocated memory: 711.940 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 714.824 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 717.920 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 720.969 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 728.682 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 735.125 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 743.118 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 744.912 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 747.291 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 750.808 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 755.707 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 761.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.85 seconds; current allocated memory: 770.084 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.35 seconds; current allocated memory: 777.281 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295404 ; free virtual = 371489
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295404 ; free virtual = 371489
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.004 ; gain = 602.977 ; free physical = 295186 ; free virtual = 371305
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:09 ; elapsed = 00:01:13 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295092 ; free virtual = 371223
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:324:9) to (./layer.h:323:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:19 . Memory (MB): peak = 1431.344 ; gain = 793.316 ; free physical = 294941 ; free virtual = 371094
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:22 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294781 ; free virtual = 370942
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.8 seconds; current allocated memory: 669.074 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 671.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 672.542 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 673.028 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 674.073 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 674.882 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 675.045 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 675.112 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 676.456 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 678.043 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 678.471 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 678.843 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 680.108 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 681.850 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 682.272 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 682.643 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 682.891 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 683.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 683.669 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 684.309 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 684.776 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 685.354 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 686.000 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 687.451 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 688.051 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 688.721 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 689.578 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.19 seconds; current allocated memory: 692.931 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 693.946 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 694.434 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 699.584 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.03 seconds; current allocated memory: 710.665 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 713.479 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 716.671 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 719.664 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.17 seconds; current allocated memory: 727.382 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 733.841 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.17 seconds; current allocated memory: 741.806 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 743.676 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 745.980 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.79 seconds; current allocated memory: 749.583 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 19 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 754.150 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 759.855 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.73 seconds; current allocated memory: 768.772 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.14 seconds; current allocated memory: 775.933 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_quantizFfa_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_Gfk_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheJfO_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cacheNgs_ram (RAM)' using distributed RAMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:01 ; elapsed = 00:01:05 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295361 ; free virtual = 371445
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:01 ; elapsed = 00:01:05 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295361 ; free virtual = 371445
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:05 ; elapsed = 00:01:10 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295135 ; free virtual = 371254
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:06 ; elapsed = 00:01:11 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295042 ; free virtual = 371173
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:09 ; elapsed = 00:01:15 . Memory (MB): peak = 1431.348 ; gain = 793.320 ; free physical = 294895 ; free virtual = 371048
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:38:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:199:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:327:35)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:331:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:183:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:188:9)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:204:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:205:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:206:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:207:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:217:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:219:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:275:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:275:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:11 ; elapsed = 00:01:17 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294741 ; free virtual = 370900
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 77.29 seconds; current allocated memory: 653.326 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 655.966 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.95 seconds; current allocated memory: 656.896 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 657.370 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 658.030 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 658.345 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 658.457 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 658.526 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 658.884 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 659.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 659.507 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 659.637 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 659.956 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 660.321 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 660.517 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 660.729 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 660.923 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 661.104 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 661.278 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 661.495 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 661.926 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 662.518 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 662.893 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 663.354 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 663.642 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 663.853 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 664.247 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.82 seconds; current allocated memory: 666.392 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 667.187 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 667.559 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 672.509 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2 seconds; current allocated memory: 683.508 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 685.663 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 686.878 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 688.150 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 689.739 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 690.795 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 692.583 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 693.805 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 694.889 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nsmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 696.945 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40socq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40socq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 700.090 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 701.829 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_sc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_tde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_udo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_vdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_wdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_xdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_weDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_ouEe0' due to the length limit 20
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:01 ; elapsed = 00:01:06 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295379 ; free virtual = 371448
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:01 ; elapsed = 00:01:06 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295379 ; free virtual = 371448
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:05 ; elapsed = 00:01:11 . Memory (MB): peak = 1241.004 ; gain = 602.977 ; free physical = 295154 ; free virtual = 371256
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295063 ; free virtual = 371178
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:324:9) to (./layer.h:323:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:11 ; elapsed = 00:01:17 . Memory (MB): peak = 1431.344 ; gain = 793.316 ; free physical = 294918 ; free virtual = 371054
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:38:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:199:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:204:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:205:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:206:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:207:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:20 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294761 ; free virtual = 370905
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 80.42 seconds; current allocated memory: 662.609 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 665.241 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 666.115 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 666.591 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 667.280 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 667.634 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 667.707 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 667.776 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 668.137 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 668.613 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 668.939 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 669.290 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 670.021 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 671.055 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 671.374 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 671.746 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 671.905 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 672.132 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 672.615 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 673.561 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 674.009 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 674.548 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 675.212 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 676.658 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 677.338 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 678.460 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 679.289 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.53 seconds; current allocated memory: 682.630 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 683.391 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 683.764 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 688.658 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.03 seconds; current allocated memory: 699.661 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 701.750 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 702.964 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 704.205 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 706.673 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 711.913 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 716.582 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 718.346 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 24 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 721.074 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nsmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 725.272 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40socq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 19 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40socq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 729.802 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 33 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 736.413 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizrcU' due to the length limit 20
INFO: [SYN 201-210]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:01 ; elapsed = 00:01:06 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295371 ; free virtual = 371456
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:01 ; elapsed = 00:01:06 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295371 ; free virtual = 371456
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:05 ; elapsed = 00:01:11 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295150 ; free virtual = 371269
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295059 ; free virtual = 371190
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:324:9) to (./layer.h:323:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:11 ; elapsed = 00:01:17 . Memory (MB): peak = 1431.348 ; gain = 793.320 ; free physical = 294910 ; free virtual = 371063
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:38:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:199:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:204:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:205:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:206:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:207:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:20 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294751 ; free virtual = 370911
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 80.34 seconds; current allocated memory: 664.380 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 667.008 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 667.901 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 668.388 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 669.397 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 670.244 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 670.451 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 670.519 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 670.884 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 671.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 671.654 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 672.005 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 672.770 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 673.808 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 674.058 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 674.468 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 674.642 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 674.870 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 675.337 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 676.253 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 676.715 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 677.291 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 677.954 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 679.401 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 680.043 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 681.163 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 682.018 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.44 seconds; current allocated memory: 685.361 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.14 seconds; current allocated memory: 686.185 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 686.594 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 691.543 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2 seconds; current allocated memory: 702.674 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 705.560 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 708.678 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 709.918 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 712.333 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 717.543 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 722.211 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 723.989 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 24 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 726.686 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nsmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 731.007 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40socq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 19 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40socq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 735.510 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 33 instance(s).
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295369 ; free virtual = 371455
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295369 ; free virtual = 371455
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.012 ; gain = 602.984 ; free physical = 295150 ; free virtual = 371270
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:08 ; elapsed = 00:01:14 . Memory (MB): peak = 1366.555 ; gain = 728.527 ; free physical = 295053 ; free virtual = 371186
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:324:9) to (./layer.h:323:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:13 ; elapsed = 00:01:19 . Memory (MB): peak = 1431.352 ; gain = 793.324 ; free physical = 294908 ; free virtual = 371063
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:38:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:199:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:204:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:205:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:206:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:207:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:16 ; elapsed = 00:01:22 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294748 ; free virtual = 370910
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.84 seconds; current allocated memory: 668.013 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 670.626 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 671.547 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 672.048 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 673.040 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 673.887 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 674.064 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 674.135 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 675.461 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 677.047 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 677.474 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 677.848 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 678.594 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 679.653 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 679.918 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 680.291 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 680.456 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 680.719 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 681.200 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 682.119 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 682.565 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 683.106 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 683.791 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 685.219 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 685.899 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 687.016 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 687.855 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.23 seconds; current allocated memory: 690.988 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.16 seconds; current allocated memory: 691.916 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 692.335 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 697.312 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2 seconds; current allocated memory: 708.430 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 711.264 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 714.412 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 717.448 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 725.137 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 730.412 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 735.119 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 736.846 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 24 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 739.530 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nsmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nsncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nsmb6': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 743.858 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40socq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 19 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40socq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 748.440 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 33 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 754.979 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizXh4' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 17 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.84 seconds; current allocated memory: 764.998 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.01 seconds; current allocated memory: 771.928 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_hbi_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nsmb6_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nsncg_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mjbC_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xlbW_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40socq_div'
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:05 ; elapsed = 00:01:11 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295386 ; free virtual = 371473
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:05 ; elapsed = 00:01:11 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295386 ; free virtual = 371473
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:09 ; elapsed = 00:01:15 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295168 ; free virtual = 371289
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:11 ; elapsed = 00:01:17 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295073 ; free virtual = 371207
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:15 ; elapsed = 00:01:22 . Memory (MB): peak = 1431.348 ; gain = 793.320 ; free physical = 294948 ; free virtual = 371103
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:18 ; elapsed = 00:01:25 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294787 ; free virtual = 370949
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 85.89 seconds; current allocated memory: 665.407 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 667.997 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 668.903 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 669.393 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 670.439 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 671.249 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 671.409 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 671.479 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 671.846 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 672.322 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 672.632 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 673.005 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 674.265 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 676.049 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 676.503 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 676.877 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 677.089 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 677.379 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 677.833 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 678.516 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 678.963 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 679.505 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 680.266 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 681.885 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 682.482 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 683.155 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 684.045 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.44 seconds; current allocated memory: 687.660 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 688.640 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 689.078 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 694.145 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.03 seconds; current allocated memory: 705.284 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 708.136 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 711.235 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 712.508 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 714.989 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 721.484 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.18 seconds; current allocated memory: 729.419 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 731.288 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 733.593 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 737.231 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 742.022 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.14 seconds; current allocated memory: 747.578 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_qcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_rcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_sc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cachetde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Bew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizFfa' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:02 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295344 ; free virtual = 371431
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:02 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295344 ; free virtual = 371431
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295122 ; free virtual = 371243
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:08 ; elapsed = 00:01:13 . Memory (MB): peak = 1367.348 ; gain = 729.320 ; free physical = 295031 ; free virtual = 371165
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:13 ; elapsed = 00:01:19 . Memory (MB): peak = 1495.242 ; gain = 857.215 ; free physical = 294834 ; free virtual = 370989
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:16 ; elapsed = 00:01:22 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294637 ; free virtual = 370799
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.81 seconds; current allocated memory: 669.265 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 671.857 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 672.779 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 673.266 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 674.314 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 675.122 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 675.283 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 675.351 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('output_0_V_load_3', ./layer.h:164) on array 'output_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 676.746 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 678.330 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 678.759 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 679.133 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 680.394 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 682.142 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 682.559 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 682.930 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 683.177 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 683.475 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 683.926 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 684.562 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 685.014 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 685.606 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 686.330 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 687.947 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 688.600 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 689.272 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 690.194 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.27 seconds; current allocated memory: 693.595 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 694.642 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 695.112 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.1 seconds; current allocated memory: 700.228 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.44 seconds; current allocated memory: 711.384 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 714.221 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 717.378 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 720.352 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 728.096 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 734.553 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 742.480 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 744.349 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 746.707 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 750.299 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 755.119 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.09 seconds; current allocated memory: 760.644 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.68 seconds; current allocated memory: 769.574 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.12 seconds; current allocated memory: 776.741 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295263 ; free virtual = 371350
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295263 ; free virtual = 371350
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295040 ; free virtual = 371161
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:08 ; elapsed = 00:01:14 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 294948 ; free virtual = 371082
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:13 ; elapsed = 00:01:19 . Memory (MB): peak = 1431.348 ; gain = 793.320 ; free physical = 294826 ; free virtual = 370981
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:16 ; elapsed = 00:01:22 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294666 ; free virtual = 370828
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.63 seconds; current allocated memory: 665.781 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 668.367 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 669.292 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 669.781 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 670.828 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 671.641 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 671.803 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 671.871 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 672.383 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 673.060 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 673.421 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 673.792 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 675.002 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 676.786 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 677.223 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 677.598 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 677.812 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 678.101 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 678.595 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 679.240 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 679.691 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 680.268 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 681.008 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 682.623 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 683.225 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 683.935 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 684.815 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.53 seconds; current allocated memory: 688.400 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 689.379 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 689.853 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 694.936 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.02 seconds; current allocated memory: 706.091 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 708.893 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 712.075 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 713.581 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 716.920 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 723.407 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.16 seconds; current allocated memory: 731.404 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 733.243 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 735.593 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 739.184 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 744.007 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 749.526 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_qcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_rcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_sc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cachetde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Bew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouEe0' due to the length limit 20
INFO: ==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:02 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295243 ; free virtual = 371314
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:02 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295243 ; free virtual = 371314
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:06 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295018 ; free virtual = 371123
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:07 ; elapsed = 00:01:13 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 294925 ; free virtual = 371042
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:12 ; elapsed = 00:01:19 . Memory (MB): peak = 1431.344 ; gain = 793.316 ; free physical = 294775 ; free virtual = 370913
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157:68) in function 'linear_forward_no_mul<1, 16, 16>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:15 ; elapsed = 00:01:22 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294622 ; free virtual = 370768
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.29 seconds; current allocated memory: 665.813 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 668.400 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.07 seconds; current allocated memory: 669.327 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 669.817 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 670.860 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 671.675 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 671.839 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 671.906 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_38', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_38', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_39', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 7, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_40', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 672.477 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 673.180 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 673.537 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 673.916 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 675.119 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 676.907 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 677.339 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 677.718 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 677.925 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 678.215 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 678.705 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 679.351 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 679.799 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 680.379 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 681.116 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 682.737 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 683.335 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 684.044 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 684.923 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.41 seconds; current allocated memory: 688.507 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 689.487 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 689.965 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.09 seconds; current allocated memory: 695.049 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.14 seconds; current allocated memory: 706.208 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 709.009 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 712.187 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 713.947 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.79 seconds; current allocated memory: 717.294 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 723.780 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.28 seconds; current allocated memory: 731.776 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 733.571 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 735.951 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 739.510 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 744.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:05 ; elapsed = 00:01:10 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 289804 ; free virtual = 365851
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:05 ; elapsed = 00:01:10 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 289803 ; free virtual = 365850
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:10 ; elapsed = 00:01:14 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 289546 ; free virtual = 365627
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:11 ; elapsed = 00:01:16 . Memory (MB): peak = 1367.344 ; gain = 729.320 ; free physical = 289529 ; free virtual = 365623
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:16 ; elapsed = 00:01:21 . Memory (MB): peak = 1495.238 ; gain = 857.215 ; free physical = 289343 ; free virtual = 365486
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:19 ; elapsed = 00:01:25 . Memory (MB): peak = 1616.148 ; gain = 978.125 ; free physical = 289162 ; free virtual = 365311
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 85.2 seconds; current allocated memory: 669.278 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 671.870 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 672.792 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 673.277 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 674.325 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 675.133 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 675.295 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 675.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 676.704 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 678.289 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 678.718 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 679.090 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 680.354 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 682.096 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 682.518 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 682.890 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 683.138 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 683.432 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 683.887 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 684.525 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 684.976 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 685.551 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 686.291 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 687.905 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 688.588 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 689.262 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 690.154 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.32 seconds; current allocated memory: 693.555 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 694.586 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 695.050 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.16 seconds; current allocated memory: 700.187 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.97 seconds; current allocated memory: 711.309 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 714.119 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 717.276 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 720.249 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.08 seconds; current allocated memory: 727.930 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 734.387 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.07 seconds; current allocated memory: 742.397 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 744.251 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 746.572 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 750.159 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 755.001 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 760.526 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.42 seconds; current allocated memory: 769.439 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2 seconds; current allocated memory: 776.593 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 293825 ; free virtual = 369839
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 293825 ; free virtual = 369838
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:08 ; elapsed = 00:01:13 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 293539 ; free virtual = 369587
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:09 ; elapsed = 00:01:14 . Memory (MB): peak = 1367.344 ; gain = 729.320 ; free physical = 293448 ; free virtual = 369508
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:20 . Memory (MB): peak = 1495.238 ; gain = 857.215 ; free physical = 292947 ; free virtual = 369029
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:23 . Memory (MB): peak = 1616.148 ; gain = 978.125 ; free physical = 292737 ; free virtual = 368827
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 83.67 seconds; current allocated memory: 669.303 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 671.877 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 672.784 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 673.271 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 674.316 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 675.124 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 675.287 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 675.356 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 20, Final II = 20, Depth = 115.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 676.701 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 678.281 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 678.725 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 679.095 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 680.346 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 682.091 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 682.526 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 682.897 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 683.148 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 683.440 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 683.892 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 684.535 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 684.986 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 685.581 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 686.302 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 687.919 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 688.584 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 689.258 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 690.150 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.26 seconds; current allocated memory: 693.535 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 694.551 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 695.016 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 700.153 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.09 seconds; current allocated memory: 711.273 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 714.117 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 717.256 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 720.287 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 727.991 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 734.418 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.1 seconds; current allocated memory: 742.382 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 744.251 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 746.553 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.79 seconds; current allocated memory: 750.162 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 754.985 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 760.494 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.73 seconds; current allocated memory: 769.407 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.24 seconds; current allocated memory: 776.588 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295424 ; free virtual = 371513
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295424 ; free virtual = 371513
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295204 ; free virtual = 371326
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:08 ; elapsed = 00:01:13 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295110 ; free virtual = 371245
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:13 ; elapsed = 00:01:19 . Memory (MB): peak = 1431.344 ; gain = 793.316 ; free physical = 294995 ; free virtual = 371151
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:16 ; elapsed = 00:01:22 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294835 ; free virtual = 370999
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.83 seconds; current allocated memory: 669.278 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 671.870 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 672.794 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 673.282 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 674.327 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 675.135 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 675.297 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 675.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 25, Final II = 25, Depth = 110.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 676.718 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 678.301 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 678.730 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 679.102 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 680.364 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 682.110 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 682.530 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 682.901 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 683.149 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 683.444 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 683.899 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 684.539 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 684.987 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 685.563 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 686.303 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 687.918 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 688.601 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 689.274 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 690.166 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.22 seconds; current allocated memory: 693.567 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 694.597 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 695.067 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.14 seconds; current allocated memory: 700.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.04 seconds; current allocated memory: 711.338 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 714.164 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 717.337 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 720.349 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.07 seconds; current allocated memory: 727.957 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 734.409 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 742.358 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 744.197 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 746.533 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 750.170 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 754.961 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 760.523 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.67 seconds; current allocated memory: 769.465 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.11 seconds; current allocated memory: 776.566 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295409 ; free virtual = 371497
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295409 ; free virtual = 371497
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:13 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295187 ; free virtual = 371309
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:08 ; elapsed = 00:01:14 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295095 ; free virtual = 371230
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:13 ; elapsed = 00:01:20 . Memory (MB): peak = 1431.348 ; gain = 793.320 ; free physical = 294947 ; free virtual = 371103
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:16 ; elapsed = 00:01:23 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294788 ; free virtual = 370951
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 83.67 seconds; current allocated memory: 669.287 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 671.879 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 672.801 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 673.287 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 674.333 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 675.142 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 675.303 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 675.372 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 30, Final II = 30, Depth = 110.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 676.724 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 678.316 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 678.744 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 679.113 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 680.379 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 682.121 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 682.542 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 682.913 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 683.163 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 683.458 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 683.912 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 684.549 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 685.000 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 685.578 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 686.315 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 687.931 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 688.614 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 689.285 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 690.177 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.28 seconds; current allocated memory: 693.578 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.32 seconds; current allocated memory: 694.608 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 695.075 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.09 seconds; current allocated memory: 700.195 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.05 seconds; current allocated memory: 711.346 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 714.173 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 717.346 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 720.437 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 728.030 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 734.482 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 742.432 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 744.272 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 746.604 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 750.213 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 755.034 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 760.597 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.67 seconds; current allocated memory: 769.539 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.2 seconds; current allocated memory: 776.615 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:04 ; elapsed = 00:01:09 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295415 ; free virtual = 371503
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:04 ; elapsed = 00:01:09 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295415 ; free virtual = 371503
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:08 ; elapsed = 00:01:14 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295187 ; free virtual = 371309
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1366.551 ; gain = 728.523 ; free physical = 295095 ; free virtual = 371230
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:21 . Memory (MB): peak = 1431.348 ; gain = 793.320 ; free physical = 294945 ; free virtual = 371102
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157:68) in function 'linear_forward_no_mul<1, 16, 16>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:24 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294796 ; free virtual = 370960
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 84.61 seconds; current allocated memory: 665.840 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 668.439 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 669.363 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 669.850 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 670.896 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 671.707 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 671.868 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 671.938 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_38', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_38', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_39', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 7, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln164', ./layer.h:164) of variable 'sub_ln703_40', ./layer.h:164 on array 'output_0_V' and 'load' operation ('output_0_V_load_2', ./layer.h:164) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 672.507 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 673.211 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 673.569 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 673.945 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 675.148 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 676.937 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 677.370 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 677.750 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 677.959 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 678.251 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 678.739 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 679.384 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 679.831 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 680.412 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 681.133 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 682.752 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 683.367 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 684.079 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 684.957 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.44 seconds; current allocated memory: 688.542 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 689.524 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 689.987 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 695.071 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.99 seconds; current allocated memory: 706.197 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 708.998 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 712.192 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_seq_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 713.968 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 717.316 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 723.817 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 731.798 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 733.592 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 735.973 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 739.552 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 744.384 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:02 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295427 ; free virtual = 371516
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:02 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295427 ; free virtual = 371516
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:06 ; elapsed = 00:01:11 . Memory (MB): peak = 1241.008 ; gain = 602.980 ; free physical = 295204 ; free virtual = 371327
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:07 ; elapsed = 00:01:13 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 295113 ; free virtual = 371248
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:12 ; elapsed = 00:01:19 . Memory (MB): peak = 1431.348 ; gain = 793.320 ; free physical = 294966 ; free virtual = 371122
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:139:11)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:15 ; elapsed = 00:01:22 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 294805 ; free virtual = 370969
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.72 seconds; current allocated memory: 669.303 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 671.878 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 672.788 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 673.273 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 674.318 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 675.126 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 675.288 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 675.354 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 100, Final II = 100, Depth = 110.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 676.704 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 678.288 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 678.732 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 679.102 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 680.350 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 682.096 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 682.532 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 682.904 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 683.155 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 683.451 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 683.903 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 684.542 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 684.993 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 685.585 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 686.308 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 687.926 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 688.589 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 689.264 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 690.157 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.18 seconds; current allocated memory: 693.543 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 694.559 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 695.025 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 700.158 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.09 seconds; current allocated memory: 711.281 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 714.136 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 717.279 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 720.876 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.17 seconds; current allocated memory: 728.416 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 734.901 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.58 seconds; current allocated memory: 742.882 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 744.707 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 747.091 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 750.711 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 755.469 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 761.030 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizVhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.76 seconds; current allocated memory: 769.974 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.34 seconds; current allocated memory: 777.112 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:04 ; elapsed = 00:01:10 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294956 ; free virtual = 371044
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:04 ; elapsed = 00:01:10 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294956 ; free virtual = 371044
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:09 ; elapsed = 00:01:15 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 294736 ; free virtual = 370858
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:10 ; elapsed = 00:01:17 . Memory (MB): peak = 1367.344 ; gain = 729.320 ; free physical = 294643 ; free virtual = 370778
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:15 ; elapsed = 00:01:24 . Memory (MB): peak = 1495.242 ; gain = 857.219 ; free physical = 294501 ; free virtual = 370657
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294932 ; free virtual = 371021
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294932 ; free virtual = 371020
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 294694 ; free virtual = 370817
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:08 ; elapsed = 00:01:13 . Memory (MB): peak = 1367.340 ; gain = 729.316 ; free physical = 294603 ; free virtual = 370738
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:289) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:203) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:215) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:251) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:324) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:289) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:271) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:275) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:202) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:214) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:181) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:199) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:200) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:199) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:200) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:199) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:200) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:327) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:203:69) to (./layer.h:203:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:20 . Memory (MB): peak = 1495.238 ; gain = 857.215 ; free physical = 294456 ; free virtual = 370612
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:250:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:320:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:182:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:270:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:249:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:181:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:40)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:234:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:201:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:270:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:270:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:252:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:163:35)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:40)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:167:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:238:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:237:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:23 . Memory (MB): peak = 1616.148 ; gain = 978.125 ; free physical = 294291 ; free virtual = 370455
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 83.94 seconds; current allocated memory: 671.329 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 673.928 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 674.833 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 675.357 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 676.739 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 678.587 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 678.762 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 678.829 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('output_0_V_load_3', ./layer.h:164) on array 'output_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 680.028 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 681.317 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 681.756 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 682.133 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 683.362 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 685.102 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 685.538 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 685.909 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 686.137 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 686.430 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 686.921 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 687.562 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 687.996 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 688.594 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 689.319 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 690.935 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 691.571 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 692.247 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 693.103 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.45 seconds; current allocated memory: 696.848 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.32 seconds; current allocated memory: 698.001 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 698.512 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.17 seconds; current allocated memory: 703.744 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2 seconds; current allocated memory: 714.866 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_1_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_8_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 720.648 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.14 seconds; current allocated memory: 726.537 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 729.135 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 736.271 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 742.712 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 750.663 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 752.533 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 754.888 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 758.467 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 763.227 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 768.848 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_QgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouThq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.8 seconds; current allocated memory: 778.599 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.28 seconds; current allocated memory: 785.864 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_Ffa_ram (RAM)' using block RAMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:04 ; elapsed = 00:01:09 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294906 ; free virtual = 370994
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:04 ; elapsed = 00:01:09 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294906 ; free virtual = 370994
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:08 ; elapsed = 00:01:13 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 294684 ; free virtual = 370806
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:09 ; elapsed = 00:01:15 . Memory (MB): peak = 1366.547 ; gain = 728.523 ; free physical = 294593 ; free virtual = 370728
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:333) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:15 ; elapsed = 00:01:21 . Memory (MB): peak = 1431.340 ; gain = 793.316 ; free physical = 294453 ; free virtual = 370609
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:322:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:18 ; elapsed = 00:01:24 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 294291 ; free virtual = 370455
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 84.67 seconds; current allocated memory: 668.533 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 671.128 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 672.051 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 672.579 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 673.943 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 675.818 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 676.008 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 676.076 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 676.832 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 678.008 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 678.465 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 678.841 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 680.069 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 681.818 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 682.251 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 682.625 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 682.892 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 683.181 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 683.638 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 684.282 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 684.718 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 685.311 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 686.038 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 687.656 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 688.294 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 688.969 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 689.824 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.67 seconds; current allocated memory: 693.676 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 694.896 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 695.406 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 700.758 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.02 seconds; current allocated memory: 711.882 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_1_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_8_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 717.649 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 723.541 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 725.946 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 730.841 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 737.318 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 745.249 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 747.089 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 749.460 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 753.043 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 757.899 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.19 seconds; current allocated memory: 763.426 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_QgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouThq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.73 seconds; current allocated memory: 773.180 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.18 seconds; current allocated memory: 780.490 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_Ffa_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheIfE_ram (RAM)' using distributed RAMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294551 ; free virtual = 370631
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:07 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294551 ; free virtual = 370631
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.008 ; gain = 602.984 ; free physical = 294329 ; free virtual = 370443
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:09 ; elapsed = 00:01:13 . Memory (MB): peak = 1367.344 ; gain = 729.320 ; free physical = 294237 ; free virtual = 370364
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:333) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:64) to (./layer.h:136:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:103) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:20 . Memory (MB): peak = 1495.238 ; gain = 857.215 ; free physical = 294094 ; free virtual = 370243
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:322:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:23 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 293929 ; free virtual = 370085
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 83.15 seconds; current allocated memory: 668.638 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 671.235 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 672.140 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 672.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 674.045 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 675.901 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 676.076 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 676.144 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 676.900 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 678.074 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 678.550 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 678.924 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 680.153 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 681.900 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 682.335 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 682.711 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 682.978 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 683.268 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 683.720 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 684.365 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 684.800 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 685.395 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 686.121 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 687.738 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 688.374 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 689.049 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 689.904 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.65 seconds; current allocated memory: 693.758 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.38 seconds; current allocated memory: 694.994 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 695.503 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 700.838 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.97 seconds; current allocated memory: 711.961 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_1_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_8_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 717.731 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.4 seconds; current allocated memory: 723.637 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 726.058 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 730.953 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 737.414 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.22 seconds; current allocated memory: 745.361 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 747.201 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 749.573 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 753.152 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 758.010 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.14 seconds; current allocated memory: 763.535 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_QgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouThq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.67 seconds; current allocated memory: 773.293 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.28 seconds; current allocated memory: 780.616 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_Ffa_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheIfE_ram (RAM)' using distributed RAMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294331 ; free virtual = 370589
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294331 ; free virtual = 370589
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 294112 ; free virtual = 370403
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:08 ; elapsed = 00:01:14 . Memory (MB): peak = 1366.547 ; gain = 728.523 ; free physical = 294018 ; free virtual = 370323
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:333) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:117:16) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:21 . Memory (MB): peak = 1431.344 ; gain = 793.320 ; free physical = 293868 ; free virtual = 370194
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:322:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:24 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 293709 ; free virtual = 370042
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 84.6 seconds; current allocated memory: 668.528 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 671.118 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.09 seconds; current allocated memory: 672.046 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 672.572 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 674.287 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 676.267 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 676.698 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 676.764 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 677.519 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 678.673 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 679.171 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 679.540 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 680.787 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 682.536 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 682.955 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 683.326 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 683.536 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 683.833 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 684.323 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 684.963 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 685.414 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 686.006 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 15, Final II = 15, Depth = 16.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 686.730 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 688.353 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 688.989 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 689.660 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 690.516 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.61 seconds; current allocated memory: 694.699 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.54 seconds; current allocated memory: 696.091 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 696.689 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.34 seconds; current allocated memory: 702.247 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.18 seconds; current allocated memory: 713.362 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 718.796 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.69 seconds; current allocated memory: 727.187 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 729.645 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.1 seconds; current allocated memory: 734.569 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 740.994 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.36 seconds; current allocated memory: 748.889 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 750.787 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 753.064 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 756.697 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 1.14 seconds; current allocated memory: 761.493 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.39 seconds; current allocated memory: 767.089 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_QgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouThq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.92 seconds; current allocated memory: 776.838 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.58 seconds; current allocated memory: 784.356 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_Ffa_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheIfE_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cacheMgi_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_proj_QgW_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_output_0_ram (RAM)' using block RAMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294326 ; free virtual = 370582
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294326 ; free virtual = 370582
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:07 ; elapsed = 00:01:12 . Memory (MB): peak = 1241.004 ; gain = 602.980 ; free physical = 294107 ; free virtual = 370397
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>131' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>131' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:09 ; elapsed = 00:01:14 . Memory (MB): peak = 1366.547 ; gain = 728.523 ; free physical = 294013 ; free virtual = 370316
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:30).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:473) in function 'exp_reduce::exp<41, 25>' completely with a factor of 41.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:476) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-3' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:485) in function 'exp_reduce::exp<41, 25>' completely with a factor of 27.
INFO: [HLS 200-489] Unrolling loop 'Loop-4' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:492) in function 'exp_reduce::exp<41, 25>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-5' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:851) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-6' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:857) in function 'exp_reduce::exp<41, 25>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-7' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:860) in function 'exp_reduce::exp<41, 25>' completely with a factor of 56.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:333) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<41, 25>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:117:16) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<41, 25>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...28 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:21 . Memory (MB): peak = 1431.340 ; gain = 793.316 ; free physical = 293866 ; free virtual = 370190
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<41, 25>' to 'exp<41, 25>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:24 . Memory (MB): peak = 1680.148 ; gain = 1042.125 ; free physical = 293704 ; free virtual = 370036
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'exp<41, 25>' to 'exp_41_25_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 84.26 seconds; current allocated memory: 667.716 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 670.313 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.09 seconds; current allocated memory: 671.236 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 671.776 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 673.490 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 675.453 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 675.885 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 675.952 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 676.708 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 677.844 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 678.357 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 678.728 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 679.960 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 681.700 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 682.136 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 682.510 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 682.737 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 683.033 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 683.524 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 684.160 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<41, 25>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 684.596 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 685.190 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 685.776 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 686.880 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 687.600 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 688.275 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 689.169 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.84 seconds; current allocated memory: 693.624 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.55 seconds; current allocated memory: 695.150 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 695.780 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.48 seconds; current allocated memory: 701.412 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.24 seconds; current allocated memory: 712.565 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 717.946 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.74 seconds; current allocated memory: 726.336 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 728.795 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.12 seconds; current allocated memory: 733.720 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 740.143 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.35 seconds; current allocated memory: 748.053 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 749.951 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 752.261 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_41_25_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_3_table_V' to 'exp_41_25_s_f_x_mhbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_f_x_msb_2_table_V' to 'exp_41_25_s_f_x_mibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_41_25_s_exp_x_msb_1_table_V' to 'exp_41_25_s_exp_xjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_67ns_62ns_129_5_1' to 'dut_mul_67ns_62nskbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72ns_68ns_140_5_1' to 'dut_mul_72ns_68nslbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_67ns_62nskbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72ns_68nslbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_41_25_s'.
INFO: [HLS 200-111]  Elapsed time: 1.08 seconds; current allocated memory: 755.880 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40smb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 33 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40smb6': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 760.326 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.39 seconds; current allocated memory: 764.552 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_QgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Rg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Shg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouThq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.86 seconds; current allocated memory: 774.328 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.73 seconds; current allocated memory: 782.024 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_67ns_62nskbM_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72ns_68nslbW_MulnS_2'
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mhbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_f_x_mibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_41_25_s_exp_xjbC_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40smb6_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_Ffa_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheIfE_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cacheMgi_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_proj_QgW_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_output_0_ram (RAM)' using block RAMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:51 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294346 ; free virtual = 370586
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:51 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294346 ; free virtual = 370586
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1171.648 ; gain = 533.625 ; free physical = 294256 ; free virtual = 370510
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:54 ; elapsed = 00:00:59 . Memory (MB): peak = 1235.207 ; gain = 597.184 ; free physical = 294210 ; free virtual = 370469
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:334) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:117:16) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:58 ; elapsed = 00:01:04 . Memory (MB): peak = 1299.289 ; gain = 661.266 ; free physical = 294128 ; free virtual = 370397
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:01 ; elapsed = 00:01:07 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293994 ; free virtual = 370266
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 67.33 seconds; current allocated memory: 441.799 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 444.378 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 445.284 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 445.822 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 447.535 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 449.515 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 449.961 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 450.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 450.769 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 451.906 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 452.405 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 452.776 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 454.038 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 455.785 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 456.203 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 456.575 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 456.790 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 457.088 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 457.579 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 458.215 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 458.641 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 459.374 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 459.976 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 460.651 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 461.509 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.16 seconds; current allocated memory: 465.273 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.36 seconds; current allocated memory: 466.630 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 467.222 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 472.665 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.01 seconds; current allocated memory: 483.813 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 489.244 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.66 seconds; current allocated memory: 497.622 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 500.078 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 505.017 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 511.439 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 519.379 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 521.276 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 523.567 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 527.687 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 531.065 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Aem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Bew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouOgC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.79 seconds; current allocated memory: 540.844 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.4 seconds; current allocated memory: 548.258 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40shbi_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigibs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigibs_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 293861 ; free virtual = 370103
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 293861 ; free virtual = 370103
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:56 ; elapsed = 00:01:00 . Memory (MB): peak = 1232.148 ; gain = 594.125 ; free physical = 293766 ; free virtual = 370021
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:56 ; elapsed = 00:01:01 . Memory (MB): peak = 1235.234 ; gain = 597.211 ; free physical = 293713 ; free virtual = 369974
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'SOFTMAX_LOOP_1' (./layer.h:322) in function 'softmax<1, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:334) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:117:16) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:01 ; elapsed = 00:01:07 . Memory (MB): peak = 1299.316 ; gain = 661.293 ; free physical = 293804 ; free virtual = 370076
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:03 ; elapsed = 00:01:10 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293664 ; free virtual = 369938
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 70.27 seconds; current allocated memory: 443.085 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 445.680 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 446.573 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 447.113 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 448.826 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 450.804 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 451.236 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 451.302 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 452.058 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 453.195 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 453.709 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 454.082 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 455.310 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 457.058 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 457.492 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 457.864 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 458.092 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 458.390 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 458.909 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 459.547 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_1'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 35, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 51, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 59, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 61, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_1): Unable to enforce a carried dependence constraint (II = 62, distance = 1, offset = 1)
   between 'sdiv' operation ('sdiv_ln1148', ./layer.h:334) and 'mux' operation ('tmp_1', ./layer.h:324).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 63, Depth = 64.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 460.028 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 460.861 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 461.445 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 462.119 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 462.980 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.51 seconds; current allocated memory: 467.023 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.71 seconds; current allocated memory: 468.347 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 468.902 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 474.362 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.99 seconds; current allocated memory: 485.481 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 490.900 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.56 seconds; current allocated memory: 499.157 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 501.604 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 506.494 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 512.954 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 520.907 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 522.776 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 525.078 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 529.361 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 532.746 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Aem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Bew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Mgi' due to the length limit 20
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:50 ; elapsed = 00:00:53 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294152 ; free virtual = 370393
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:50 ; elapsed = 00:00:53 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294152 ; free virtual = 370393
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1232.148 ; gain = 594.125 ; free physical = 294062 ; free virtual = 370316
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:53 ; elapsed = 00:00:56 . Memory (MB): peak = 1235.238 ; gain = 597.215 ; free physical = 294012 ; free virtual = 370272
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:335) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:117:16) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:57 ; elapsed = 00:01:02 . Memory (MB): peak = 1299.320 ; gain = 661.297 ; free physical = 293933 ; free virtual = 370202
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:00 ; elapsed = 00:01:05 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293798 ; free virtual = 370070
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 65.46 seconds; current allocated memory: 444.276 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 446.867 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 447.794 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 448.318 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 450.045 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 452.008 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 452.441 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 452.507 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 453.263 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 454.421 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 454.901 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 455.277 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 456.521 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 458.265 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 458.701 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 459.073 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 459.300 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 459.599 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 460.090 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 460.728 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 461.266 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 462.161 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 462.722 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 463.435 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 464.291 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.34 seconds; current allocated memory: 468.070 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.41 seconds; current allocated memory: 469.463 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 470.018 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 475.533 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2 seconds; current allocated memory: 486.617 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 492.066 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.62 seconds; current allocated memory: 500.329 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 502.479 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 507.268 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 513.721 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 521.656 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 523.524 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 525.826 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 530.159 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.08 seconds; current allocated memory: 534.146 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Aem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Bew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouOgC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.82 seconds; current allocated memory: 543.895 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.27 seconds; current allocated memory: 551.356 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40shbi_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigibs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigibs_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294149 ; free virtual = 370390
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294149 ; free virtual = 370389
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:54 ; elapsed = 00:00:59 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294059 ; free virtual = 370313
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:55 ; elapsed = 00:00:59 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 294010 ; free virtual = 370270
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:291) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:274) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:323) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:326) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:335) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:292) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:277) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:254) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:326) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:117:16) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:59 ; elapsed = 00:01:05 . Memory (MB): peak = 1299.293 ; gain = 661.270 ; free physical = 293930 ; free virtual = 370199
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:252:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:272:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:251:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:272:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:272:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:240:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:21)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:02 ; elapsed = 00:01:08 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293797 ; free virtual = 370069
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 68.34 seconds; current allocated memory: 442.127 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 444.721 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 445.643 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 446.167 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 447.882 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 449.861 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 450.274 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 450.342 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 451.097 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 452.238 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 452.734 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 453.110 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 454.355 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 456.094 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 456.530 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 456.902 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 457.136 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 457.428 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 457.907 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 458.546 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 459.053 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 459.884 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 460.433 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 461.108 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 462.000 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.26 seconds; current allocated memory: 465.782 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.43 seconds; current allocated memory: 467.155 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 467.712 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 473.221 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.05 seconds; current allocated memory: 484.358 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 489.739 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.48 seconds; current allocated memory: 498.112 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 500.248 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 505.068 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 511.518 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.32 seconds; current allocated memory: 519.457 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 521.324 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 523.627 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 527.861 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 531.596 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_Aem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_Bew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouOgC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.7 seconds; current allocated memory: 541.325 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.39 seconds; current allocated memory: 548.705 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_39s_39nsdEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40shbi_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigibs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigibs_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:53 ; elapsed = 00:00:57 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294173 ; free virtual = 370416
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:53 ; elapsed = 00:00:57 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294173 ; free virtual = 370416
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:55 ; elapsed = 00:00:59 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294082 ; free virtual = 370337
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:325) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:56 ; elapsed = 00:01:00 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 294033 ; free virtual = 370295
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:290) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:322) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:238) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:322) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:325) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:327) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:334) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:239) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:325) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:117:16) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:238:8) to (./layer.h:237:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:00 ; elapsed = 00:01:06 . Memory (MB): peak = 1299.223 ; gain = 661.199 ; free physical = 293956 ; free virtual = 370227
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:251:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:237:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:250:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:271:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:271:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:253:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:03 ; elapsed = 00:01:09 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293823 ; free virtual = 370097
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 69.08 seconds; current allocated memory: 442.313 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 444.893 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 445.816 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 446.339 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 448.051 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 450.032 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 450.463 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 450.531 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 451.300 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 452.456 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 452.951 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 453.325 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 454.568 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 456.317 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 456.731 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 457.074 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 457.334 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 457.630 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 458.068 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 458.710 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 459.214 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 460.045 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 460.578 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 461.254 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 462.154 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.3 seconds; current allocated memory: 465.944 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.41 seconds; current allocated memory: 467.302 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 467.893 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 473.368 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.05 seconds; current allocated memory: 484.520 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_39s_39ns_39_43_seq_1' to 'dut_udiv_39s_39nsdEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_39s_39nsdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 489.982 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.64 seconds; current allocated memory: 498.305 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 500.410 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 505.260 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 511.747 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.34 seconds; current allocated memory: 519.697 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 521.412 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 523.715 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 527.975 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 531.680 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
ERROR: [HLS 200-70] Compilation errors found: In file included from attention.cpp:1:
In file included from attention.cpp:7:
./layer.h:321:29: error: member reference type 'attn_fixed_t *' (aka 'ap_fixed<40, 24> *') is a pointer; maybe you meant to use '->'?
    attn_fixed_t right = arr.size() - 1;
                         ~~~^
                            ->
./layer.h:321:30: error: no member named 'size' in 'ap_fixed<40, 24, 5, 3, 0>'
    attn_fixed_t right = arr.size() - 1;
                         ~~~ ^
In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
In file included from ./model.h:9:
In file included from ./typedefs.h:9:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:367:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed.h:55:
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:838:13: warning: shift count is negative [-Wshift-count-negative]
      ret.V <<= (_AP_I - _AP_W);
            ^   ~~~~~~~~~~~~~~~
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int_base.h:359:18: note: in instantiation of member function 'ap_fixed_base<40, 24, true, 5, 3, 0>::to_ap_int_base' requested here
    Base::V = op.to_ap_int_base().V;
                 ^
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:108:9: note: in instantiation of function template specialization 'ap_int_base<32, true>::ap_int_base<40, 24, true, 5, 3, 0>' requested here
      : Base((ap_fixed_base<_AP_W2, _AP_I2, true, _AP_Q2, _AP_O2, _AP_N2>)op) {}
        ^
./layer.h:137:38: note: in instantiation of function template specialization 'ap_int<32>::ap_int<40, 24, 5, 3, 0>' requested here
          sbit32_t quantized_value = attention_round(input[i][j] * scale);
                                     ^
attention.cpp:106:3: note: in instantiation of function template specialization 'quantize_activation<1, 16>' requested here
  quantize_activation<SEQ_LEN, HS_COLS>(
  ^
attention.cpp:31:3: note: in instantiation of function template specialization 'attention<2, 1, 16, 16, 4, 4>' requested here
  attention<
  ^
In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
In file included from ./model.h:9:
In file included from ./typedefs.h:9:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:367:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed.h:55:
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:838:13: warning: shift count is negative [-Wshift-count-negative]
      ret.V <<= (_AP_I - _AP_W);
            ^   ~~~~~~~~~~~~~~~
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:873:76: note: in instantiation of member function 'ap_fixed_base<49, 33, true, 5, 3, 0>::to_ap_int_base' requested here
  inline __attribute__((always_inline)) ap_slong to_int64() const { return to_ap_int_base().to_int64(); }
                                                                           ^
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:1048:78: note: in instantiation of member function 'ap_fixed_base<49, 33, true, 5, 3, 0>::to_int64' requested here
  inline __attribute__((always_inline)) operator long() const { return (long)to_int64(); }
                                                                             ^
./layer.h:324:28: note: in instantiation of member function 'ap_fixed_base<49, 33, true, 5, 3, 0>::operator long' requested here
        if (arr[mid] < arr[mid + 1]) left = mid + 1;
                           ^
./layer.h:340:17: note: in instantiation of function template specialization 'find_max<3>' requested here
      max_val = find_max<C>(input[i][j]);
                ^
attention.cpp:198:3: note: in instantiation of function template specialization 'softmax<1, 4, 3>' requested here
  softmax<SEQ_LEN, NUM_HEADS, CACHE_SIZE_INIT+1>(attn_weights);
  ^
attention.cpp:31:3: note: in instantiation of function template specialization 'attention<2, 1, 16, 16, 4, 4>' requested here
  attention<
  ^
2 warnings and 2 errors generated.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
ERROR: [HLS 200-70] Compilation errors found: In file included from attention.cpp:1:
In file included from attention.cpp:7:
./layer.h:321:20: error: member reference type 'attn_fixed_t *' (aka 'ap_fixed<40, 24> *') is a pointer; maybe you meant to use '->'?
    int right = arr.size() - 1;
                ~~~^
                   ->
./layer.h:321:21: error: no member named 'size' in 'ap_fixed<40, 24, 5, 3, 0>'
    int right = arr.size() - 1;
                ~~~ ^
In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
In file included from ./model.h:9:
In file included from ./typedefs.h:9:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:367:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed.h:55:
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:838:13: warning: shift count is negative [-Wshift-count-negative]
      ret.V <<= (_AP_I - _AP_W);
            ^   ~~~~~~~~~~~~~~~
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int_base.h:359:18: note: in instantiation of member function 'ap_fixed_base<40, 24, true, 5, 3, 0>::to_ap_int_base' requested here
    Base::V = op.to_ap_int_base().V;
                 ^
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:108:9: note: in instantiation of function template specialization 'ap_int_base<32, true>::ap_int_base<40, 24, true, 5, 3, 0>' requested here
      : Base((ap_fixed_base<_AP_W2, _AP_I2, true, _AP_Q2, _AP_O2, _AP_N2>)op) {}
        ^
./layer.h:137:38: note: in instantiation of function template specialization 'ap_int<32>::ap_int<40, 24, 5, 3, 0>' requested here
          sbit32_t quantized_value = attention_round(input[i][j] * scale);
                                     ^
attention.cpp:106:3: note: in instantiation of function template specialization 'quantize_activation<1, 16>' requested here
  quantize_activation<SEQ_LEN, HS_COLS>(
  ^
attention.cpp:31:3: note: in instantiation of function template specialization 'attention<2, 1, 16, 16, 4, 4>' requested here
  attention<
  ^
1 warning and 2 errors generated.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294145 ; free virtual = 370384
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294145 ; free virtual = 370384
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:55 ; elapsed = 00:01:00 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294054 ; free virtual = 370307
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:325) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:56 ; elapsed = 00:01:01 . Memory (MB): peak = 1235.207 ; gain = 597.184 ; free physical = 294002 ; free virtual = 370261
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:290) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:322) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:238) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:322) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:325) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:327) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:239) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:130) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: ==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
ERROR: [HLS 200-70] Compilation errors found: In file included from attention.cpp:1:
In file included from attention.cpp:7:
./layer.h:321:20: error: member reference type 'attn_fixed_t *' (aka 'ap_fixed<40, 24> *') is a pointer; maybe you meant to use '->'?
    int right = arr.size() - 1;
                ~~~^
                   ->
./layer.h:321:21: error: no member named 'size' in 'ap_fixed<40, 24, 5, 3, 0>'
    int right = arr.size() - 1;
                ~~~ ^
In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
In file included from ./model.h:9:
In file included from ./typedefs.h:9:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:367:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed.h:55:
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:838:13: warning: shift count is negative [-Wshift-count-negative]
      ret.V <<= (_AP_I - _AP_W);
            ^   ~~~~~~~~~~~~~~~
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int_base.h:359:18: note: in instantiation of member function 'ap_fixed_base<40, 24, true, 5, 3, 0>::to_ap_int_base' requested here
    Base::V = op.to_ap_int_base().V;
                 ^
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:108:9: note: in instantiation of function template specialization 'ap_int_base<32, true>::ap_int_base<40, 24, true, 5, 3, 0>' requested here
      : Base((ap_fixed_base<_AP_W2, _AP_I2, true, _AP_Q2, _AP_O2, _AP_N2>)op) {}
        ^
./layer.h:137:38: note: in instantiation of function template specialization 'ap_int<32>::ap_int<40, 24, 5, 3, 0>' requested here
          sbit32_t quantized_value = attention_round(input[i][j] * scale);
                                     ^
attention.cpp:106:3: note: in instantiation of function template specialization 'quantize_activation<1, 16>' requested here
  quantize_activation<SEQ_LEN, HS_COLS>(
  ^
attention.cpp:31:3: note: in instantiation of function template specialization 'attention<2, 1, 16, 16, 4, 4>' requested here
  attention<
  ^
1 warning and 2 errors generated.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294159 ; free virtual = 370399
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294159 ; free virtual = 370399
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:54 ; elapsed = 00:00:58 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294068 ; free virtual = 370322
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'find_max<16>130' (./layer.h:125) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<16>130' into 'quantize_activation<1, 16>118' (./layer.h:155) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>118' (./layer.h:156) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>118' (./layer.h:162) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<3>' into 'softmax<1, 4, 3>' (./layer.h:348) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:55 ; elapsed = 00:00:59 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 294019 ; free virtual = 370279
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:315) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:230) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:242) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:298) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:347) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:315) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:316) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:298) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:277) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:263) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:229) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:241) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:208) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:181) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:182) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:152) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:301) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:347) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:350) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:315) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:316) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:301) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:278) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:264) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:229) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:241) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:208) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:181) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:183) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:186) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:152) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:161) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:226) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:227) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'find_max<16>' (./layer.h:125) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<16>' into 'quantize_activation<1, 16>' (./layer.h:155) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:156) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:162) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<3>' into 'softmax<1, 4, 3>' (./layer.h:348) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights[0].V' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights[1].V' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights[2].V' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[0].V' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[1].V' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[2].V' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[0].V' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[1].V' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[2].V' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:128:14) to (./layer.h:167:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:182:68) to (./layer.h:182:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:263:8) to (./layer.h:262:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:230:69) to (./layer.h:230:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:175)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:59 ; elapsed = 00:01:05 . Memory (MB): peak = 1299.219 ; gain = 661.195 ; free physical = 293940 ; free virtual = 370209
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:276:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:209:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:262:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:296:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:296:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:275:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:208:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:181:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:261:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:228:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:293:39)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:296:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:278:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:194:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:264:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:02 ; elapsed = 00:01:07 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293807 ; free virtual = 370079
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 67.76 seconds; current allocated memory: 444.353 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 446.946 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 447.854 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 448.393 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 449.890 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 451.546 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 451.928 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 451.994 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 452.788 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 453.929 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 454.373 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 454.745 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 455.991 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 457.735 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 458.188 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 458.533 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 458.760 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 459.052 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 459.623 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 460.581 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 461.277 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 462.645 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 463.183 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 463.857 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 464.680 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.48 seconds; current allocated memory: 468.595 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.41 seconds; current allocated memory: 469.899 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 470.509 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 475.947 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.98 seconds; current allocated memory: 487.054 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.95 seconds; current allocated memory: 492.083 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.41 seconds; current allocated memory: 499.107 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 501.241 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 506.091 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 512.512 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 520.548 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 522.168 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_1_1_1': 12 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 525.263 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 27 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 1.16 seconds; current allocated memory: 531.304 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.19 seconds; current allocated memory: 537.246 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.9 seconds; current allocated memory: 546.892 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294099 ; free virtual = 370340
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294099 ; free virtual = 370340
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:55 ; elapsed = 00:01:00 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294008 ; free virtual = 370262
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'find_max<16>130' (./layer.h:125) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<16>130' into 'quantize_activation<1, 16>118' (./layer.h:155) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>118' (./layer.h:156) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>118' (./layer.h:162) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<3>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:56 ; elapsed = 00:01:01 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 293959 ; free virtual = 370219
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:315) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:230) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:242) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:298) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:328) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:315) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:316) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:298) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:277) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:263) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:229) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:241) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:208) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:181) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:182) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:152) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:301) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:331) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:337) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:315) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:316) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:301) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:278) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:264) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:229) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:241) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:208) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:181) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:183) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:186) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:152) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:161) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:226) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:227) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'find_max<16>' (./layer.h:125) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<16>' into 'quantize_activation<1, 16>' (./layer.h:155) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:156) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:162) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<3>' into 'softmax<1, 4, 3>' (./layer.h:329) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights[0].V' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights[1].V' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights[2].V' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[0].V' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[1].V' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[2].V' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[0].V' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[1].V' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights[2].V' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:128:14) to (./layer.h:167:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:182:68) to (./layer.h:182:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:263:8) to (./layer.h:262:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:230:69) to (./layer.h:230:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:175)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:00 ; elapsed = 00:01:06 . Memory (MB): peak = 1299.215 ; gain = 661.191 ; free physical = 293876 ; free virtual = 370145
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:276:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:209:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:262:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:296:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:296:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:275:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:208:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:181:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:261:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:228:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:293:39)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:296:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:278:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:194:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:264:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:03 ; elapsed = 00:01:09 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293742 ; free virtual = 370015
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 69.54 seconds; current allocated memory: 443.166 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 445.759 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 446.683 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 447.207 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 448.724 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 450.382 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 450.749 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 450.816 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 451.609 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 452.769 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 453.210 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 453.582 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 454.829 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 456.571 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 457.024 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 457.368 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 457.590 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 457.885 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 458.441 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 459.396 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 459.914 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 460.851 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 461.437 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 462.110 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 462.970 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.34 seconds; current allocated memory: 466.897 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 468.272 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 468.827 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 474.271 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.95 seconds; current allocated memory: 485.374 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 490.376 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.35 seconds; current allocated memory: 497.387 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 499.460 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 504.279 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 510.782 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 518.800 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 520.479 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_1_1_1': 12 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 523.465 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 3 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 528.913 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 532.957 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294451 ; free virtual = 370833
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294451 ; free virtual = 370833
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:55 ; elapsed = 00:01:00 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294359 ; free virtual = 370754
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'find_max<16>126' (./layer.h:125) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<16>126' into 'quantize_activation<1, 16>114' (./layer.h:155) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>114' (./layer.h:156) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>114' (./layer.h:162) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:55 ; elapsed = 00:01:01 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 294311 ; free virtual = 370712
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:315) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:230) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:242) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:298) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:328) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:315) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:316) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:298) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:277) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:263) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:229) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:241) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:208) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:181) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:182) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:152) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:301) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:337) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:343) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:315) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:316) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:297) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:301) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:278) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:264) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:229) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:241) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:208) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:181) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:183) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:186) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:152) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:161) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:226) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:227) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:226) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:227) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:226) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:227) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:226) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:227) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'find_max<16>' (./layer.h:125) automatically.
INFO: [XFORM 203-602] Inlining function 'find_max<16>' into 'quantize_activation<1, 16>' (./layer.h:155) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:156) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:162) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:264:9).
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:128:14) to (./layer.h:167:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:182:68) to (./layer.h:182:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:263:8) to (./layer.h:262:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:230:69) to (./layer.h:230:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:175)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:00 ; elapsed = 00:01:06 . Memory (MB): peak = 1299.223 ; gain = 661.199 ; free physical = 294228 ; free virtual = 370638
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:276:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:209:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:262:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:296:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:296:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:275:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:208:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:1)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:181:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:261:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:228:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:296:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:296:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:278:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:194:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:264:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:02 ; elapsed = 00:01:09 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 294095 ; free virtual = 370509
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 69.41 seconds; current allocated memory: 442.813 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 445.406 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 446.329 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 446.853 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 448.369 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 450.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 450.395 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 450.461 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 451.254 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 452.414 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 452.856 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 453.229 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 454.474 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 456.221 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 456.673 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 457.015 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 457.241 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 457.536 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 457.974 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 458.654 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 459.167 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 460.106 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 460.693 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 461.365 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 462.264 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.34 seconds; current allocated memory: 466.275 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.28 seconds; current allocated memory: 467.634 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 468.187 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 473.618 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.67 seconds; current allocated memory: 484.800 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 489.768 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 496.907 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 498.984 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 503.833 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 510.284 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 518.286 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 519.965 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 522.284 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 3 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 526.776 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 530.781 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.46 seconds; current allocated memory: 540.655 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
ERROR: [HLS 200-70] Compilation errors found: In file included from attention.cpp:1:
In file included from attention.cpp:7:
./layer.h:128:30: error: use of undeclared identifier 'find_max'
      attn_fixed_t max_val = find_max<C>(input[i], 1);
                             ^
In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
In file included from ./model.h:9:
In file included from ./typedefs.h:9:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:367:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed.h:55:
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:838:13: warning: shift count is negative [-Wshift-count-negative]
      ret.V <<= (_AP_I - _AP_W);
            ^   ~~~~~~~~~~~~~~~
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int_base.h:359:18: note: in instantiation of member function 'ap_fixed_base<40, 24, true, 5, 3, 0>::to_ap_int_base' requested here
    Base::V = op.to_ap_int_base().V;
                 ^
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:108:9: note: in instantiation of function template specialization 'ap_int_base<32, true>::ap_int_base<40, 24, true, 5, 3, 0>' requested here
      : Base((ap_fixed_base<_AP_W2, _AP_I2, true, _AP_Q2, _AP_O2, _AP_N2>)op) {}
        ^
./layer.h:135:38: note: in instantiation of function template specialization 'ap_int<32>::ap_int<40, 24, 5, 3, 0>' requested here
          sbit32_t quantized_value = attention_round(input[i][j] * scale);
                                     ^
attention.cpp:106:3: note: in instantiation of function template specialization 'quantize_activation<1, 16>' requested here
  quantize_activation<SEQ_LEN, HS_COLS>(
  ^
attention.cpp:31:3: note: in instantiation of function template specialization 'attention<2, 1, 16, 16, 4, 4>' requested here
  attention<
  ^
1 warning and 1 error generated.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
ERROR: [HLS 200-70] Compilation errors found: In file included from attention.cpp:1:
In file included from attention.cpp:7:
./layer.h:129:58: error: use of undeclared identifier 'k'
      QUANTIZE_ACTIVATION_LOOP_2: for (int j = 1; j < C; k++)
                                                         ^
In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
In file included from ./model.h:9:
In file included from ./typedefs.h:9:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:367:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed.h:55:
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:838:13: warning: shift count is negative [-Wshift-count-negative]
      ret.V <<= (_AP_I - _AP_W);
            ^   ~~~~~~~~~~~~~~~
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int_base.h:359:18: note: in instantiation of member function 'ap_fixed_base<40, 24, true, 5, 3, 0>::to_ap_int_base' requested here
    Base::V = op.to_ap_int_base().V;
                 ^
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:108:9: note: in instantiation of function template specialization 'ap_int_base<32, true>::ap_int_base<40, 24, true, 5, 3, 0>' requested here
      : Base((ap_fixed_base<_AP_W2, _AP_I2, true, _AP_Q2, _AP_O2, _AP_N2>)op) {}
        ^
./layer.h:137:38: note: in instantiation of function template specialization 'ap_int<32>::ap_int<40, 24, 5, 3, 0>' requested here
          sbit32_t quantized_value = attention_round(input[i][j] * scale);
                                     ^
attention.cpp:106:3: note: in instantiation of function template specialization 'quantize_activation<1, 16>' requested here
  quantize_activation<SEQ_LEN, HS_COLS>(
  ^
attention.cpp:31:3: note: in instantiation of function template specialization 'attention<2, 1, 16, 16, 4, 4>' requested here
  attention<
  ^
1 warning and 1 error generated.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:52 ; elapsed = 00:00:55 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294363 ; free virtual = 370747
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:52 ; elapsed = 00:00:55 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294363 ; free virtual = 370747
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:54 ; elapsed = 00:00:57 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294273 ; free virtual = 370672
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:54 ; elapsed = 00:00:58 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 294224 ; free virtual = 370628
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:290) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:238) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:308) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:314) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:239) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:306:9) to (./layer.h:305:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:132:28) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:238:8) to (./layer.h:237:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:59 ; elapsed = 00:01:04 . Memory (MB): peak = 1299.223 ; gain = 661.199 ; free physical = 294147 ; free virtual = 370561
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:251:5) in function 'transpose_last_two_dims<3, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:237:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:250:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:271:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:271:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:253:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:01 ; elapsed = 00:01:06 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 294014 ; free virtual = 370431
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 66.86 seconds; current allocated memory: 441.889 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 444.485 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 445.391 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 445.915 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 447.403 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 449.021 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 449.388 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 449.453 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 450.209 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 451.391 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 451.850 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 452.223 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 453.466 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 455.213 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 455.611 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 455.956 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 456.201 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 456.493 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 456.949 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 457.590 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 458.119 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 459.017 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 459.588 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 460.259 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 461.121 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.42 seconds; current allocated memory: 465.168 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.34 seconds; current allocated memory: 466.471 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 467.040 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.19 seconds; current allocated memory: 472.486 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.98 seconds; current allocated memory: 483.622 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 488.523 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.38 seconds; current allocated memory: 495.320 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 497.453 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 502.250 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 508.716 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 516.720 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 518.427 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 520.727 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 524.999 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 528.876 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 2.17 seconds; current allocated memory: 538.726 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294334 ; free virtual = 370719
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:53 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294334 ; free virtual = 370719
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:55 ; elapsed = 00:01:00 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294242 ; free virtual = 370640
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:56 ; elapsed = 00:01:01 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 294194 ; free virtual = 370598
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:290) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:238) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:308) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:314) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:239) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:306:10) to (./layer.h:305:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:132:28) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:238:8) to (./layer.h:237:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:00 ; elapsed = 00:01:07 . Memory (MB): peak = 1299.219 ; gain = 661.195 ; free physical = 294115 ; free virtual = 370529
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:251:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:302:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:237:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:250:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:271:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:271:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:253:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:03 ; elapsed = 00:01:09 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293986 ; free virtual = 370403
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 69.88 seconds; current allocated memory: 442.069 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 444.647 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 445.567 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 446.091 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 447.592 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 449.232 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 449.600 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 449.665 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 450.452 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 451.647 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 452.088 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 452.461 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 453.706 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 455.451 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 455.870 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 456.252 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 456.473 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 456.767 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 457.202 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 457.845 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 458.388 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 459.285 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 459.869 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 460.545 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 461.403 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.4 seconds; current allocated memory: 465.449 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 466.755 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 467.361 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.19 seconds; current allocated memory: 472.738 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.97 seconds; current allocated memory: 483.887 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 488.917 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 495.719 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 497.821 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 502.641 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 509.079 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 517.112 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 518.792 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 521.158 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 525.428 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 529.314 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:54 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294351 ; free virtual = 370736
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:54 ; elapsed = 00:00:58 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294351 ; free virtual = 370736
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:56 ; elapsed = 00:01:00 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294260 ; free virtual = 370658
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:56 ; elapsed = 00:01:01 . Memory (MB): peak = 1235.211 ; gain = 597.188 ; free physical = 294212 ; free virtual = 370616
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:290) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:238) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:308) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:314) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:239) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'input.V' (attention.cpp:21) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
INFO: [XFORM 203-102] Automatically partitioning small array 'input[0].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[1].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[2].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[3].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[4].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[5].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[6].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[7].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[8].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[9].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[10].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[11].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[12].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[13].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[14].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'input[15].V' (attention.cpp:21) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[0].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[1].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[2].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[3].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[4].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[5].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[6].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[7].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[8].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[9].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[10].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[11].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[12].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[13].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[14].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output_2D[15].V' (attention.cpp:215) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'input[0].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[1].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[2].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[3].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[4].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[5].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[6].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[7].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[8].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[9].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[10].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[11].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[12].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[13].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[14].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'input[15].V' (attention.cpp:21) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[0].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[1].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[2].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[3].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[4].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[5].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[6].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[7].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[8].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[9].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[10].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[11].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[12].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[13].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[14].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output_2D[15].V' (attention.cpp:215) in dimension 1 completely.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:306:10) to (./layer.h:305:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:132:28) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:238:8) to (./layer.h:237:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:01 ; elapsed = 00:01:08 . Memory (MB): peak = 1235.227 ; gain = 597.203 ; free physical = 294125 ; free virtual = 370539
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:251:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:302:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:237:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:250:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:61)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >.1' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem.1' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:271:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:271:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:253:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem.V.0' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:04 ; elapsed = 00:01:11 . Memory (MB): peak = 1488.148 ; gain = 850.125 ; free physical = 293967 ; free virtual = 370384
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'init_2d_mem.1' to 'init_2d_mem_1'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 71.17 seconds; current allocated memory: 454.937 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 457.536 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
WARNING: [SCHED 204-68] The II Violation in module 'rms_norm_16_s' (Loop: RMS_NORM_LOOP_2): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 0)
   between 'mux' operation ('input_V_1', ./layer.h:94) and 'mux' operation ('tmp_7', ./layer.h:94).
WARNING: [SCHED 204-68] The II Violation in module 'rms_norm_16_s' (Loop: RMS_NORM_LOOP_2): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 0)
   between 'mux' operation ('input_V_1', ./layer.h:94) and 'mux' operation ('tmp_7', ./layer.h:94).
WARNING: [SCHED 204-68] The II Violation in module 'rms_norm_16_s' (Loop: RMS_NORM_LOOP_2): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 0)
   between 'mux' operation ('input_V_1', ./layer.h:94) and 'mux' operation ('tmp_7', ./layer.h:94).
WARNING: [SCHED 204-68] The II Violation in module 'rms_norm_16_s' (Loop: RMS_NORM_LOOP_2): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 0)
   between 'mux' operation ('input_V_1', ./layer.h:94) and 'mux' operation ('tmp_7', ./layer.h:94).
WARNING: [SCHED 204-68] The II Violation in module 'rms_norm_16_s' (Loop: RMS_NORM_LOOP_2): Unable to enforce a carried dependence constraint (II = 5, distance = 1, offset = 0)
   between 'mux' operation ('input_V_1', ./layer.h:94) and 'mux' operation ('tmp_7', ./layer.h:94).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 6, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.58 seconds; current allocated memory: 458.654 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 459.745 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 461.348 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 463.135 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 463.518 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 463.582 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 464.375 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 465.519 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 465.976 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 466.346 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 467.575 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 469.319 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 469.738 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 470.120 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 470.337 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 470.631 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 471.088 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 471.766 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 472.268 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 473.167 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 473.733 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 474.403 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 474.576 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 475.033 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 1, Depth = 1.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 476.101 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.79 seconds; current allocated memory: 480.553 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.56 seconds; current allocated memory: 481.971 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 482.908 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.34 seconds; current allocated memory: 488.368 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_72s_72_5_1' to 'dut_mul_40s_72s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_72s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_40_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 2.1 seconds; current allocated memory: 501.555 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 507.988 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem_1'.
INFO: [HLS 200-111]  Elapsed time: 1.47 seconds; current allocated memory: 515.464 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 517.570 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 522.420 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 528.844 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.33 seconds; current allocated memory: 536.875 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 538.528 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 540.911 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 545.184 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 549.089 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_1_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 553.044 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 37 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.91 seconds; current allocated memory: 563.036 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
WARNING: [RTGEN 206-101] RTL name 'dut_mux_164_40_1_1' is changed to 'dut_mux_164_40_1_1_x' due to conflict.
INFO: [RTGEN 206-100] Generating core module 'dut_mux_164_40_1_1_x': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.55 seconds; current allocated memory: 572.776 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nsbkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_72s_7cud_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40ndEe_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61seOg_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_fYi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40shbi_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigibs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigibs_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighmb6_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cachevdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cachevdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cachewdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cachewdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cachexdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cachexdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cacheyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cacheyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cachezec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cachezec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cacheAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cacheAem_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cacheBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cacheBew_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cacheCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cacheCeG_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigDeQ_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighEe0_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighFfa_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighGfk_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighHfu' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294317 ; free virtual = 370702
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:52 ; elapsed = 00:00:56 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 294317 ; free virtual = 370702
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:26) in function 'void init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:54 ; elapsed = 00:00:59 . Memory (MB): peak = 1171.652 ; gain = 533.629 ; free physical = 294235 ; free virtual = 370633
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>110' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>110' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>110' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:89: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:55 ; elapsed = 00:00:59 . Memory (MB): peak = 1235.207 ; gain = 597.184 ; free physical = 294186 ; free virtual = 370591
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:290) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:205) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:217) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:219) in function 'attention<2, 1, 16, 16, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:273) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:252) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'CACHE_UPDATE_LOOP_2' (./layer.h:238) in function 'cache_update<4, 2, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:157) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:91) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:187) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:194) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:195) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:218) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:220) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:224) in function 'attention<2, 1, 16, 16, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:303) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_4' (./layer.h:308) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:314) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:290) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:291) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:276) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:253) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'CACHE_UPDATE_LOOP_3' (./layer.h:239) in function 'cache_update<4, 2, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:204) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:216) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:183) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:156) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:161) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:125) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_3' (./layer.h:136) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:38) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:101) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:190) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:233) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:190) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache.V'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:100) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:144) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:153) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:158) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:167) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:171) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:232) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:201) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:202) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15' (attention.cpp:100) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:144) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:153) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:171) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15' (attention.cpp:232) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:201) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:202) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15' (attention.cpp:100) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15' (attention.cpp:232) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:144) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:153) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:171) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:201) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:202) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:114) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:215) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<16>' (./layer.h:92) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:137) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:131) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4>' (attention.cpp:191) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[0].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[1].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[2].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'cache_in[3].V' in function 'cache_update<4, 2, 4>' (./layer.h:239:9).
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:306:10) to (./layer.h:305:46) in function 'softmax<1, 4, 3>'... converting 4 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:132:28) to (./layer.h:142:1) in function 'quantize_activation<1, 16>'... converting 65 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:157:68) to (./layer.h:157:62) in function 'linear_forward_no_mul<1, 16, 16>'... converting 33 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:238:8) to (./layer.h:237:51) in function 'cache_update<4, 2, 4>'... converting 13 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:205:69) to (./layer.h:205:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 16, 16>' (./layer.h:150)...16 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:59 ; elapsed = 00:01:05 . Memory (MB): peak = 1299.219 ; gain = 661.195 ; free physical = 294104 ; free virtual = 370518
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:251:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:302:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:184:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'CACHE_UPDATE_LOOP_1' (./layer.h:237:5) in function 'cache_update<4, 2, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:271:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:250:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:183:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:60:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:156:62)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:38:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:236:38)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4>' to 'attention' (attention.cpp:92:57)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:203:106)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:271:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:271:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:253:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:94:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:169:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:39:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:220:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:02 ; elapsed = 00:01:08 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 293981 ; free virtual = 370397
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 68.27 seconds; current allocated memory: 442.053 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 444.647 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 445.571 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 446.096 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 447.595 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 449.234 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 449.604 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 449.669 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 80.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 450.455 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 451.636 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 452.094 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 452.466 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 453.694 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 455.441 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'CACHE_UPDATE_LOOP_1_CACHE_UPDATE_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 455.875 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 456.258 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 456.463 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 456.757 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 457.210 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 457.853 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 458.393 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 459.293 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 459.862 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 460.537 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 461.395 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.43 seconds; current allocated memory: 465.439 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.4 seconds; current allocated memory: 466.762 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 467.369 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 472.747 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nsbkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nsbkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 1.96 seconds; current allocated memory: 483.884 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40ndEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40ndEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 488.930 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.37 seconds; current allocated memory: 495.824 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61seOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61seOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 497.927 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 502.777 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 509.199 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.22 seconds; current allocated memory: 517.184 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 518.863 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_40_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 521.233 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 15 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40shbi': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 525.486 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_40_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 529.371 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_0' to 'attention_k_cachevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_1' to 'attention_k_cachewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_2' to 'attention_k_cachexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_V_3' to 'attention_k_cacheyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_0' to 'attention_v_cachezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_1' to 'attention_v_cacheAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_2' to 'attention_v_cacheBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_V_3' to 'attention_v_cacheCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cachePgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Thq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_UhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_VhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouWhU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_40_1_1': 1 instance(s).
