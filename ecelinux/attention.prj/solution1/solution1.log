==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:21 ; elapsed = 00:00:24 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 308733 ; free virtual = 381958
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:21 ; elapsed = 00:00:24 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 308733 ; free virtual = 381958
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:24) in function 'void init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:22 ; elapsed = 00:00:26 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 308703 ; free virtual = 381932
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>122' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_exp' into 'softmax<1, 4, 3>' (./layer.h:323) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
WARNING: [SYNCHK 200-120] ./layer.h:87: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 1 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:22 ; elapsed = 00:00:26 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 308701 ; free virtual = 381931
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:286) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:200) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:212) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'attention_exp' (./layer.h:300:61).
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:222) in function 'attention<2, 1, 16, 16, 4, 4, 2>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:248) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:154) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTENTION_EXP_LOOP_1' (./layer.h:300) in function 'attention_exp' completely with a factor of 5.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:223) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:320) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:327) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:249) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:155) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:128) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:101) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:102) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:193) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:235) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:236) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:193) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:147) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:155) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:160) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:169) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:173) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:196) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:197) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:117) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:218) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:134:64) to (./layer.h:134:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:200:69) to (./layer.h:200:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:25 ; elapsed = 00:00:29 . Memory (MB): peak = 1168.148 ; gain = 530.125 ; free physical = 308662 ; free virtual = 381895
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:247:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:316:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:179:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:246:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:178:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:58:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:153:41)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:231:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4, 2>' to 'attention' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:198:78)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:249:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:92:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:137:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:160:36)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:161:41)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:21)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:235:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:234:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:223:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:27 ; elapsed = 00:00:31 . Memory (MB): peak = 1296.148 ; gain = 658.125 ; free physical = 308543 ; free virtual = 381776
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 31.37 seconds; current allocated memory: 303.455 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 304.253 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 304.952 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 305.787 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 305.932 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 305.998 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('output_0_V_load_3', ./layer.h:161) on array 'output_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 307.404 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 308.994 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 309.421 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 309.795 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 311.039 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 312.784 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 313.220 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 313.591 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('output_1_V_addr_2_write_ln249', ./layer.h:249) of variable 'input_2_V_load', ./layer.h:249 on array 'output_1_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_1_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 313.854 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 314.148 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 314.604 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 315.246 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'attention_exp'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 11.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 315.555 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 315.976 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 12, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 13, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 316.726 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 318.297 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 318.895 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 319.603 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('attn_output_2D_0_V_2_write_ln223', attention.cpp:223) of variable 'tmp_6', attention.cpp:223 on array 'attn_output_2D[0].V', attention.cpp:218 due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'attn_output_2D_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 320.457 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 323.039 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 323.708 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 324.018 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_53s_32s_53_57_seq_1' to 'dut_sdiv_53s_32s_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_46ns_32s_32_50_seq_1' to 'dut_sdiv_46ns_32scud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_64s_32s_76_5_1' to 'dut_mul_64s_32s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_64s_32s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_46ns_32scud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_53s_32s_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 326.270 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_51s_31ns_32_55_seq_1' to 'dut_udiv_51s_31nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_51s_31nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 329.865 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 333.109 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_76ns_59s_32_80_1' to 'dut_sdiv_76ns_59sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_76ns_59sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 336.138 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 343.957 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 350.439 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 358.404 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 360.271 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_32_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 362.575 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention_exp'.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 365.831 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_54ns_32s_32_58_seq_1' to 'dut_sdiv_54ns_32sibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_54ns_32sibs': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 369.942 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 375.416 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_0' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_1' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_2' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_3' to 'attention_q_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_0' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_1' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_2' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_3' to 'attention_k_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_0' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_1' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_2' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_3' to 'attention_v_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_0' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_1' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_2' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_3' to 'attention_o_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_OgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_PgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizRg6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.1 seconds; current allocated memory: 384.360 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 391.059 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_53s_32s_bkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_46ns_32scud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_64s_32s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_51s_31nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_76ns_59sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_54ns_32sibs_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighmb6_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighAem_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_quantizBew_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_CeG_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheFfa_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cacheJfO_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:41 ; elapsed = 00:00:55 . Memory (MB): peak = 1424.148 ; gain = 786.125 ; free physical = 308398 ; free virtual = 381674
INFO: [VHDL 208-304] Generating VHDL RTL for dut.
INFO: [VLOG 209-307] Generating Verilog RTL for dut.
INFO: [COSIM 212-47] Using XSIM for RTL simulation.
INFO: [COSIM 212-14] Instrumenting C test bench ...
INFO: [COSIM 212-302] Starting C TB testing ... 
INFO: [COSIM 212-333] Generating C post check test bench ...
INFO: [COSIM 212-12] Generating RTL test bench ...
INFO: [COSIM 212-1] *** C/RTL co-simulation file generation completed. ***
INFO: [COSIM 212-323] Starting verilog simulation. 
INFO: [COSIM 212-15] Starting XSIM ...
INFO: ==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:21 ; elapsed = 00:00:25 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308732 ; free virtual = 381958
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:21 ; elapsed = 00:00:25 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308732 ; free virtual = 381958
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:24) in function 'void init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:22 ; elapsed = 00:00:26 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308703 ; free virtual = 381932
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>122' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_exp' into 'softmax<1, 4, 3>' (./layer.h:323) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
WARNING: [SYNCHK 200-120] ./layer.h:87: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 1 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:22 ; elapsed = 00:00:27 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308701 ; free virtual = 381931
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:286) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:200) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:212) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'attention_exp' (./layer.h:300:61).
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:222) in function 'attention<2, 1, 16, 16, 4, 4, 2>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:248) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:154) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTENTION_EXP_LOOP_1' (./layer.h:300) in function 'attention_exp' completely with a factor of 5.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:223) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:320) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:327) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:249) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:155) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:128) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:101) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:102) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:193) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:235) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:236) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:193) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:147) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:155) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:160) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:169) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:173) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:196) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:197) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:117) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:218) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:134:64) to (./layer.h:134:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:200:69) to (./layer.h:200:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:25 ; elapsed = 00:00:30 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 308664 ; free virtual = 381897
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:247:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:316:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:179:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:246:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:178:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:58:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:153:41)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:231:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4, 2>' to 'attention' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:198:78)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:249:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:92:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:137:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:160:36)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:161:41)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:21)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:235:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:234:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:223:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:27 ; elapsed = 00:00:32 . Memory (MB): peak = 1296.152 ; gain = 658.125 ; free physical = 308544 ; free virtual = 381777
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 32.23 seconds; current allocated memory: 303.487 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 304.282 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 305.001 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 305.836 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 305.963 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 306.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('output_0_V_load_3', ./layer.h:161) on array 'output_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 307.464 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 309.054 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 309.496 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 309.871 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 311.114 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 312.871 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 313.292 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 313.664 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('output_1_V_addr_2_write_ln249', ./layer.h:249) of variable 'input_2_V_load', ./layer.h:249 on array 'output_1_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_1_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 313.928 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 314.217 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 314.677 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 315.316 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'attention_exp'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 11.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 315.626 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 316.049 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 12, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 13, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.87 seconds; current allocated memory: 316.785 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 318.339 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 318.954 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 319.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('attn_output_2D_0_V_2_write_ln223', attention.cpp:223) of variable 'tmp_6', attention.cpp:223 on array 'attn_output_2D[0].V', attention.cpp:218 due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'attn_output_2D_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 320.520 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 323.099 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 323.738 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 324.064 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_53s_32s_53_57_seq_1' to 'dut_sdiv_53s_32s_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_46ns_32s_32_50_seq_1' to 'dut_sdiv_46ns_32scud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_64s_32s_76_5_1' to 'dut_mul_64s_32s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_64s_32s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_46ns_32scud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_53s_32s_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 326.315 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_51s_31ns_32_55_seq_1' to 'dut_udiv_51s_31nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_51s_31nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 329.912 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 333.185 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_76ns_59s_32_80_1' to 'dut_sdiv_76ns_59sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_76ns_59sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 336.231 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 344.004 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 350.469 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 358.512 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 360.304 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_32_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 362.682 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention_exp'.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 365.880 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_54ns_32s_32_58_seq_1' to 'dut_sdiv_54ns_32sibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_54ns_32sibs': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 370.047 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 375.478 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_0' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_1' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_2' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_3' to 'attention_q_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_0' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_1' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_2' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_3' to 'attention_k_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_0' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_1' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_2' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_3' to 'attention_v_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_0' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_1' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_2' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_3' to 'attention_o_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_OgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_PgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizRg6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.17 seconds; current allocated memory: 384.406 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 391.135 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_53s_32s_bkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_46ns_32scud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_64s_32s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_51s_31nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_76ns_59sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_54ns_32sibs_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighmb6_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighAem_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_quantizBew_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_CeG_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheFfa_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cacheJfO_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:42 ; elapsed = 00:00:57 . Memory (MB): peak = 1424.152 ; gain = 786.125 ; free physical = 308396 ; free virtual = 381671
INFO: [VHDL 208-304] Generating VHDL RTL for dut.
INFO: [VLOG 209-307] Generating Verilog RTL for dut.
INFO: [COSIM 212-47] Using XSIM for RTL simulation.
INFO: [COSIM 212-14] Instrumenting C test bench ...
INFO: [COSIM 212-302] Starting C TB testing ... 
INFO: [COSIM 212-333] Generating C post check test bench ...
INFO: [COSIM 212-12] Generating RTL test bench ...
INFO: [COSIM 212-1] *** C/RTL co-simulation file generation completed. ***
INFO: [COSIM 212-323] Starting verilog simulation. 
INFO: [COSIM 212-15] Starting XSIM ...
INFO: ==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:21 ; elapsed = 00:00:26 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 306060 ; free virtual = 379363
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:21 ; elapsed = 00:00:26 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 306060 ; free virtual = 379363
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:24) in function 'void init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:23 ; elapsed = 00:00:27 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 306027 ; free virtual = 379338
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>122' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_exp' into 'softmax<1, 4, 3>' (./layer.h:323) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
WARNING: [SYNCHK 200-120] ./layer.h:87: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 1 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:23 ; elapsed = 00:00:28 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 306022 ; free virtual = 379335
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:286) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:200) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:212) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'attention_exp' (./layer.h:300:61).
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:222) in function 'attention<2, 1, 16, 16, 4, 4, 2>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:248) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:154) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTENTION_EXP_LOOP_1' (./layer.h:300) in function 'attention_exp' completely with a factor of 5.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:223) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:317) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:320) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:327) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:249) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:155) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:128) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:101) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:102) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:193) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:235) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:236) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:193) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:147) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:155) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:160) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:169) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:173) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:196) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:197) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:117) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:218) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:320) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:134:64) to (./layer.h:134:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:200:69) to (./layer.h:200:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:26 ; elapsed = 00:00:31 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 305984 ; free virtual = 379299
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:247:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:316:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:179:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:246:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:178:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:58:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:153:41)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:231:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4, 2>' to 'attention' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:198:78)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:249:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:92:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:137:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:160:36)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:161:41)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:21)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:235:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:234:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:223:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:27 ; elapsed = 00:00:33 . Memory (MB): peak = 1296.152 ; gain = 658.125 ; free physical = 305862 ; free virtual = 379179
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 33.21 seconds; current allocated memory: 303.448 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 304.246 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 304.961 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 305.799 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 305.940 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 306.004 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('output_0_V_load_3', ./layer.h:161) on array 'output_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 307.438 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 309.032 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 309.461 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 309.831 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 311.092 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 312.839 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 313.258 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 313.630 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('output_1_V_addr_2_write_ln249', ./layer.h:249) of variable 'input_2_V_load', ./layer.h:249 on array 'output_1_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_1_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 313.898 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 314.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 314.643 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 315.279 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'attention_exp'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 11.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 315.578 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 316.003 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
WARNING: [SCHED 204-68] The II Violation in module 'softmax_1_4_3_s' (Loop: SOFTMAX_LOOP_4): Unable to enforce a carried dependence constraint (II = 12, distance = 1, offset = 1)
   between 'switch' operation ('_ln323', ./layer.h:323) and 'sub' operation ('sub_ln703', ./layer.h:323).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 13, Depth = 14.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 316.737 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 318.307 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_2_0_V_load_2', ./layer.h:272) on array 'input_2_0_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_2_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 318.923 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 319.631 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('attn_output_2D_0_V_2_write_ln223', attention.cpp:223) of variable 'tmp_6', attention.cpp:223 on array 'attn_output_2D[0].V', attention.cpp:218 due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'attn_output_2D_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 320.485 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 323.065 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 323.719 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 324.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_53s_32s_53_57_seq_1' to 'dut_sdiv_53s_32s_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_46ns_32s_32_50_seq_1' to 'dut_sdiv_46ns_32scud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_64s_32s_76_5_1' to 'dut_mul_64s_32s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_64s_32s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_46ns_32scud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_53s_32s_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 326.295 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_51s_31ns_32_55_seq_1' to 'dut_udiv_51s_31nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_51s_31nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 329.878 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 333.164 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_76ns_59s_32_80_1' to 'dut_sdiv_76ns_59sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_76ns_59sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 336.197 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 343.968 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 350.449 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 358.492 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 360.299 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_32_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 362.676 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention_exp'.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 365.874 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_54ns_32s_32_58_seq_1' to 'dut_sdiv_54ns_32sibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_54ns_32sibs': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 370.043 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 375.474 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_0' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_1' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_2' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_3' to 'attention_q_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_0' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_1' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_2' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_3' to 'attention_k_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_0' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_1' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_2' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_3' to 'attention_v_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_0' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_1' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_2' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_3' to 'attention_o_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_OgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_PgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizRg6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 384.402 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 391.130 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_53s_32s_bkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_46ns_32scud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_64s_32s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_51s_31nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_76ns_59sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_54ns_32sibs_div'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigjbC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigjbC_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighkbM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighkbM_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighlbW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighlbW_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighmb6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighmb6_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighncg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighncg_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighocq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighocq_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighpcA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighpcA_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighqcK_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighrcU_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighsc4_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weightde_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighudo_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighvdy_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigwdI_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighxdS_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighyd2_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighzec_rom' using distributed ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighAem_rom' using distributed ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_quantizBew_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_CeG_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cacheFfa_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cacheJfO_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:43 ; elapsed = 00:00:58 . Memory (MB): peak = 1424.152 ; gain = 786.125 ; free physical = 305725 ; free virtual = 379086
INFO: [VHDL 208-304] Generating VHDL RTL for dut.
INFO: [VLOG 209-307] Generating Verilog RTL for dut.
INFO: [COSIM 212-47] Using XSIM for RTL simulation.
INFO: [COSIM 212-14] Instrumenting C test bench ...
INFO: [COSIM 212-302] Starting C TB testing ... 
INFO: [COSIM 212-333] Generating C post check test bench ...
INFO: [COSIM 212-12] Generating RTL test bench ...
INFO: [COSIM 212-1] *** C/RTL co-simulation file generation completed. ***
INFO: [COSIM 212-323] Starting verilog simulation. 
INFO: [COSIM 212-15] Starting XSIM ...
INFO: 