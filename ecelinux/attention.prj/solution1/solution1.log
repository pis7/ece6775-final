==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:16 ; elapsed = 00:00:19 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 9124 ; free virtual = 24563
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:16 ; elapsed = 00:00:19 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 9124 ; free virtual = 24563
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:24) in function 'void init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >(FORWARD_REFERENCE*, FORWARD_REFERENCE)' completely with a factor of 1.
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:17 ; elapsed = 00:00:20 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 9008 ; free virtual = 24449
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>122' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>122' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:321) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_exp' into 'softmax<1, 4, 3>' (./layer.h:324) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
WARNING: [SYNCHK 200-120] ./layer.h:87: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 1 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:18 ; elapsed = 00:00:20 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 8997 ; free virtual = 24440
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:286) in function 'create_causal_mask<1>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:200) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:212) in function 'apply_rotary_pos_emb<1, 4, 4>'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >'.
WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >'.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:25) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:57) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'attention_exp' (./layer.h:300:61).
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ATTN_2D_LOOP_2' (attention.cpp:222) in function 'attention<2, 1, 16, 16, 4, 4, 2>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:318) in function 'softmax<1, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'GEMM_3D_FLOAT_LOOP_3' (./layer.h:269) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_2' (./layer.h:248) in function 'transpose_last_two_dims<3, 4, 4>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:154) in function 'linear_forward_no_mul<1, 16, 16>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:25) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:57) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTENTION_EXP_LOOP_1' (./layer.h:300) in function 'attention_exp' completely with a factor of 5.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:92) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:190) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:197) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:198) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:221) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_3' (attention.cpp:223) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:227) in function 'attention<2, 1, 16, 16, 4, 4, 2>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:318) in function 'softmax<1, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_3' (./layer.h:321) in function 'softmax<1, 4, 3>' completely with a factor of 2.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_5' (./layer.h:328) in function 'softmax<1, 4, 3>' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:286) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:287) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:268) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:272) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_3' (./layer.h:249) in function 'transpose_last_two_dims<3, 4, 4>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:199) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:211) in function 'apply_rotary_pos_emb<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:178) in function 'reshape_2D_to_3D<1, 4, 4>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:153) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:155) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:158) in function 'linear_forward_no_mul<1, 16, 16>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:123) in function 'quantize_activation<1, 16>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_2' (./layer.h:128) in function 'quantize_activation<1, 16>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:36) in function 'init_2d_mem<1, 16, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V' (attention.cpp:101) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:102) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:193) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V' (attention.cpp:235) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:236) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:193) automatically.
INFO: [XFORM 203-102] Partitioning array 'v_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'o_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_weights.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V' (attention.cpp:145) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V' (attention.cpp:146) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V' (attention.cpp:147) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:154) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V' (attention.cpp:155) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_cache_upd.V' (attention.cpp:159) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'v_cache_upd.V' (attention.cpp:160) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:169) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V' (attention.cpp:173) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output'  in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V' (./layer.h:196) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V' (./layer.h:197) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.0' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.1' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.2' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_proj.V.3' (attention.cpp:145) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.0' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.1' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.2' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_proj.V.3' (attention.cpp:146) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.0' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.1' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.2' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'v_proj.V.3' (attention.cpp:147) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.0' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.1' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.2' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'q_embed.V.3' (attention.cpp:154) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.0' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.1' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.2' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'k_embed.V.3' (attention.cpp:155) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.0' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.1' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_weights.V.2' (attention.cpp:173) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.0' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.1' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.2' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'attn_output.3' completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.0' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.1' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.2' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_q.V.3' (./layer.h:196) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.0' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.1' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.2' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'rotated_k.V.3' (./layer.h:197) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.0' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.1' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.2' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_proj.V.3' (attention.cpp:145) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.0' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.1' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.2' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_proj.V.3' (attention.cpp:146) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.0' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.1' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.2' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'v_proj.V.3' (attention.cpp:147) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.0' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.1' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.2' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V.3' (attention.cpp:154) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.0' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.1' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.2' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'k_embed.V.3' (attention.cpp:155) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.0' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.1' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_weights.V.2' (attention.cpp:173) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.0' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.1' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.2' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'attn_output.3' in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.0' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.1' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.2' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_q.V.3' (./layer.h:196) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.0' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.1' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.2' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'rotated_k.V.3' (./layer.h:197) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:115) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:116) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:117) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:218) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:21) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_sqrt' (./layer.h:70) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_sqrt' into 'rms_norm<16>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 16>' (./layer.h:128) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 16>' (./layer.h:135) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'attention_exp' (./layer.h:302) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'softmax<1, 4, 3>' (./layer.h:321) automatically.
INFO: [XFORM 203-602] Inlining function 'init_2d_mem<1, 16, ap_int<8> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:104) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:105) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<2, 1, 16, 16, 4, 4, 2>' (attention.cpp:194) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:134:64) to (./layer.h:134:58) in function 'quantize_activation<1, 16>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:200:69) to (./layer.h:200:63) in function 'apply_rotary_pos_emb<1, 4, 4>'... converting 65 basic blocks.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:20 ; elapsed = 00:00:23 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 8950 ; free virtual = 24395
INFO: [XFORM 203-541] Flattening a loop nest 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1' (./layer.h:247:5) in function 'transpose_last_two_dims<3, 4, 4>'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'SOFTMAX_LOOP_1' (./layer.h:317:48) in function 'softmax<1, 4, 3>' : 

the outer loop is not a perfect loop because there is nontrivial logic in the loop latch.
INFO: [XFORM 203-541] Flattening a loop nest 'RESHAPE_2D_TO_3D_LOOP_2' (./layer.h:179:7) in function 'reshape_2D_to_3D<1, 4, 4>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>'.
INFO: [XFORM 203-541] Flattening a loop nest 'GEMM_3D_FLOAT_LOOP_1' (./layer.h:267:55) in function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<3, 4, 4>' to 'transpose_last_two_d' (./layer.h:246:49)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 4, 4>' to 'reshape_2D_to_3D' (./layer.h:178:9)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 16>' to 'quantize_activation' (./layer.h:58:58)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 16, 16>' to 'linear_forward_no_mu' (./layer.h:153:41)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 16, ap_fixed<32, 10, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'cache_update<4, 2, 4>' to 'cache_update' (./layer.h:231:51)
WARNING: [XFORM 203-631] Renaming function 'attention<2, 1, 16, 16, 4, 4, 2>' to 'attention' (./layer.h:36:48)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 4, 4>' to 'apply_rotary_pos_emb' (./layer.h:198:78)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 4, 4, 4, 3>' to 'GEMM_3D_float' (./layer.h:267:30)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<4, 1, 3, 4, 3, 4>' to 'GEMM_3D_float.1' (./layer.h:267:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:249:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:92:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0].V' (./layer.h:137:11)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:160:36)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:161:41)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:24)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:27:21)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:235:14)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out[0].V' (./layer.h:234:21)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_hidden_sta' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'quantized_final_outp' (./layer.h:37:7)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:223:9)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:21 ; elapsed = 00:00:25 . Memory (MB): peak = 1296.152 ; gain = 658.125 ; free physical = 8811 ; free virtual = 24257
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<16>' to 'rms_norm_16_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 4, 3>' to 'softmax_1_4_3_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.
INFO: [SCHED 204-61] Pipelining loop 'RMS_NORM_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 24.68 seconds; current allocated memory: 303.456 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 304.255 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'QUANTIZE_ACTIVATION_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 304.972 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 305.806 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 305.932 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.04 seconds; current allocated memory: 306.001 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 17, Final II = 17, Depth = 120.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 307.400 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 308.947 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'RESHAPE_2D_TO_3D_LOOP_2_RESHAPE_2D_TO_3D_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 309.392 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 309.768 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 311.019 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 312.762 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 313.200 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 313.608 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'TRANSPOSE_LAST_TWO_DIMS_LOOP_1_TRANSPOSE_LAST_TWO_DIMS_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 313.868 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.06 seconds; current allocated memory: 314.163 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 314.619 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 315.257 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'attention_exp'.
INFO: [SCHED 204-61] Pipelining result : Target II = 13, Final II = 11, Depth = 11.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 315.555 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 315.978 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'SOFTMAX_LOOP_4'.
INFO: [SCHED 204-61] Pipelining result : Target II = 13, Final II = 13, Depth = 13.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 316.680 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 318.235 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'GEMM_3D_FLOAT_LOOP_1_GEMM_3D_FLOAT_LOOP_3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 5.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 318.855 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 319.564 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'ATTN_2D_LOOP_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 2, Final II = 2, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 320.419 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 322.984 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 323.638 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 323.963 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_16_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_53s_32s_53_57_seq_1' to 'dut_sdiv_53s_32s_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_46ns_32s_32_50_seq_1' to 'dut_sdiv_46ns_32scud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_64s_32s_76_5_1' to 'dut_mul_64s_32s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_64s_32s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_46ns_32scud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_53s_32s_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_16_s'.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 326.213 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_51s_31ns_32_55_seq_1' to 'dut_udiv_51s_31nseOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_51s_31nseOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 329.809 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 333.055 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_76ns_59s_32_80_1' to 'dut_sdiv_76ns_59sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_76ns_59sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 336.080 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 343.898 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_2' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_2' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 8 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 350.368 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_165_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 358.397 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 360.251 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_32_32_1_1': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 4 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 362.600 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention_exp' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention_exp'.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 365.747 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_4_3_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_54ns_32s_32_58_seq_1' to 'dut_sdiv_54ns_32sibs' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 18 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_54ns_32sibs': 3 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_4_3_s'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 369.562 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 5 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 375.013 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigjbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_0' to 'attention_q_weighkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_1' to 'attention_q_weighlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_2' to 'attention_q_weighmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_V_3' to 'attention_q_weighncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_0' to 'attention_k_weighocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_1' to 'attention_k_weighpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_2' to 'attention_k_weighqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_V_3' to 'attention_k_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_0' to 'attention_v_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_1' to 'attention_v_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_2' to 'attention_v_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_V_3' to 'attention_v_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_0' to 'attention_o_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_1' to 'attention_o_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_2' to 'attention_o_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_V_3' to 'attention_o_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_CeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_0_V' to 'attention_k_cacheFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_1_V' to 'attention_k_cacheGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_2_V' to 'attention_k_cacheHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_3_V' to 'attention_k_cacheIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_0_V' to 'attention_v_cacheJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_1_V' to 'attention_v_cacheKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_2_V' to 'attention_v_cacheLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_3_V' to 'attention_v_cacheMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_Ngs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_OgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_PgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizRg6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_124_32_1_1': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_32_1_1': 21 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 1.12 seconds; current allocated memory: 383.930 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 1.1 seconds; current allocated memory: 390.641 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_53s_32s_bkb_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_46ns_32scud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_64s_32s_7dEe_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_51s_31nseOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_76ns_59sfYi_div'
INFO: [RTMG 210-279]