==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:13 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297697 ; free virtual = 376088
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:13 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297697 ; free virtual = 376088
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:19 . Memory (MB): peak = 1241.336 ; gain = 603.309 ; free physical = 297277 ; free virtual = 375703
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>134' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>134' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>134' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:97) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:184) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:88: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:21 . Memory (MB): peak = 1392.105 ; gain = 754.078 ; free physical = 297395 ; free virtual = 375835
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:188) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:211) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:217) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:162) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_3D_MEM_LOOP_1' (./layer.h:50) in function 'init_3d_mem<1, 96, 4, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:188) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:211) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:217) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:163) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:166) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:138) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_3D_MEM_LOOP_1' (./layer.h:50) in function 'init_3d_mem<1, 96, 4, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:207) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:208) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:137) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:138) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:139) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:147) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:164) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:183) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:226) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:183) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:225) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:225) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:107) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:108) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:109) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:208) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:97) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:184) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:74) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:162:86) to (./layer.h:162:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:245:9) to (./layer.h:244:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:153)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:22 ; elapsed = 00:01:27 . Memory (MB): peak = 1520.168 ; gain = 882.141 ; free physical = 297265 ; free virtual = 375725
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:161:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:256:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:189:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:59:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:160:49)
WARNING: [XFORM 203-631] Renaming function 'init_3d_mem<1, 96, 4, ap_int<8> >' to 'init_3d_mem' (./layer.h:50:50)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:37:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:242:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:207:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:277:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:277:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:259:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:317:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:321:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:93:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:191:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:141:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:175:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:171:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0][0][0].V' (./layer.h:52:9)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:38:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:245:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:181:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:213:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:212:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:213:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:214:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:215:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:225:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:227:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:35 ; elapsed = 00:01:41 . Memory (MB): peak = 1705.184 ; gain = 1067.156 ; free physical = 297067 ; free virtual = 375535
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 101.1 seconds; current allocated memory: 701.955 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 704.582 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 705.506 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 706.037 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_3d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 707.202 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 708.324 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 709.158 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 710.466 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 710.642 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 710.708 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 713.649 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.93 seconds; current allocated memory: 717.862 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.13 seconds; current allocated memory: 718.568 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 718.718 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 719.048 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 719.439 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 719.641 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 719.902 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 720.117 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 720.336 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 720.510 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 720.753 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 721.190 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 721.650 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 722.035 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 722.472 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 722.728 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 722.975 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 723.697 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.15 seconds; current allocated memory: 727.243 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.62 seconds; current allocated memory: 728.627 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 729.351 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.57 seconds; current allocated memory: 734.770 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.35 seconds; current allocated memory: 745.838 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_3d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_3d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 748.458 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 754.855 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 759.938 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 766.478 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.44 seconds; current allocated memory: 783.051 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 784.249 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 786.219 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 787.622 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 788.894 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 790.774 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 793.543 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 795.312 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.17 seconds; current allocated memory: 805.327 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.72 seconds; current allocated memory: 815.112 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbsm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbsm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbtn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:40 ; elapsed = 00:01:49 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297580 ; free virtual = 376093
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:40 ; elapsed = 00:01:49 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297580 ; free virtual = 376093
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:45 ; elapsed = 00:01:54 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 297341 ; free virtual = 375889
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>134' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>134' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>134' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:97) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:184) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:88: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:47 ; elapsed = 00:01:56 . Memory (MB): peak = 1367.449 ; gain = 729.422 ; free physical = 297254 ; free virtual = 375816
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:188) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:211) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:217) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:162) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_3D_MEM_LOOP_1' (./layer.h:50) in function 'init_3d_mem<1, 96, 4, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:188) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:211) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:217) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:163) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:166) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:138) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_3D_MEM_LOOP_1' (./layer.h:50) in function 'init_3d_mem<1, 96, 4, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:207) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:208) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:137) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:138) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:139) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:147) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:164) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:183) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:226) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:183) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:225) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:225) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:107) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:108) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:109) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:208) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:97) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:184) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:74) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:162:86) to (./layer.h:162:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:245:9) to (./layer.h:244:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:153)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:52 ; elapsed = 00:02:03 . Memory (MB): peak = 1495.137 ; gain = 857.109 ; free physical = 297107 ; free virtual = 375690
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:161:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:256:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:189:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:59:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:160:49)
WARNING: [XFORM 203-631] Renaming function 'init_3d_mem<1, 96, 4, ap_int<8> >' to 'init_3d_mem' (./layer.h:50:50)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:37:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:242:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:207:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:277:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:277:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:259:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:317:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:321:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:93:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:191:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:141:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:175:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:171:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0][0][0].V' (./layer.h:52:9)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:38:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:245:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:181:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:213:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:212:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:213:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:214:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:215:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:225:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:227:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:02:05 ; elapsed = 00:02:16 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 296919 ; free virtual = 375509
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 136.39 seconds; current allocated memory: 702.612 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 705.240 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.29 seconds; current allocated memory: 706.183 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 706.709 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_3d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 707.859 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 708.977 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 709.811 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 711.122 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.64 seconds; current allocated memory: 711.297 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 711.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 714.318 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.02 seconds; current allocated memory: 718.514 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.19 seconds; current allocated memory: 719.236 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 719.386 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 719.733 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 720.124 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 720.323 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 720.619 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 720.804 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 721.023 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 721.194 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 721.437 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 721.860 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 722.319 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 722.674 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 723.128 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 723.381 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 723.628 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 724.354 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.21 seconds; current allocated memory: 727.891 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.54 seconds; current allocated memory: 729.241 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 729.984 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.56 seconds; current allocated memory: 735.402 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.33 seconds; current allocated memory: 746.495 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_3d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_3d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 749.102 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.79 seconds; current allocated memory: 755.527 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 760.609 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 767.153 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.61 seconds; current allocated memory: 783.723 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 784.924 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 786.877 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 788.295 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 789.522 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 791.401 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.11 seconds; current allocated memory: 794.215 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 1.04 seconds; current allocated memory: 795.940 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.19 seconds; current allocated memory: 805.955 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.87 seconds; current allocated memory: 815.771 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:40 ; elapsed = 00:01:49 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297638 ; free virtual = 376151
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:40 ; elapsed = 00:01:49 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297638 ; free virtual = 376151
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:45 ; elapsed = 00:01:54 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 297400 ; free virtual = 375948
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>134' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>134' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>134' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:97) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:184) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:88: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:47 ; elapsed = 00:01:56 . Memory (MB): peak = 1367.449 ; gain = 729.422 ; free physical = 297318 ; free virtual = 375880
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:188) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:211) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:217) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:162) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_3D_MEM_LOOP_1' (./layer.h:50) in function 'init_3d_mem<1, 96, 4, ap_int<8> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:188) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:211) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:217) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:163) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:166) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:138) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_3D_MEM_LOOP_1' (./layer.h:50) in function 'init_3d_mem<1, 96, 4, ap_int<8> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:207) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:208) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:137) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:138) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:139) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:147) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:164) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:183) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:226) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:183) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:225) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:225) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:107) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:108) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:109) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:208) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:225) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:97) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:184) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:74) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:162:86) to (./layer.h:162:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:245:9) to (./layer.h:244:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:153)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:53 ; elapsed = 00:02:03 . Memory (MB): peak = 1495.137 ; gain = 857.109 ; free physical = 297173 ; free virtual = 375756
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:161:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:256:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:189:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:59:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:160:49)
WARNING: [XFORM 203-631] Renaming function 'init_3d_mem<1, 96, 4, ap_int<8> >' to 'init_3d_mem' (./layer.h:50:50)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:37:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:242:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:207:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:277:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:277:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:259:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:317:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:321:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:93:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:191:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:141:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:175:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:171:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0][0][0].V' (./layer.h:52:9)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:38:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:245:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:181:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:213:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:212:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:213:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:214:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:215:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:225:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:227:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:02:06 ; elapsed = 00:02:16 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 296978 ; free virtual = 375569
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 136.5 seconds; current allocated memory: 702.600 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 705.231 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 706.172 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 706.700 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_3d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 707.848 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 708.970 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 709.804 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 711.112 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 711.289 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 711.354 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 714.309 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.98 seconds; current allocated memory: 718.524 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.15 seconds; current allocated memory: 719.261 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 719.410 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 719.755 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 720.146 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 720.346 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 720.643 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 720.827 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 721.046 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 721.216 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 721.460 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 721.883 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 722.355 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 722.696 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 723.151 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 723.406 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 723.654 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 724.380 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.13 seconds; current allocated memory: 727.942 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.59 seconds; current allocated memory: 729.308 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 730.031 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.64 seconds; current allocated memory: 735.453 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.35 seconds; current allocated memory: 746.531 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_3d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_3d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 749.140 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 755.562 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 760.647 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 767.187 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.73 seconds; current allocated memory: 783.758 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 784.960 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 786.913 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 788.315 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 789.557 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 791.436 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.95 seconds; current allocated memory: 794.251 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 795.976 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.02 seconds; current allocated memory: 806.006 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.78 seconds; current allocated memory: 815.822 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297743 ; free virtual = 376133
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297743 ; free virtual = 376134
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:20 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 297509 ; free virtual = 375934
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>130' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>130' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>130' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:96) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:88: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:22 . Memory (MB): peak = 1367.445 ; gain = 729.418 ; free physical = 297426 ; free virtual = 375865
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:162) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:163) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:166) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:138) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:207) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:208) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:137) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:138) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:145) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:163) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:182) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:225) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:182) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:107) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:108) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:207) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:96) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:74) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:162:86) to (./layer.h:162:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:245:9) to (./layer.h:244:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:153)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:22 ; elapsed = 00:01:29 . Memory (MB): peak = 1495.125 ; gain = 857.098 ; free physical = 297273 ; free virtual = 375734
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:161:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:256:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:189:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:59:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:160:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:37:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:242:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:207:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:277:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:277:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:259:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:317:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:321:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:93:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:191:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:141:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:175:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:171:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:38:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:245:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:180:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:212:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:212:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:213:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:214:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:215:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:225:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:227:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:32 ; elapsed = 00:01:39 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 297095 ; free virtual = 375562
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 99.17 seconds; current allocated memory: 689.707 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 692.334 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 693.243 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 693.773 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 695.019 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 696.349 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 696.503 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 696.570 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 699.548 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.94 seconds; current allocated memory: 703.722 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.11 seconds; current allocated memory: 704.512 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 704.662 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 704.984 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 705.411 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 705.609 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 705.868 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 706.091 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 706.310 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 706.480 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 706.723 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 707.123 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 707.617 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 707.977 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 708.380 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 708.670 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 708.918 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 709.634 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.78 seconds; current allocated memory: 712.944 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.53 seconds; current allocated memory: 714.331 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 715.018 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.56 seconds; current allocated memory: 720.450 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.32 seconds; current allocated memory: 731.481 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 735.106 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.33 seconds; current allocated memory: 740.189 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 746.760 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.45 seconds; current allocated memory: 763.333 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 764.529 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 766.456 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 767.904 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 769.130 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 771.008 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 773.792 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 775.544 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.52 seconds; current allocated memory: 784.652 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.65 seconds; current allocated memory: 793.969 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297656 ; free virtual = 376046
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297656 ; free virtual = 376046
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:20 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 297410 ; free virtual = 375836
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>130' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>130' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>130' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:96) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:88: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:22 . Memory (MB): peak = 1367.449 ; gain = 729.422 ; free physical = 297327 ; free virtual = 375767
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:162) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:163) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:166) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:138) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:207) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:208) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:137) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:138) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:145) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:163) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:182) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:225) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:182) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a block factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a block factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a block factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a block factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a block factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 2 with a block factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:107) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:108) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:207) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:96) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:74) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:171:13) to (./layer.h:162:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 237 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:245:9) to (./layer.h:244:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:153)...67 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:25 ; elapsed = 00:01:32 . Memory (MB): peak = 1495.137 ; gain = 857.109 ; free physical = 297167 ; free virtual = 375628
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:161:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:256:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:189:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:59:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:160:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:37:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:242:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:207:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:277:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:277:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:259:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:317:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:321:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:93:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:191:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0].V' (./layer.h:141:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:175:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:171:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:38:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:245:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:180:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:212:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:212:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:213:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:214:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:215:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:225:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:227:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:34 ; elapsed = 00:01:40 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 296973 ; free virtual = 375441
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 101.43 seconds; current allocated memory: 726.536 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 729.163 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.31 seconds; current allocated memory: 730.090 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 730.600 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 731.866 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 733.663 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 733.841 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 733.906 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_29', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_29', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_0_3_V_load_10', ./layer.h:169) on array 'input_0_3_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_0_3_V'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_0_2_V_load_7', ./layer.h:169) on array 'input_0_2_V' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_0_2_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 99.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 29.29 seconds; current allocated memory: 745.123 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 11.48 seconds; current allocated memory: 763.599 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 10.18 seconds; current allocated memory: 764.465 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 764.614 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 764.943 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 765.337 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 765.547 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 765.845 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 766.030 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 766.248 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 766.419 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 766.662 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 767.078 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 767.547 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 767.892 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 768.365 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 768.616 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 768.864 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 769.574 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.31 seconds; current allocated memory: 772.925 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.88 seconds; current allocated memory: 774.311 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 774.998 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.87 seconds; current allocated memory: 780.496 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.26 seconds; current allocated memory: 791.544 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_urem_7ns_4ns_7_11_seq_1' to 'dut_urem_7ns_4ns_fYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_urem_7ns_4ns_fYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 795.254 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 800.420 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_urem_7ns_4ns_7_11_1' to 'dut_urem_7ns_4ns_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_1632_8_1_1': 75 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61shbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_urem_7ns_4ns_g8j': 16 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 820.535 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 12.74 seconds; current allocated memory: 876.997 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_kbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_lbW' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 878.169 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 880.157 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 881.512 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 882.770 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nspcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsqcK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nspcA': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsqcK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 884.393 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40srcU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40srcU': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 887.171 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 888.896 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdMK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdNK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dOK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dOK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.31 seconds; current allocated memory: 900.073 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 4.1 seconds; current allocated memory: 910.583 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_urem_7ns_4ns_fYi_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_urem_7ns_4ns_g8j_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61shbi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_ibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_jbC_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_kbM_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nspcA_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsqcK_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mmb6_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mncg_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xocq_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40srcU_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dOK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbsm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbsm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbtn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbtn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbun' is read-only, switch it to a ROM.
INFO: [RTMG 210-279]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:11 ; elapsed = 00:01:16 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 296988 ; free virtual = 375384
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:11 ; elapsed = 00:01:16 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 296988 ; free virtual = 375384
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:16 ; elapsed = 00:01:22 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 296931 ; free virtual = 375365
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>130' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>130' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>130' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:96) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:88: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:18 ; elapsed = 00:01:24 . Memory (MB): peak = 1367.445 ; gain = 729.418 ; free physical = 296865 ; free virtual = 375313
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:162) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:309) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:296) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:297) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:278) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:210) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:222) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:189) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:160) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:163) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:166) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:37) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:124) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:138) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_1D_MEM_LOOP_1' (./layer.h:25) in function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:207) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:208) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:137) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:138) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:145) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:146) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:163) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:182) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:225) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:182) automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:107) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:108) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:207) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:91) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:127) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:130) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:139) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:317) automatically.
INFO: [XFORM 203-602] Inlining function 'init_1d_mem<1, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:96) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:136:74) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:139:56) to (./layer.h:141:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:162:86) to (./layer.h:162:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:245:9) to (./layer.h:244:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:153)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:23 ; elapsed = 00:01:30 . Memory (MB): peak = 1495.121 ; gain = 857.094 ; free physical = 297076 ; free virtual = 375535
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:161:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:256:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:189:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:59:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:160:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:37:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:242:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:207:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:277:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:277:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:259:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:317:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:321:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:93:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:191:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:141:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:175:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:171:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:38:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:245:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:180:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:212:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:212:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:213:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:214:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:215:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:225:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:227:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:282:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:32 ; elapsed = 00:01:40 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 296894 ; free virtual = 375359
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 100.17 seconds; current allocated memory: 689.697 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 692.324 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.28 seconds; current allocated memory: 693.248 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 693.763 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 695.012 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 696.342 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 696.464 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 696.530 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln171', ./layer.h:171) of variable 'add_ln703_89', ./layer.h:171 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:171) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 699.523 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.94 seconds; current allocated memory: 703.697 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.08 seconds; current allocated memory: 704.516 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 704.665 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 704.956 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 705.384 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 705.583 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 705.844 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 706.069 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 706.287 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 706.470 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 706.714 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 707.099 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 707.594 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 707.950 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 708.368 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 708.659 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 708.907 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 709.619 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.88 seconds; current allocated memory: 712.919 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.58 seconds; current allocated memory: 714.307 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 714.981 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.61 seconds; current allocated memory: 720.442 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.28 seconds; current allocated memory: 731.473 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 735.100 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 740.185 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 746.752 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.4 seconds; current allocated memory: 763.312 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 764.509 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 766.435 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 767.882 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 769.109 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 770.986 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 773.770 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 775.523 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.32 seconds; current allocated memory: 784.612 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.58 seconds; current allocated memory: 793.962 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:13 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297549 ; free virtual = 375906
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:13 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297549 ; free virtual = 375906
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:19 . Memory (MB): peak = 1241.332 ; gain = 603.305 ; free physical = 297314 ; free virtual = 375707
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:37) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>122' (./layer.h:73) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>122' (./layer.h:76) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>122' (./layer.h:85) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:263) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:177) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:34: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:21 . Memory (MB): peak = 1380.102 ; gain = 742.074 ; free physical = 297219 ; free virtual = 375625
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:173) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:181) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:203) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:209) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:225) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:256) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:243) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:244) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:225) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:157) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:136) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:106) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:109) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:70) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:173) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:180) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:181) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:203) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:209) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:225) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:256) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:243) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:244) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:225) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:157) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:136) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:106) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:110) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:113) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:70) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:84) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:154) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:155) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:130) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:131) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:132) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:139) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:140) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:157) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:176) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:218) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:176) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:217) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:217) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:201) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:217) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:37) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:73) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:76) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:85) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:263) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:177) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:82:74) to (./layer.h:87:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:85:56) to (./layer.h:87:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:85:56) to (./layer.h:87:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:85:56) to (./layer.h:87:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:109:86) to (./layer.h:109:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:192:9) to (./layer.h:191:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:99)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:22 ; elapsed = 00:01:27 . Memory (MB): peak = 1508.152 ; gain = 870.125 ; free physical = 297067 ; free virtual = 375495
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:107:68) in function 'linear_forward_no_mul<1, 384, 384>' : 

the outer loop is not a perfect loop.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:203:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:136:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:20:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:106:24)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:189:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:154:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:224:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:224:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:206:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:263:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:267:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:39:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:138:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:87:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:122:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:108:7)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:118:13)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:192:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:205:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:159:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:160:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:161:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:162:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:172:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:174:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:229:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:229:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:31 ; elapsed = 00:01:36 . Memory (MB): peak = 1693.184 ; gain = 1055.156 ; free physical = 296895 ; free virtual = 375330
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 96.83 seconds; current allocated memory: 679.699 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 682.346 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 683.287 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 683.818 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.66 seconds; current allocated memory: 685.064 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 686.395 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln118', ./layer.h:118) of variable 'add_ln703_89', ./layer.h:118 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:118) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln118', ./layer.h:118) of variable 'add_ln703_89', ./layer.h:118 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:118) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln118', ./layer.h:118) of variable 'add_ln703_89', ./layer.h:118 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:118) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 81.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 689.467 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.95 seconds; current allocated memory: 693.555 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.08 seconds; current allocated memory: 694.296 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 694.445 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 694.764 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 695.159 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 695.395 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 695.653 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 695.874 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 696.092 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 696.264 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 696.507 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 696.892 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 697.400 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 697.760 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 698.180 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 698.468 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 698.717 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 699.416 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.66 seconds; current allocated memory: 702.543 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.53 seconds; current allocated memory: 703.908 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 704.596 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.56 seconds; current allocated memory: 710.063 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.29 seconds; current allocated memory: 721.056 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 724.682 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 1.19 seconds; current allocated memory: 735.839 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.47 seconds; current allocated memory: 752.076 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 753.280 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 755.262 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 756.651 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 757.907 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 759.813 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 762.570 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 764.293 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.07 seconds; current allocated memory: 773.360 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.67 seconds; current allocated memory: 782.498 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:16 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297560 ; free virtual = 375918
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:16 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 297560 ; free virtual = 375918
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:21 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 297314 ; free virtual = 375706
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:23 . Memory (MB): peak = 1367.445 ; gain = 729.418 ; free physical = 297229 ; free virtual = 375636
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:74) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:86) to (./layer.h:121:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:23 ; elapsed = 00:01:29 . Memory (MB): peak = 1495.125 ; gain = 857.098 ; free physical = 297080 ; free virtual = 375508
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:100:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:184:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:32 ; elapsed = 00:01:39 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 296899 ; free virtual = 375334
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 99.31 seconds; current allocated memory: 689.564 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 692.203 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.22 seconds; current allocated memory: 693.128 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 693.643 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 694.889 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 696.219 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 696.341 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 696.408 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 699.397 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.89 seconds; current allocated memory: 703.571 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.98 seconds; current allocated memory: 704.391 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 704.540 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 704.831 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 705.258 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 705.457 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 705.718 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 705.937 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 706.157 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 706.346 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 706.589 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 706.974 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 707.469 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 707.826 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 708.245 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 708.536 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 708.783 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 709.497 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3 seconds; current allocated memory: 712.793 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.58 seconds; current allocated memory: 714.182 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 714.852 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.62 seconds; current allocated memory: 720.314 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.16 seconds; current allocated memory: 731.345 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 734.971 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 740.054 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 746.624 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.49 seconds; current allocated memory: 763.182 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 764.382 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 766.306 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 767.753 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 768.979 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.79 seconds; current allocated memory: 770.858 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 773.641 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.92 seconds; current allocated memory: 775.395 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.26 seconds; current allocated memory: 784.481 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.67 seconds; current allocated memory: 793.826 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:12 ; elapsed = 00:01:18 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295731 ; free virtual = 375029
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:12 ; elapsed = 00:01:18 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295731 ; free virtual = 375029
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:17 ; elapsed = 00:01:24 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 295476 ; free virtual = 374809
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:64) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>134' (./layer.h:100) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>134' (./layer.h:103) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>134' (./layer.h:112) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:288) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:61: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:19 ; elapsed = 00:01:26 . Memory (MB): peak = 1367.449 ; gain = 729.422 ; free physical = 295391 ; free virtual = 374738
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:251) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_3D_MEM_LOOP_2' (./layer.h:39) in function 'init_3d_mem<8, 1, 48, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:281) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:268) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:269) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:251) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_3D_MEM_LOOP_2' (./layer.h:39) in function 'init_3d_mem<8, 1, 6, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:183) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:195) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:162) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:133) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:135) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:97) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:179) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:187) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:210) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:216) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:251) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_3D_MEM_LOOP_2' (./layer.h:39) in function 'init_3d_mem<8, 1, 48, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:281) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:268) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:269) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:251) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'INIT_3D_MEM_LOOP_2' (./layer.h:39) in function 'init_3d_mem<8, 1, 6, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:183) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:195) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:162) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:133) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:136) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:139) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:111) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:180) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:181) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:182) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output.V' (attention.cpp:193) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:225) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:182) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:224) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:208) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:224) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:64) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:100) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:103) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:112) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:288) automatically.
INFO: [XFORM 203-602] Inlining function 'init_3d_mem<8, 1, 6, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:163) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:183) automatically.
INFO: [XFORM 203-602] Inlining function 'init_3d_mem<8, 1, 48, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:194) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:109:74) to (./layer.h:114:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:112:56) to (./layer.h:114:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:112:56) to (./layer.h:114:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:112:56) to (./layer.h:114:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:135:86) to (./layer.h:135:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:218:9) to (./layer.h:217:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:126)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:24 ; elapsed = 00:01:32 . Memory (MB): peak = 1495.125 ; gain = 857.098 ; free physical = 295246 ; free virtual = 374615
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:134:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:229:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:162:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:47:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:133:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:215:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:37:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:180:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:250:55)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:250:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output.V' (./layer.h:232:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:288:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:292:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:66:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:164:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:114:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:148:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:144:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (./layer.h:40:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:180:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output[0].V' (./layer.h:40:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:212:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:185:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:186:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:187:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:188:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0].V' (./layer.h:198:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:200:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:254:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:34 ; elapsed = 00:01:42 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 295065 ; free virtual = 374441
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 102.78 seconds; current allocated memory: 690.023 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 692.654 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 693.581 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 694.096 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 695.343 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 696.672 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 696.794 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 696.862 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln144', ./layer.h:144) of variable 'add_ln703_89', ./layer.h:144 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:144) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln144', ./layer.h:144) of variable 'add_ln703_89', ./layer.h:144 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:144) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln144', ./layer.h:144) of variable 'add_ln703_89', ./layer.h:144 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:144) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 699.854 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.93 seconds; current allocated memory: 704.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.12 seconds; current allocated memory: 704.848 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 704.998 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 705.291 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 705.718 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 705.918 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 706.179 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 706.398 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 706.617 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 706.789 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 707.027 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 707.412 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 707.925 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 708.278 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 708.681 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 708.976 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 709.218 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 710.068 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3 seconds; current allocated memory: 713.566 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.68 seconds; current allocated memory: 714.905 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 715.627 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.6 seconds; current allocated memory: 721.072 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.35 seconds; current allocated memory: 732.119 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.88 seconds; current allocated memory: 735.680 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 740.777 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 747.348 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.53 seconds; current allocated memory: 763.958 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 765.155 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 767.081 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 768.529 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 769.754 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 771.627 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 774.412 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 776.165 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_V' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0_V' to 'attention_attn_oucIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_dMK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_dMK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 6.33 seconds; current allocated memory: 785.430 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.94 seconds; current allocated memory: 795.405 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_dMK_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbsm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbsm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbtn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbtn_rom' using block ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_quantizbun_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_cwx_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_czy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cachecEy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_attn_wecHz_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_input_0_V_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_output_0_ram (RAM)' using block RAMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:02:19 ; elapsed = 00:02:52 . Memory (MB): peak = 1808.152 ; gain = 1170.125 ; free physical = 294452 ; free virtual = 374326
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:11 ; elapsed = 00:01:16 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295198 ; free virtual = 374817
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:11 ; elapsed = 00:01:16 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295198 ; free virtual = 374817
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:16 ; elapsed = 00:01:22 . Memory (MB): peak = 1241.332 ; gain = 603.305 ; free physical = 294954 ; free virtual = 374609
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:18 ; elapsed = 00:01:24 . Memory (MB): peak = 1392.102 ; gain = 754.074 ; free physical = 294852 ; free virtual = 374520
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:74) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:86) to (./layer.h:121:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:24 ; elapsed = 00:01:30 . Memory (MB): peak = 1520.172 ; gain = 882.145 ; free physical = 294703 ; free virtual = 374393
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:100:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:184:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:33 ; elapsed = 00:01:40 . Memory (MB): peak = 1705.184 ; gain = 1067.156 ; free physical = 294522 ; free virtual = 374219
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 100.66 seconds; current allocated memory: 692.025 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 694.640 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 695.565 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 696.077 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 697.338 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 698.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 698.825 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 698.892 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 701.845 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2 seconds; current allocated memory: 706.022 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.13 seconds; current allocated memory: 706.810 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 706.960 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 707.439 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 708.103 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 708.330 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 708.593 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 708.935 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 709.409 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 710.007 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 710.911 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 711.358 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 711.852 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 712.226 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 712.645 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 713.105 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 713.654 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 714.450 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.38 seconds; current allocated memory: 718.205 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.73 seconds; current allocated memory: 719.623 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 720.333 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.72 seconds; current allocated memory: 725.891 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.37 seconds; current allocated memory: 736.913 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.93 seconds; current allocated memory: 740.554 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 745.622 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 752.200 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.48 seconds; current allocated memory: 768.748 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 770.253 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 773.132 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 774.787 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 777.754 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 1.15 seconds; current allocated memory: 782.079 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 784.926 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 787.082 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedcGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedcIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedcJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_V' to 'attention_q_embedcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_V' to 'attention_q_embedcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_V' to 'attention_q_embedcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_V' to 'attention_q_embedcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_cVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_cWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_cXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_cYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_cZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_c0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_c1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_c2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_c3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_c4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_c5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_c6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_c7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_c8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_c9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_daE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wedbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oudcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdMK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdNK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdOK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdPK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdQK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdRK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdSL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdTL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdUL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdVL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdWL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdXL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizdYM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdZM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizd0M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizd1M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizd2M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizd3M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizd4N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizd5N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizd6N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizd7N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizd8N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizd9N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizeaO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizebO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizecO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizedO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizeeO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizefO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_egO' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_egO': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 14.6 seconds; current allocated memory: 798.716 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.98 seconds; current allocated memory: 808.895 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_egO_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295206 ; free virtual = 374825
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295206 ; free virtual = 374825
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=24) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:20 . Memory (MB): peak = 1241.336 ; gain = 603.309 ; free physical = 294953 ; free virtual = 374607
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:22 . Memory (MB): peak = 1367.445 ; gain = 729.418 ; free physical = 294877 ; free virtual = 374545
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=24) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 24.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 24.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.16' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.17' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.18' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.19' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.20' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.21' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.22' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.23' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:70) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:99:48) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:99:48) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:99:48) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:79) to (./layer.h:121:72) in function 'linear_forward_no_mul<1, 384, 384>'... converting 385 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...96 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:45 ; elapsed = 00:01:53 . Memory (MB): peak = 1488.152 ; gain = 850.125 ; free physical = 294675 ; free virtual = 374366
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:13)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:184:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:02:04 ; elapsed = 00:02:13 . Memory (MB): peak = 1744.152 ; gain = 1106.125 ; free physical = 294414 ; free virtual = 374112
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 133.8 seconds; current allocated memory: 805.903 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 808.530 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.4 seconds; current allocated memory: 809.456 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 809.964 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 3.86 seconds; current allocated memory: 815.774 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 4.17 seconds; current allocated memory: 822.344 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 3.36 seconds; current allocated memory: 822.503 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 822.570 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_121', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_121', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_121', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 827.490 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.64 seconds; current allocated memory: 835.300 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 3.68 seconds; current allocated memory: 836.364 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 836.512 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 837.030 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 837.968 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 838.234 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 838.493 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 838.909 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 839.631 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 840.756 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.84 seconds; current allocated memory: 842.356 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 842.872 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 843.327 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 843.707 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 844.145 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 844.623 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 845.130 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.86 seconds; current allocated memory: 847.434 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 4.89 seconds; current allocated memory: 855.789 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 4.04 seconds; current allocated memory: 858.378 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 859.554 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 3.01 seconds; current allocated memory: 866.219 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.51 seconds; current allocated memory: 877.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_urem_10ns_6ns_6_14_seq_1' to 'dut_urem_10ns_6nsfYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_mul_12ns_10ns_22_1_1' to 'dut_mul_mul_12ns_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_mul_12ns_g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_urem_10ns_6nsfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 8.44 seconds; current allocated memory: 1.186 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 4.22 seconds; current allocated memory: 1.206 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_8_1_1': 96 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61shbi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 1.218 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 5.82 seconds; current allocated memory: 1.246 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_kbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_lbW' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 1.248 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.18 seconds; current allocated memory: 1.251 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 1.253 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 1.258 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nspcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsqcK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nspcA': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsqcK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 2.06 seconds; current allocated memory: 1.265 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40srcU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40srcU': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 1.268 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 1.270 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_16' to 'attention_q_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_17' to 'attention_q_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_18' to 'attention_q_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_19' to 'attention_q_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_20' to 'attention_q_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_21' to 'attention_q_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_22' to 'attention_q_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_23' to 'attention_q_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_16' to 'attention_k_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_17' to 'attention_k_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_18' to 'attention_k_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_19' to 'attention_k_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_20' to 'attention_k_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_21' to 'attention_k_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_22' to 'attention_k_weighbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_23' to 'attention_k_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_16' to 'attention_v_weighbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_17' to 'attention_v_weighbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_18' to 'attention_v_weighbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_19' to 'attention_v_weighbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_20' to 'attention_v_weighbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_21' to 'attention_v_weighbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_22' to 'attention_v_weighbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_23' to 'attention_v_weighbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_16' to 'attention_o_weighbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_17' to 'attention_o_weighbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_18' to 'attention_o_weighbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_19' to 'attention_o_weighbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_20' to 'attention_o_weighbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_21' to 'attention_o_weighbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_22' to 'attention_o_weighb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_23' to 'attention_o_weighb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_b2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_b3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_b4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_b5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_b6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_b7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_V' to 'attention_q_embedckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_V' to 'attention_q_embedclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_V' to 'attention_q_embedcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_V' to 'attention_q_embedcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_16_0_V' to 'attention_q_embedcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_17_0_V' to 'attention_q_embedcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_18_0_V' to 'attention_q_embedcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_19_0_V' to 'attention_q_embedcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_20_0_V' to 'attention_q_embedcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_21_0_V' to 'attention_q_embedctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_22_0_V' to 'attention_q_embedcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_23_0_V' to 'attention_q_embedcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_cCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_cDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_cEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_cFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_cGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_cHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_cIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_cJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_cKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_cLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_cMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_cNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_cOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_16' to 'attention_k_proj_cPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_17' to 'attention_k_proj_cQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_18' to 'attention_k_proj_cRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_19' to 'attention_k_proj_cSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_20' to 'attention_k_proj_cTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_21' to 'attention_k_proj_cUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_22' to 'attention_k_proj_cVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_23' to 'attention_k_proj_cWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wecXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oucYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oucZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_c0C' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_c0C': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 17.52 seconds; current allocated memory: 1.456 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 6.08 seconds; current allocated memory: 1.471 GB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_urem_10ns_6nsfYi_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61shbi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_ibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_jbC_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_kbM_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nspcA_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsqcK_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mmb6_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mncg_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xocq_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40srcU_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_c0C_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbsm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbsm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbtn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbtn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbun' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbun_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbvn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbvn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbwn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbwn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbxn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbxn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbyn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbyn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbzo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbzo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbAo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbAo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbBo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbBo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbCo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbCo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbDo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbDo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbEo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbEo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbFp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbFp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbGp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbGp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbHp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbHp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbIp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbIp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbJp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbJp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbKp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbKp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbLp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbLp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbMq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbMq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbNq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbNq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbOq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbOq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbPq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbPq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbQq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbQq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbRq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbRq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbSr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbSr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbTr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbTr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbUr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbUr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbVr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbVr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbWr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbWr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbXr' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295183 ; free virtual = 374802
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295183 ; free virtual = 374802
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=36) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:21 . Memory (MB): peak = 1241.332 ; gain = 603.305 ; free physical = 294938 ; free virtual = 374592
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:23 . Memory (MB): peak = 1392.102 ; gain = 754.074 ; free physical = 294839 ; free virtual = 374506
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=36) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 24.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
WARNING: [XFORM 203-561] Updating loop upper bound from 2 to 1 for loop 'GEMM_3D_FLOAT_LOOP_4' in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>'.
WARNING: [XFORM 203-561] Updating loop lower bound from 2 to 1 for loop 'GEMM_3D_FLOAT_LOOP_4' in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>'.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 36.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 36.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 24.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.0.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.1.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.2.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.3.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.4.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.5.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.6.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.7.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.8.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.9.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.10.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.11.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.12.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.13.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.14.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.15.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.20.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.21.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.22.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.23.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.0.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.1.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.2.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.3.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.4.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.5.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.6.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.7.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.8.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.9.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.10.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.11.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.12.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.13.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.14.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.15.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.20.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.21.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.22.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.23.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.20.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.21.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.22.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.23.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.20.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.21.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.22.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.23.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'output_q.V12.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V13.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V14.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V15.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V16.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V17.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V18.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V19.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V20.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V21.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V22.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V23.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V24.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V25.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V26.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V27.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V28.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V29.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V30.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V31.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V32.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V33.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V34.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'output_q.V35.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V12.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V13.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V14.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V15.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V16.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V17.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V18.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V19.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V20.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V21.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V22.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V23.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V24.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V25.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V26.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V27.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V28.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V29.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V30.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V31.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V32.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V33.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V34.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input_1.V35.0' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.16' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.17' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.18' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.19' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.20' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.21' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.22' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.23' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.24' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.25' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.26' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.27' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.28' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.29' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.30' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.31' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.32' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.33' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.34' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.35' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.12' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.13' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.14' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.15' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.16' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.17' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.18' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.19' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.20' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.21' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.22' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.23' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.24' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.25' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.26' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.27' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.28' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.29' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.30' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.31' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.32' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.33' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.34' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_transposed.V.35' (attention.cpp:158) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.16.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.17.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.18.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.19.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.20.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.21.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.22.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.23.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.24.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.25.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.26.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.27.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.28.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.29.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.30.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.31.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.32.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.33.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.34.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.35.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:70) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:99:48) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:99:48) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:99:48) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:79) to (./layer.h:121:72) in function 'linear_forward_no_mul<1, 384, 384>'... converting 385 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...96 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:46 ; elapsed = 00:01:56 . Memory (MB): peak = 1513.184 ; gain = 875.156 ; free physical = 294655 ; free virtual = 374346
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:13)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:184:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:02:06 ; elapsed = 00:02:17 . Memory (MB): peak = 1769.184 ; gain = 1131.156 ; free physical = 294392 ; free virtual = 374090
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 137.48 seconds; current allocated memory: 807.754 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 810.396 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.38 seconds; current allocated memory: 811.309 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 811.818 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 4.03 seconds; current allocated memory: 817.612 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 4.08 seconds; current allocated memory: 824.197 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 3.42 seconds; current allocated memory: 824.357 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 824.424 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_121', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_121', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_121', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 829.327 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.69 seconds; current allocated memory: 837.160 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 3.68 seconds; current allocated memory: 838.209 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 838.358 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 838.975 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 840.027 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 840.293 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 840.553 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 841.062 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 841.906 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 843.227 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 845.231 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 845.759 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 846.214 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 846.597 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 847.042 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 847.516 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 848.031 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 850.388 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 5.63 seconds; current allocated memory: 858.900 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 4.03 seconds; current allocated memory: 861.471 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 862.646 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 3.06 seconds; current allocated memory: 869.326 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.5 seconds; current allocated memory: 880.329 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_urem_10ns_6ns_6_14_seq_1' to 'dut_urem_10ns_6nsfYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_mul_12ns_10ns_22_1_1' to 'dut_mul_mul_12ns_g8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_mul_12ns_g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_urem_10ns_6nsfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 8.06 seconds; current allocated memory: 1.189 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 4.24 seconds; current allocated memory: 1.209 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61shbi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_8_1_1': 96 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61shbi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 1.221 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 5.81 seconds; current allocated memory: 1.250 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_kbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_lbW' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.81 seconds; current allocated memory: 1.251 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 1.25 seconds; current allocated memory: 1.255 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.89 seconds; current allocated memory: 1.257 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 1.1 seconds; current allocated memory: 1.263 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nspcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsqcK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nspcA': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsqcK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 2.29 seconds; current allocated memory: 1.271 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40srcU' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40srcU': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.16 seconds; current allocated memory: 1.274 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 1.15 seconds; current allocated memory: 1.276 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_16' to 'attention_q_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_17' to 'attention_q_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_18' to 'attention_q_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_19' to 'attention_q_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_20' to 'attention_q_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_21' to 'attention_q_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_22' to 'attention_q_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_23' to 'attention_q_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_16' to 'attention_k_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_17' to 'attention_k_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_18' to 'attention_k_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_19' to 'attention_k_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_20' to 'attention_k_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_21' to 'attention_k_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_22' to 'attention_k_weighbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_23' to 'attention_k_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_16' to 'attention_v_weighbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_17' to 'attention_v_weighbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_18' to 'attention_v_weighbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_19' to 'attention_v_weighbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_20' to 'attention_v_weighbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_21' to 'attention_v_weighbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_22' to 'attention_v_weighbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_23' to 'attention_v_weighbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_16' to 'attention_o_weighbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_17' to 'attention_o_weighbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_18' to 'attention_o_weighbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_19' to 'attention_o_weighbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_20' to 'attention_o_weighbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_21' to 'attention_o_weighbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_22' to 'attention_o_weighb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_23' to 'attention_o_weighb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_b2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_b3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_b4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_b5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_b6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_b7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_0_V' to 'attention_q_embedckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_0_V' to 'attention_q_embedclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_0_V' to 'attention_q_embedcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_0_V' to 'attention_q_embedcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_16_0_0_V' to 'attention_q_embedcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_17_0_0_V' to 'attention_q_embedcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_18_0_0_V' to 'attention_q_embedcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_19_0_0_V' to 'attention_q_embedcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_20_0_0_V' to 'attention_q_embedcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_21_0_0_V' to 'attention_q_embedctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_22_0_0_V' to 'attention_q_embedcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_23_0_0_V' to 'attention_q_embedcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_24_0_0_V' to 'attention_q_embedcwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_25_0_0_V' to 'attention_q_embedcxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_26_0_0_V' to 'attention_q_embedcyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_27_0_0_V' to 'attention_q_embedczy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_28_0_0_V' to 'attention_q_embedcAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_29_0_0_V' to 'attention_q_embedcBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_30_0_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_31_0_0_V' to 'attention_q_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_32_0_0_V' to 'attention_q_embedcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_33_0_0_V' to 'attention_q_embedcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_34_0_0_V' to 'attention_q_embedcGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_35_0_0_V' to 'attention_q_embedcHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_cLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_cMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_cNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_cOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_cPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_cQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_cRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_cSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_cTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_cUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_cVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_cWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_cXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_cYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_cZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_c0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_16' to 'attention_k_proj_c1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_17' to 'attention_k_proj_c2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_18' to 'attention_k_proj_c3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_19' to 'attention_k_proj_c4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_20' to 'attention_k_proj_c5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_21' to 'attention_k_proj_c6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_22' to 'attention_k_proj_c7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_23' to 'attention_k_proj_c8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_24' to 'attention_k_proj_c9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_25' to 'attention_k_proj_daE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_26' to 'attention_k_proj_dbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_27' to 'attention_k_proj_dcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_28' to 'attention_k_proj_ddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_29' to 'attention_k_proj_deE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_30' to 'attention_k_proj_dfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_31' to 'attention_k_proj_dgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_32' to 'attention_k_proj_dhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_33' to 'attention_k_proj_diF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_34' to 'attention_k_proj_djF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_35' to 'attention_k_proj_dkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wedlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oudmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oudnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_doG' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_doG': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 19.5 seconds; current allocated memory: 1.464 GB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 6.22 seconds; current allocated memory: 1.480 GB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_urem_10ns_6nsfYi_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61shbi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_ibs_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_jbC_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_kbM_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nspcA_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsqcK_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mmb6_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mncg_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xocq_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40srcU_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_doG_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbsm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbsm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbtn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbtn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbun' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbun_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbvn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbvn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbwn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbwn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbxn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbxn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbyn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbyn_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbzo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbzo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbAo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbAo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbBo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbBo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbCo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbCo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbDo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbDo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbEo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbEo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbFp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbFp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbGp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbGp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbHp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbHp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbIp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbIp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbJp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbJp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbKp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbKp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbLp' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbLp_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbMq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbMq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbNq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbNq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbOq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbOq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbPq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbPq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbQq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbQq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbRq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbRq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbSr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbSr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbTr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbTr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbUr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbUr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbVr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbVr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbWr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbWr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbXr' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbXr_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbYs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbYs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbZs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbZs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighb0s' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighb0s_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighb1s' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighb1s_rom' using block ROMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_b2s_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_proj_b5t_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_embedb8t_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_q_embedckv_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_cachecJz_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_v_cachecKz_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_proj_cLz_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_k_proj_cXB_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'attention_attn_wedlF_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'dut_output_0_ram (RAM)' using block RAMs.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:03:45 ; elapsed = 00:04:29 . Memory (MB): peak = 2665.184 ; gain = 2027.156 ; free physical = 293455 ; free virtual = 373286
INFO: [VHDL 208-304] Generating VHDL RTL for dut.
INFO: [VLOG 209-307] Generating Verilog RTL for dut.
INFO: [HLS 200-112] Total elapsed time: 269.91 seconds; peak allocated memory: 1.480 GB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:14 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295157 ; free virtual = 374776
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:14 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295156 ; free virtual = 374776
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:245) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=20) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:20 . Memory (MB): peak = 1241.332 ; gain = 603.305 ; free physical = 294911 ; free virtual = 374565
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:99) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:279) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:22 . Memory (MB): peak = 1392.102 ; gain = 754.074 ; free physical = 294872 ; free virtual = 374539
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:272) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:259) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:260) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:173) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:185) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:152) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:123) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:245) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=20) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:245) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:272) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:259) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:260) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:245) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 20.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:173) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:185) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:152) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:124) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 20.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:128) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:98) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:170) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:171) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
WARNING: [XFORM 203-561] Updating loop upper bound from 3 to 2 for loop 'GEMM_3D_FLOAT_LOOP_4' in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>'.
WARNING: [XFORM 203-561] Updating loop lower bound from 3 to 2 for loop 'GEMM_3D_FLOAT_LOOP_4' in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>'.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 20.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.16.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.17.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.18.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.0' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.1' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.2' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_hidden_states.V.19.3' (attention.cpp:93) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.16.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.17.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.18.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.0' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.1' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.2' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-102] Automatically partitioning small array 'quantized_final_output.V.19.3' (attention.cpp:222) completely based on array size.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.0' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.1' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.2' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.3' (attention.cpp:93) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.0' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.1' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.2' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.3' (attention.cpp:222) in dimension 1 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.16.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.17.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.18.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.0' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.1' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.2' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V.19.3' (attention.cpp:93) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.16.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.17.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.18.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.0' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.1' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.2' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V.19.3' (attention.cpp:222) in dimension 2 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.16' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.17' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.18' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.19' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:99) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:279) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'packed_weights[16]' in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:127:54).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'packed_weights[17]' in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:127:54).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'packed_weights[18]' in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:127:54).
WARNING: [ANALYSIS 214-31] The program may have out of bound access of array variable 'packed_weights[19]' in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:127:54).
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:99:58) to (./layer.h:101:15) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:100:50) to (./layer.h:101:15) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:100:50) to (./layer.h:101:15) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:100:50) to (./layer.h:101:15) in function 'quantize_activation<1, 384>'... converting 3 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:123:85) to (./layer.h:126:11) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) to (./layer.h:126:11) in function 'linear_forward_no_mul<1, 384, 384>'... converting 193 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:127:54) in function 'linear_forward_no_mul<1, 384, 384>'... converting 17 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:208:9) to (./layer.h:207:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:114)...130 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:26 ; elapsed = 00:01:32 . Memory (MB): peak = 1577.184 ; gain = 939.156 ; free physical = 294725 ; free virtual = 374415
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:122:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:219:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:152:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:15)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:121:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:205:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:170:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:240:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:240:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:222:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:283:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:154:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][3][0].V' (./layer.h:101:15)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:138:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:133:15)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:208:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:175:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:176:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:177:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:178:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:190:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:188:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:245:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:243:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:245:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:43 ; elapsed = 00:01:49 . Memory (MB): peak = 1641.184 ; gain = 1003.156 ; free physical = 294550 ; free virtual = 374246
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 109.94 seconds; current allocated memory: 704.809 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.07 seconds; current allocated memory: 707.430 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.33 seconds; current allocated memory: 708.372 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 708.884 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 711.222 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 714.018 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.2 seconds; current allocated memory: 714.175 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 714.241 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_29', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_29', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_41', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_41', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 11, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_97', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 15, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_101', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 17, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_105', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 18, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln133', ./layer.h:133) of variable 'add_ln703_105', ./layer.h:133 on array 'output_0_V' and 'load' operation ('output_0_V_load_1', ./layer.h:133) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 19, Depth = 99.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 718.089 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.72 seconds; current allocated memory: 723.695 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.77 seconds; current allocated memory: 724.588 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 724.740 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 725.275 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 726.193 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 726.460 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 726.719 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 727.104 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 727.842 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 728.686 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 729.910 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.71 seconds; current allocated memory: 730.407 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 730.880 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 731.219 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 731.672 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 732.103 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 732.651 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.48 seconds; current allocated memory: 733.766 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.83 seconds; current allocated memory: 738.394 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.11 seconds; current allocated memory: 739.992 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 740.770 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.95 seconds; current allocated memory: 746.454 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.32 seconds; current allocated memory: 757.469 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_urem_7ns_6ns_6_11_seq_1' to 'dut_urem_7ns_6ns_fYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_urem_7ns_6ns_fYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 771.986 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.65 seconds; current allocated memory: 782.188 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sg8j' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mux_42_8_1_1': 16 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sg8j': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 791.016 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 4.49 seconds; current allocated memory: 812.830 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_kbM' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.67 seconds; current allocated memory: 814.489 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.98 seconds; current allocated memory: 817.768 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 819.556 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 823.535 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nspcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nspcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 1.52 seconds; current allocated memory: 829.147 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40sqcK' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40sqcK': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.12 seconds; current allocated memory: 831.871 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 834.057 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_16' to 'attention_q_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_17' to 'attention_q_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_18' to 'attention_q_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_19' to 'attention_q_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_16' to 'attention_k_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_17' to 'attention_k_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_18' to 'attention_k_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_19' to 'attention_k_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weighbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_16' to 'attention_v_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_17' to 'attention_v_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_18' to 'attention_v_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_19' to 'attention_v_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_16' to 'attention_o_weighbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_17' to 'attention_o_weighbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_18' to 'attention_o_weighbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_19' to 'attention_o_weighbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizcyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizczy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizcBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_cQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedcTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedcUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedcXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedcZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedc0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedc1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedc2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedc3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedc4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_V' to 'attention_q_embedc5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_V' to 'attention_q_embedc6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_V' to 'attention_q_embedc7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_V' to 'attention_q_embedc8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_16_0_V' to 'attention_q_embedc9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_17_0_V' to 'attention_q_embeddaE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_18_0_V' to 'attention_q_embeddbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_19_0_V' to 'attention_q_embeddcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachedeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachedfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_dgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_dhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_diF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_djF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_dkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_dlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_dmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_dnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_doG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_dpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_dqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_drG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_dsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_dtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_duH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_dvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_16' to 'attention_k_proj_dwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_17' to 'attention_k_proj_dxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_18' to 'attention_k_proj_dyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_19' to 'attention_k_proj_dzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wedAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oudBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_oudCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizdMK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizdNK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizdOK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizdPK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizdQK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizdRK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizdSL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizdTL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizdUL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizdVL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizdWL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizdXL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizdYM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizdZM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizd0M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizd1M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizd2M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizd3M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizd4N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizd5N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizd6N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizd7N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizd8N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizd9N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizeaO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizebO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizecO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizedO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizeeO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizefO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizegO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizehP' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizeiP' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizejP' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizekP' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizelP' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizemP' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizenQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizeoQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizepQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizeqQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizerQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizesQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizetR' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizeuR' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizevR' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizewR' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizexR' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizeyR' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizezS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizeAS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizeBS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizeCS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizeDS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizeES' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_eFT' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_eFT': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 17.62 seconds; current allocated memory: 852.371 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 4.67 seconds; current allocated memory: 864.901 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_urem_7ns_6ns_fYi_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sg8j_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_ibs_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_jbC_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsocq_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nspcA_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mmb6_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xncg_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40sqcK_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_eFT_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh8jQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh8jQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh9j0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh9j0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbak' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbak_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbbk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbbk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbck' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbck_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbdk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbdk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbek' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbek_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbgk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbgk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbhl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbhl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbil' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbil_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbjl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbjl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbkl' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbkl_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbll' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbll_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbml' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbml_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbnm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbnm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbom' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbom_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighbpm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighbpm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_cache_V' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_cache_V_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_ln_weigbqm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigbqm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbrm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbrm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbsm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbsm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_o_weighbtn' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_o_weighbtn_rom' using block ROMs.
WARNING: [RTMG 210-274]==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295168 ; free virtual = 374788
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:01:10 ; elapsed = 00:01:15 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 295168 ; free virtual = 374788
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:15 ; elapsed = 00:01:21 . Memory (MB): peak = 1241.332 ; gain = 603.305 ; free physical = 294924 ; free virtual = 374578
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:17 ; elapsed = 00:01:22 . Memory (MB): peak = 1392.102 ; gain = 754.074 ; free physical = 294850 ; free virtual = 374518
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:74) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:86) to (./layer.h:121:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:23 ; elapsed = 00:01:29 . Memory (MB): peak = 1520.172 ; gain = 882.145 ; free physical = 295189 ; free virtual = 374878
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:100:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:184:106)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:32 ; elapsed = 00:01:39 . Memory (MB): peak = 1705.184 ; gain = 1067.156 ; free physical = 295009 ; free virtual = 374705
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 99.57 seconds; current allocated memory: 692.024 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.03 seconds; current allocated memory: 694.633 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 695.558 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 696.071 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.69 seconds; current allocated memory: 697.333 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 698.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 698.825 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 698.891 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 701.841 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.96 seconds; current allocated memory: 706.019 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 2.15 seconds; current allocated memory: 706.807 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 706.958 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 707.437 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 708.100 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 708.326 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 708.587 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 708.931 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 709.409 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 710.002 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 710.905 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 711.350 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 711.845 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 712.222 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 712.642 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.37 seconds; current allocated memory: 713.105 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 713.649 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 714.446 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 3.33 seconds; current allocated memory: 718.200 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.68 seconds; current allocated memory: 719.621 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 720.328 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.72 seconds; current allocated memory: 725.887 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 2.34 seconds; current allocated memory: 736.908 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 740.549 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 745.616 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.52 seconds; current allocated memory: 752.199 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 3.57 seconds; current allocated memory: 768.743 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 770.252 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 773.128 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.77 seconds; current allocated memory: 774.782 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.78 seconds; current allocated memory: 777.750 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 782.075 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 1.02 seconds; current allocated memory: 784.921 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.95 seconds; current allocated memory: 787.077 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedcGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedcIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedcJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_V' to 'attention_q_embedcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_V' to 'attention_q_embedcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_V' to 'attention_q_embedcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_V' to 'attention_q_embedcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_cVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_cWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_cXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_cYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_cZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_c0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_c1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_c2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_c3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_c4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_c5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_c6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_c7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_c8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_c9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_daE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wedbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oudcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdMK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdNK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdOK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdPK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdQK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdRK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdSL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdTL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdUL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdVL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdWL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdXL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizdYM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdZM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizd0M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizd1M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizd2M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizd3M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizd4N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizd5N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizd6N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizd7N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizd8N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizd9N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizeaO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizebO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizecO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizedO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizeeO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizefO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_egO' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_egO': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 8.9 seconds; current allocated memory: 798.710 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 3.87 seconds; current allocated memory: 808.889 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_egO_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh7jG_rom' using block ROMs.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
ERROR: [HLS 200-70] Compilation errors found: In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
./model.h:55:7: error: unknown type name 'uint8_t'; did you mean 'bit8_t'?
const uint8_t P_ID = CACHE_SIZE_INIT;
      ^~~~~~~
      bit8_t
./typedefs.h:22:20: note: 'bit8_t' declared here
typedef ap_uint<8> bit8_t;
                   ^
In file included from attention.cpp:1:
In file included from attention.cpp:6:
In file included from ./attention.h:9:
In file included from ./model.h:9:
In file included from ./typedefs.h:9:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:367:
In file included from /opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed.h:55:
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_fixed_base.h:838:13: warning: shift count is negative [-Wshift-count-negative]
      ret.V <<= (_AP_I - _AP_W);
            ^   ~~~~~~~~~~~~~~~
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int_base.h:359:18: note: in instantiation of member function 'ap_fixed_base<40, 24, true, 5, 3, 0>::to_ap_int_base' requested here
    Base::V = op.to_ap_int_base().V;
                 ^
/opt/xilinx/Vivado/2019.2/common/technology/autopilot/ap_int.h:108:9: note: in instantiation of function template specialization 'ap_int_base<32, true>::ap_int_base<40, 24, true, 5, 3, 0>' requested here
      : Base((ap_fixed_base<_AP_W2, _AP_I2, true, _AP_Q2, _AP_O2, _AP_N2>)op) {}
        ^
./layer.h:98:40: note: in instantiation of function template specialization 'ap_int<32>::ap_int<40, 24, 5, 3, 0>' requested here
            sbit32_t quantized_value = attention_round(input[i][(j << 2) + k] * scale);
                                       ^
attention.cpp:96:3: note: in instantiation of function template specialization 'quantize_activation<1, 384>' requested here
  quantize_activation<SEQ_LEN, HS_COLS>(
  ^
attention.cpp:24:3: note: in instantiation of function template specialization 'attention<5, 1, 384, 384, 8, 48>' requested here
  attention<
  ^
1 warning and 1 error generated.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:54 ; elapsed = 00:00:57 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 2406 ; free virtual = 7791
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:54 ; elapsed = 00:00:57 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 2406 ; free virtual = 7791
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> const (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> const (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:59 ; elapsed = 00:01:03 . Memory (MB): peak = 1360.152 ; gain = 722.125 ; free physical = 2163 ; free virtual = 7589
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:00 ; elapsed = 00:01:05 . Memory (MB): peak = 1367.441 ; gain = 729.414 ; free physical = 2083 ; free virtual = 7515
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:74) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:86) to (./layer.h:121:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:06 ; elapsed = 00:01:11 . Memory (MB): peak = 1495.141 ; gain = 857.113 ; free physical = 1929 ; free virtual = 7385
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:100:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:63)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:184:63)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:15 ; elapsed = 00:01:20 . Memory (MB): peak = 1680.152 ; gain = 1042.125 ; free physical = 1691 ; free virtual = 7147
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 80.73 seconds; current allocated memory: 691.990 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.76 seconds; current allocated memory: 694.601 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 695.522 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 696.040 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 697.288 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 698.617 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.47 seconds; current allocated memory: 698.795 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 698.861 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 701.813 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.52 seconds; current allocated memory: 705.992 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.6 seconds; current allocated memory: 706.779 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.07 seconds; current allocated memory: 706.929 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 707.410 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 708.068 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 708.301 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 708.562 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 708.906 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 709.383 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 709.982 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 710.880 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 711.331 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 711.823 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 712.203 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 712.621 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 713.086 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 713.629 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 714.431 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.47 seconds; current allocated memory: 718.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 719.603 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 720.314 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 725.872 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 1.69 seconds; current allocated memory: 736.881 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 740.520 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 745.527 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 752.125 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 2.44 seconds; current allocated memory: 768.654 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 770.160 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 773.056 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 774.695 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 777.663 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 782.007 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 784.853 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 787.010 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedcGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedcIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedcJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_V' to 'attention_q_embedcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_V' to 'attention_q_embedcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_V' to 'attention_q_embedcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_V' to 'attention_q_embedcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_cVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_cWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_cXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_cYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_cZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_c0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_c1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_c2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_c3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_c4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_c5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_c6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_c7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_c8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_c9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_daE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wedbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oudcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdMK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdNK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdOK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdPK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdQK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdRK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdSL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdTL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdUL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdVL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdWL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdXL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizdYM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdZM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizd0M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizd1M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizd2M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizd3M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizd4N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizd5N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizd6N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizd7N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizd8N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizd9N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizeaO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizebO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizecO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizedO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizeeO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizefO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_egO' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_egO': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 5.29 seconds; current allocated memory: 798.644 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.74 seconds; current allocated memory: 808.838 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_egO_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:54 ; elapsed = 00:01:00 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 3498 ; free virtual = 8393
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:54 ; elapsed = 00:01:00 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 3498 ; free virtual = 8393
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> const (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> const (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:59 ; elapsed = 00:01:05 . Memory (MB): peak = 1241.332 ; gain = 603.305 ; free physical = 3391 ; free virtual = 8326
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:00 ; elapsed = 00:01:07 . Memory (MB): peak = 1392.102 ; gain = 754.074 ; free physical = 3384 ; free virtual = 8331
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:74) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:86) to (./layer.h:121:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:05 ; elapsed = 00:01:13 . Memory (MB): peak = 1520.176 ; gain = 882.148 ; free physical = 3291 ; free virtual = 8204
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:100:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:63)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:184:63)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:14 ; elapsed = 00:01:22 . Memory (MB): peak = 1705.184 ; gain = 1067.156 ; free physical = 3091 ; free virtual = 8019
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 82.46 seconds; current allocated memory: 691.992 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 694.609 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.01 seconds; current allocated memory: 695.533 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 696.044 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 697.292 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 698.618 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 698.809 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 698.878 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 701.826 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.54 seconds; current allocated memory: 706.004 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.62 seconds; current allocated memory: 706.793 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.08 seconds; current allocated memory: 706.944 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.14 seconds; current allocated memory: 707.423 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 708.090 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 708.318 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 708.579 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 708.926 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 709.401 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 709.998 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 710.899 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 711.366 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 711.860 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 712.226 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 712.629 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 713.111 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 713.656 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 714.454 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.47 seconds; current allocated memory: 718.207 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 719.627 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 720.339 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.29 seconds; current allocated memory: 725.898 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 1.76 seconds; current allocated memory: 736.890 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 740.532 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.7 seconds; current allocated memory: 745.537 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 752.134 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 2.51 seconds; current allocated memory: 768.663 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.36 seconds; current allocated memory: 770.184 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 773.049 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 774.704 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 777.674 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.74 seconds; current allocated memory: 782.015 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 784.864 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 787.019 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedcGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedcIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedcJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_V' to 'attention_q_embedcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_V' to 'attention_q_embedcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_V' to 'attention_q_embedcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_V' to 'attention_q_embedcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_cVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_cWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_cXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_cYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_cZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_c0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_c1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_c2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_c3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_c4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_c5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_c6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_c7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_c8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_c9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_daE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wedbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oudcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdMK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdNK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdOK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdPK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdQK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdRK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdSL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdTL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdUL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdVL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdWL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdXL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizdYM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdZM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizd0M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizd1M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizd2M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizd3M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizd4N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizd5N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizd6N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizd7N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizd8N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizd9N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizeaO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizebO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizecO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizedO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizeeO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizefO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_egO' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_egO': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 5.94 seconds; current allocated memory: 798.641 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.78 seconds; current allocated memory: 808.850 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_egO_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [HLS 200-10] Setting target device to 'xc7z020-clg484-1'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'attention.cpp' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:56 ; elapsed = 00:01:01 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 4660 ; free virtual = 9475
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:56 ; elapsed = 00:01:01 . Memory (MB): peak = 1168.152 ; gain = 530.125 ; free physical = 4660 ; free virtual = 9475
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'void GEMM_3D_float<8, 1, 6, 8, 6, 48>(ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> const (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> const (*) [FORWARD_REFERENCE][FORWARD_REFERENCE], ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> (*) [FORWARD_REFERENCE][FORWARD_REFERENCE])': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:01 ; elapsed = 00:01:06 . Memory (MB): peak = 1242.406 ; gain = 604.379 ; free physical = 4399 ; free virtual = 9254
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>126' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>126' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>126' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
WARNING: [SYNCHK 200-23] /opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:272: variable-indexed range selection may cause suboptimal QoR.
WARNING: [SYNCHK 200-120] ./layer.h:47: multiplication is assumed not to overflow by default, otherwise, please add option '-fwrapv'.
INFO: [SYNCHK 200-10] 0 error(s), 2 warning(s).
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:03 ; elapsed = 00:01:08 . Memory (MB): peak = 1368.516 ; gain = 730.488 ; free physical = 4320 ; free virtual = 9189
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'sqrt_fixed<42, 26>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:44:36).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:26).
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-1' (attention.cpp:18) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'Loop-2' (attention.cpp:50) in function 'dut' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' automatically.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LINEAR_FORWARD_NO_MUL_LOOP_3' (./layer.h:121) in function 'linear_forward_no_mul<1, 384, 384>' for pipelining.
INFO: [XFORM 203-502] Unrolling small iteration loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' automatically.
INFO: [XFORM 203-502] Unrolling small iteration loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' automatically.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:267) in function 'sqrt_fixed<42, 26>' completely with a factor of 13.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:295) in function 'sqrt_fixed<42, 26>' completely with a factor of 17.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:277) in function 'exp_reduce::exp<40, 24>' completely with a factor of 19.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:284) in function 'exp_reduce::exp<40, 24>' completely with a factor of 46.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (attention.cpp:18) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Loop-2' (attention.cpp:50) in function 'dut' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_1' (attention.cpp:84) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'SF_LOOP_2' (attention.cpp:178) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_2' (attention.cpp:185) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CM_LOOP_3' (attention.cpp:186) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ATTN_2D_LOOP_1' (attention.cpp:208) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RMS_NORM_LOOP_2' (attention.cpp:214) in function 'attention<5, 1, 384, 384, 8, 48>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>': changing partial unrolling into complete unrolling since the unrolling factor (=16) is no less than the loop trip count (=6).
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' completely with a factor of 6.
INFO: [HLS 200-489] Unrolling loop 'SOFTMAX_LOOP_2' (./layer.h:268) in function 'softmax<1, 8, 6>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_1' (./layer.h:255) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'CREATE_CAUSAL_MASK_LOOP_2' (./layer.h:256) in function 'create_causal_mask<1>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'GEMM_3D_FLOAT_LOOP_2' (./layer.h:237) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' completely with a factor of 1.
INFO: [XFORM 203-501] Unrolling loop 'GEMM_3D_FLOAT_LOOP_4' (./layer.h:241) in function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' partially with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_2' (./layer.h:169) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'APPLY_ROTARY_POS_EMB_LOOP_5' (./layer.h:181) in function 'apply_rotary_pos_emb<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'RESHAPE_2D_TO_3D_LOOP_1' (./layer.h:148) in function 'reshape_2D_to_3D<1, 8, 48>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_1' (./layer.h:119) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_4' (./layer.h:122) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 16.
INFO: [HLS 200-489] Unrolling loop 'LINEAR_FORWARD_NO_MUL_LOOP_5' (./layer.h:125) in function 'linear_forward_no_mul<1, 384, 384>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'INIT_2D_MEM_LOOP_1' (./layer.h:25) in function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_1' (./layer.h:83) in function 'quantize_activation<1, 384>' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'QUANTIZE_ACTIVATION_LOOP_5' (./layer.h:97) in function 'quantize_activation<1, 384>' completely with a factor of 4.
INFO: [XFORM 203-102] Partitioning array 'rotated_q.V' (./layer.h:166) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'rotated_k.V' (./layer.h:167) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'mask.V.0' automatically.
INFO: [XFORM 203-102] Partitioning array 'scales.V' (attention.cpp:94) automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj_re.V' (attention.cpp:104) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj_re.V' (attention.cpp:105) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj_re.V' (attention.cpp:106) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_proj.V' (attention.cpp:134) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_proj.V' (attention.cpp:135) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'v_proj.V' (attention.cpp:136) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'k_embed.V' (attention.cpp:144) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_weights.V' (attention.cpp:162) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V' (attention.cpp:181) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output' in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'final_scales.V' (attention.cpp:223) automatically.
INFO: [XFORM 203-102] Partitioning array 'causal_mask.V.0' (attention.cpp:181) automatically.
INFO: [XFORM 203-102] Partitioning array 'output' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'sin_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'cos_tab.V' in dimension 1 automatically.
INFO: [XFORM 203-101] Partitioning array 'k_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'o_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'v_weights'  in dimension 1 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'q_embed.V' (attention.cpp:143) in dimension 3 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'k_proj_transposed.V' (attention.cpp:158) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 2 with a cyclic factor 16.
INFO: [XFORM 203-101] Partitioning array 'quantized_hidden_states.V' (attention.cpp:93) in dimension 3 completely.
INFO: [XFORM 203-101] Partitioning array 'quantized_final_output.V' (attention.cpp:222) in dimension 3 completely.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.0.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.1.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.2.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.3.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.4.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.5.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.6.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.7.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.8.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.9.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.10.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.11.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.12.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.13.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.14.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.0' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.1' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.2' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_hidden_states.V.15.3' (attention.cpp:93) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.0' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.1' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.2' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.3' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.4' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.5' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.6' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.7' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.8' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.9' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.10' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.11' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.12' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.13' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.14' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'q_embed.V.15' (attention.cpp:143) in dimension 2 automatically.
INFO: [XFORM 203-102] Partitioning array 'attn_output_2D.V' (attention.cpp:206) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.0.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.1.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.2.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.3.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.4.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.5.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.6.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.7.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.8.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.9.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.10.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.11.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.12.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.13.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.14.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.0' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.1' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.2' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'quantized_final_output.V.15.3' (attention.cpp:222) in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'input.V' (attention.cpp:14) in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'hls::sqrt<42, 26>' into 'rms_norm<384>' (./layer.h:50) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_abs' into 'quantize_activation<1, 384>' (./layer.h:86) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_max<ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' into 'quantize_activation<1, 384>' (./layer.h:89) automatically.
INFO: [XFORM 203-602] Inlining function 'attention_round' into 'quantize_activation<1, 384>' (./layer.h:98) automatically.
INFO: [XFORM 203-602] Inlining function 'hls::exp<40, 24>' into 'softmax<1, 8, 6>' (./layer.h:275) automatically.
INFO: [XFORM 203-602] Inlining function 'create_causal_mask<1>' into 'attention<5, 1, 384, 384, 8, 48>' (attention.cpp:182) automatically.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_sqrt_apfixed.h:372:1) in function 'sqrt_fixed<42, 26>'... converting 142 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:95:74) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:98:56) to (./layer.h:100:13) in function 'quantize_activation<1, 384>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:121:86) to (./layer.h:121:79) in function 'linear_forward_no_mul<1, 384, 384>'... converting 257 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1) to (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:867:1) in function 'exp_reduce::exp<40, 24>'... converting 5 basic blocks.
INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (./layer.h:204:9) to (./layer.h:203:51) in function 'cache_update<8, 5, 48>'... converting 4 basic blocks.
INFO: [XFORM 203-11] Balancing expressions in function 'linear_forward_no_mul<1, 384, 384>' (./layer.h:112)...64 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'exp_reduce::exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)...19 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:08 ; elapsed = 00:01:14 . Memory (MB): peak = 1504.176 ; gain = 866.148 ; free physical = 4167 ; free virtual = 9058
INFO: [XFORM 203-541] Flattening a loop nest 'LINEAR_FORWARD_NO_MUL_LOOP_2' (./layer.h:120:68) in function 'linear_forward_no_mul<1, 384, 384>'.
WARNING: [XFORM 203-631] Renaming function 'transpose_last_two_dims<6, 8, 48>' to 'transpose_last_two_d' (./layer.h:215:62)
WARNING: [XFORM 203-631] Renaming function 'reshape_2D_to_3D<1, 8, 48>' to 'reshape_2D_to_3D' (./layer.h:148:55)
WARNING: [XFORM 203-631] Renaming function 'quantize_activation<1, 384>' to 'quantize_activation' (./layer.h:33:67)
WARNING: [XFORM 203-631] Renaming function 'linear_forward_no_mul<1, 384, 384>' to 'linear_forward_no_mu' (./layer.h:119:49)
WARNING: [XFORM 203-631] Renaming function 'init_2d_mem<1, 384, ap_fixed<40, 24, (ap_q_mode)5, (ap_o_mode)3, 0> >' to 'init_2d_mem' (./layer.h:25:48)
WARNING: [XFORM 203-631] Renaming function 'exp_reduce::exp<40, 24>' to 'exp<40, 24>' (/opt/xilinx/Vivado/2019.2/common/technology/autopilot/hls_exp_apfixed.h:41:1)
WARNING: [XFORM 203-631] Renaming function 'cache_update<8, 5, 48>' to 'cache_update' (./layer.h:201:51)
WARNING: [XFORM 203-631] Renaming function 'attention<5, 1, 384, 384, 8, 48>' to 'attention' (attention.cpp:85:53)
WARNING: [XFORM 203-631] Renaming function 'apply_rotary_pos_emb<1, 8, 48>' to 'apply_rotary_pos_emb' (./layer.h:166:59)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 6, 8, 6, 48>' to 'GEMM_3D_float' (./layer.h:236:53)
WARNING: [XFORM 203-631] Renaming function 'GEMM_3D_float<8, 1, 48, 8, 48, 6>' to 'GEMM_3D_float.1' (./layer.h:236:55)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:218:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:275:26)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:279:9)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (./layer.h:52:17)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:150:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_states[0][0][0].V' (./layer.h:100:13)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:134:24)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:130:13)
INFO: [HLS 200-472] Inferring partial write operation for 'mem[0].V' (./layer.h:26:7)
INFO: [HLS 200-472] Inferring partial write operation for 'input[0].V' (attention.cpp:20:35)
INFO: [HLS 200-472] Inferring partial write operation for 'cache_out.V' (./layer.h:204:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_weights[0].V' (attention.cpp:179:9)
INFO: [HLS 200-472] Inferring partial write operation for 'attn_output_2D[0].V' (attention.cpp:210:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:171:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:172:30)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_q[0].V' (./layer.h:173:9)
INFO: [HLS 200-472] Inferring partial write operation for 'rotated_k[0].V' (./layer.h:174:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output_k[0].V' (./layer.h:186:63)
INFO: [HLS 200-472] Inferring partial write operation for 'output_q[0][0].V' (./layer.h:184:63)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:239:9)
INFO: [HLS 200-472] Inferring partial write operation for 'output[0].V' (./layer.h:241:30)
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:17 ; elapsed = 00:01:23 . Memory (MB): peak = 1689.184 ; gain = 1051.156 ; free physical = 3953 ; free virtual = 8851
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'dut' ...
WARNING: [SYN 201-103] Legalizing function name 'sqrt_fixed<42, 26>' to 'sqrt_fixed_42_26_s'.
WARNING: [SYN 201-103] Legalizing function name 'rms_norm<384>' to 'rms_norm_384_s'.
WARNING: [SYN 201-103] Legalizing function name 'GEMM_3D_float.1' to 'GEMM_3D_float_1'.
WARNING: [SYN 201-103] Legalizing function name 'exp<40, 24>' to 'exp_40_24_s'.
WARNING: [SYN 201-103] Legalizing function name 'softmax<1, 8, 6>' to 'softmax_1_8_6_s'.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'sqrt_fixed<42, 26>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 18.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 83.68 seconds; current allocated memory: 691.990 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 694.613 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 695.543 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 696.073 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 697.336 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.43 seconds; current allocated memory: 698.666 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.44 seconds; current allocated memory: 698.828 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.05 seconds; current allocated memory: 698.895 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
WARNING: [SCHED 204-68] The II Violation in module 'linear_forward_no_mu' (Loop: LINEAR_FORWARD_NO_MUL_LOOP_2_LINEAR_FORWARD_NO_MUL_LOOP_3): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('output_0_V_addr_write_ln130', ./layer.h:130) of variable 'add_ln703_89', ./layer.h:130 on array 'output_0_V' and 'load' operation ('output_0_V_load', ./layer.h:130) on array 'output_0_V'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 82.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 701.861 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.45 seconds; current allocated memory: 706.038 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.58 seconds; current allocated memory: 706.780 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 706.930 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 707.412 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 708.075 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 708.301 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 708.562 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 708.905 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 709.381 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 709.999 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 710.902 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining function 'exp<40, 24>'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 711.351 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 711.845 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 712.209 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 712.614 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 713.093 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.21 seconds; current allocated memory: 713.634 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 714.438 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.5 seconds; current allocated memory: 718.159 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.24 seconds; current allocated memory: 719.594 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.53 seconds; current allocated memory: 720.334 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'sqrt_fixed_42_26_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'sqrt_fixed_42_26_s'.
INFO: [HLS 200-111]  Elapsed time: 1.35 seconds; current allocated memory: 725.866 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'rms_norm_384_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_mul_40s_42ns_81_2_1' to 'dut_mul_40s_42ns_bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_udiv_33s_29ns_33_37_seq_1' to 'dut_udiv_33s_29nscud' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_72s_40s_72_5_1' to 'dut_mul_72s_40s_7dEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_40s_42ns_bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_72s_40s_7dEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_33s_29nscud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'rms_norm_384_s'.
INFO: [HLS 200-111]  Elapsed time: 1.65 seconds; current allocated memory: 736.889 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'quantize_activation' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_udiv_40ns_40ns_40_44_seq_1' to 'dut_udiv_40ns_40neOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_udiv_40ns_40neOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'quantize_activation'.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 740.546 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'init_2d_mem' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'init_2d_mem'.
INFO: [HLS 200-111]  Elapsed time: 0.72 seconds; current allocated memory: 745.534 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'linear_forward_no_mu' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_72ns_61s_40_76_1' to 'dut_sdiv_72ns_61sfYi' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_72ns_61sfYi': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_forward_no_mu'.
INFO: [HLS 200-111]  Elapsed time: 0.31 seconds; current allocated memory: 752.151 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'reshape_2D_to_3D' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'reshape_2D_to_3D'.
INFO: [HLS 200-111]  Elapsed time: 2.47 seconds; current allocated memory: 768.661 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'apply_rotary_pos_emb' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_cos_tab_V_5' to 'apply_rotary_pos_g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_sin_tab_V_5' to 'apply_rotary_pos_hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_q_0_V' to 'apply_rotary_pos_ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'apply_rotary_pos_emb_rotated_k_0_V' to 'apply_rotary_pos_jbC' due to the length limit 20
INFO: [RTGEN 206-100] Finished creating RTL model for 'apply_rotary_pos_emb'.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 770.170 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'cache_update' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'cache_update'.
INFO: [HLS 200-111]  Elapsed time: 0.54 seconds; current allocated memory: 773.063 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'transpose_last_two_d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'transpose_last_two_d'.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 774.718 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float_1'.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 777.690 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'exp_40_24_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_3_table_V' to 'exp_40_24_s_f_x_mkbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_f_x_msb_2_table_V' to 'exp_40_24_s_f_x_mlbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'exp_40_24_s_exp_x_msb_1_table_V' to 'exp_40_24_s_exp_xmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_47ns_97_2_1' to 'dut_mul_50ns_47nsncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_50ns_50ns_100_2_1' to 'dut_mul_50ns_50nsocq' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_47nsncg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'dut_mul_50ns_50nsocq': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'exp_40_24_s'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 782.031 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'softmax_1_8_6_s' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'dut_sdiv_56ns_40s_40_60_seq_1' to 'dut_sdiv_56ns_40spcA' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_sdiv_56ns_40spcA': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_1_8_6_s'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 784.878 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'GEMM_3D_float' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'GEMM_3D_float'.
INFO: [HLS 200-111]  Elapsed time: 0.55 seconds; current allocated memory: 787.034 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'attention' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_in_V' to 'attention_ln_weigqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_0' to 'attention_q_weighrcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_1' to 'attention_q_weighsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_2' to 'attention_q_weightde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_3' to 'attention_q_weighudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_4' to 'attention_q_weighvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_5' to 'attention_q_weighwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_6' to 'attention_q_weighxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_7' to 'attention_q_weighyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_8' to 'attention_q_weighzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_9' to 'attention_q_weighAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_10' to 'attention_q_weighBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_11' to 'attention_q_weighCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_12' to 'attention_q_weighDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_13' to 'attention_q_weighEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_14' to 'attention_q_weighFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_weights_15' to 'attention_q_weighGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_0' to 'attention_k_weighHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_1' to 'attention_k_weighIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_2' to 'attention_k_weighJfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_3' to 'attention_k_weighKfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_4' to 'attention_k_weighLf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_5' to 'attention_k_weighMgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_6' to 'attention_k_weighNgs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_7' to 'attention_k_weighOgC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_8' to 'attention_k_weighPgM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_9' to 'attention_k_weighQgW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_10' to 'attention_k_weighRg6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_11' to 'attention_k_weighShg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_12' to 'attention_k_weighThq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_13' to 'attention_k_weighUhA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_14' to 'attention_k_weighVhK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_weights_15' to 'attention_k_weighWhU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_0' to 'attention_v_weighXh4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_1' to 'attention_v_weighYie' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_2' to 'attention_v_weighZio' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_3' to 'attention_v_weigh0iy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_4' to 'attention_v_weigh1iI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_5' to 'attention_v_weigh2iS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_6' to 'attention_v_weigh3i2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_7' to 'attention_v_weigh4jc' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_8' to 'attention_v_weigh5jm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_9' to 'attention_v_weigh6jw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_10' to 'attention_v_weigh7jG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_11' to 'attention_v_weigh8jQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_12' to 'attention_v_weigh9j0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_13' to 'attention_v_weighbak' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_14' to 'attention_v_weighbbk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_weights_15' to 'attention_v_weighbck' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_ln_weight_V' to 'attention_ln_weigbdk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_0' to 'attention_o_weighbek' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_1' to 'attention_o_weighbfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_2' to 'attention_o_weighbgk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_3' to 'attention_o_weighbhl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_4' to 'attention_o_weighbil' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_5' to 'attention_o_weighbjl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_6' to 'attention_o_weighbkl' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_7' to 'attention_o_weighbll' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_8' to 'attention_o_weighbml' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_9' to 'attention_o_weighbnm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_10' to 'attention_o_weighbom' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_11' to 'attention_o_weighbpm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_12' to 'attention_o_weighbqm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_13' to 'attention_o_weighbrm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_14' to 'attention_o_weighbsm' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_o_weights_15' to 'attention_o_weighbtn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta' to 'attention_quantizbun' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_1' to 'attention_quantizbvn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_2' to 'attention_quantizbwn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_3' to 'attention_quantizbxn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_4' to 'attention_quantizbyn' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_5' to 'attention_quantizbzo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_6' to 'attention_quantizbAo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_7' to 'attention_quantizbBo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_8' to 'attention_quantizbCo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_9' to 'attention_quantizbDo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_10' to 'attention_quantizbEo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_11' to 'attention_quantizbFp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_12' to 'attention_quantizbGp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_13' to 'attention_quantizbHp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_14' to 'attention_quantizbIp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_15' to 'attention_quantizbJp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_16' to 'attention_quantizbKp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_17' to 'attention_quantizbLp' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_18' to 'attention_quantizbMq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_19' to 'attention_quantizbNq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_20' to 'attention_quantizbOq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_21' to 'attention_quantizbPq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_22' to 'attention_quantizbQq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_23' to 'attention_quantizbRq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_24' to 'attention_quantizbSr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_25' to 'attention_quantizbTr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_26' to 'attention_quantizbUr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_27' to 'attention_quantizbVr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_28' to 'attention_quantizbWr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_29' to 'attention_quantizbXr' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_30' to 'attention_quantizbYs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_31' to 'attention_quantizbZs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_32' to 'attention_quantizb0s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_33' to 'attention_quantizb1s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_34' to 'attention_quantizb2s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_35' to 'attention_quantizb3s' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_36' to 'attention_quantizb4t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_37' to 'attention_quantizb5t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_38' to 'attention_quantizb6t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_39' to 'attention_quantizb7t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_40' to 'attention_quantizb8t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_41' to 'attention_quantizb9t' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_42' to 'attention_quantizcau' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_43' to 'attention_quantizcbu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_44' to 'attention_quantizccu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_45' to 'attention_quantizcdu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_46' to 'attention_quantizceu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_47' to 'attention_quantizcfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_48' to 'attention_quantizcgu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_49' to 'attention_quantizchv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_50' to 'attention_quantizciv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_51' to 'attention_quantizcjv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_52' to 'attention_quantizckv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_53' to 'attention_quantizclv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_54' to 'attention_quantizcmv' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_55' to 'attention_quantizcnw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_56' to 'attention_quantizcow' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_57' to 'attention_quantizcpw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_58' to 'attention_quantizcqw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_59' to 'attention_quantizcrw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_60' to 'attention_quantizcsw' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_61' to 'attention_quantizctx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_62' to 'attention_quantizcux' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_hidden_sta_63' to 'attention_quantizcvx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_re_0_V' to 'attention_q_proj_cwx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_re_0_V' to 'attention_k_proj_cxx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_re_0_V' to 'attention_v_proj_cyx' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_proj_0_V' to 'attention_q_proj_czy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_0_V' to 'attention_k_proj_cAy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_proj_0_V' to 'attention_v_proj_cBy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_0_0_V' to 'attention_q_embedcCy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_1_0_V' to 'attention_q_embedcDy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_2_0_V' to 'attention_q_embedcEy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_3_0_V' to 'attention_q_embedcFz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_4_0_V' to 'attention_q_embedcGz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_5_0_V' to 'attention_q_embedcHz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_6_0_V' to 'attention_q_embedcIz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_7_0_V' to 'attention_q_embedcJz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_8_0_V' to 'attention_q_embedcKz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_9_0_V' to 'attention_q_embedcLz' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_10_0_V' to 'attention_q_embedcMA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_11_0_V' to 'attention_q_embedcNA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_12_0_V' to 'attention_q_embedcOA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_13_0_V' to 'attention_q_embedcPA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_14_0_V' to 'attention_q_embedcQA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_q_embed_15_0_V' to 'attention_q_embedcRA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_embed_0_V' to 'attention_k_embedcSB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_cache_upd_V' to 'attention_k_cachecTB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_v_cache_upd_V' to 'attention_v_cachecUB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_0' to 'attention_k_proj_cVB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_1' to 'attention_k_proj_cWB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_2' to 'attention_k_proj_cXB' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_3' to 'attention_k_proj_cYC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_4' to 'attention_k_proj_cZC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_5' to 'attention_k_proj_c0C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_6' to 'attention_k_proj_c1C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_7' to 'attention_k_proj_c2C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_8' to 'attention_k_proj_c3C' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_9' to 'attention_k_proj_c4D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_10' to 'attention_k_proj_c5D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_11' to 'attention_k_proj_c6D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_12' to 'attention_k_proj_c7D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_13' to 'attention_k_proj_c8D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_14' to 'attention_k_proj_c9D' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_k_proj_transposed_15' to 'attention_k_proj_daE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_weights_0_V' to 'attention_attn_wedbE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_0' to 'attention_attn_oudcE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_attn_output_2D_0_V' to 'attention_attn_ouddE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp' to 'attention_quantizdeE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_1' to 'attention_quantizdfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_2' to 'attention_quantizdgE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_3' to 'attention_quantizdhF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_4' to 'attention_quantizdiF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_5' to 'attention_quantizdjF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_6' to 'attention_quantizdkF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_7' to 'attention_quantizdlF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_8' to 'attention_quantizdmF' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_9' to 'attention_quantizdnG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_10' to 'attention_quantizdoG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_11' to 'attention_quantizdpG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_12' to 'attention_quantizdqG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_13' to 'attention_quantizdrG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_14' to 'attention_quantizdsG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_15' to 'attention_quantizdtH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_16' to 'attention_quantizduH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_17' to 'attention_quantizdvH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_18' to 'attention_quantizdwH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_19' to 'attention_quantizdxH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_20' to 'attention_quantizdyH' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_21' to 'attention_quantizdzI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_22' to 'attention_quantizdAI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_23' to 'attention_quantizdBI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_24' to 'attention_quantizdCI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_25' to 'attention_quantizdDI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_26' to 'attention_quantizdEI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_27' to 'attention_quantizdFJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_28' to 'attention_quantizdGJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_29' to 'attention_quantizdHJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_30' to 'attention_quantizdIJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_31' to 'attention_quantizdJJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_32' to 'attention_quantizdKJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_33' to 'attention_quantizdLJ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_34' to 'attention_quantizdMK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_35' to 'attention_quantizdNK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_36' to 'attention_quantizdOK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_37' to 'attention_quantizdPK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_38' to 'attention_quantizdQK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_39' to 'attention_quantizdRK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_40' to 'attention_quantizdSL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_41' to 'attention_quantizdTL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_42' to 'attention_quantizdUL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_43' to 'attention_quantizdVL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_44' to 'attention_quantizdWL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_45' to 'attention_quantizdXL' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_46' to 'attention_quantizdYM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_47' to 'attention_quantizdZM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_48' to 'attention_quantizd0M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_49' to 'attention_quantizd1M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_50' to 'attention_quantizd2M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_51' to 'attention_quantizd3M' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_52' to 'attention_quantizd4N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_53' to 'attention_quantizd5N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_54' to 'attention_quantizd6N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_55' to 'attention_quantizd7N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_56' to 'attention_quantizd8N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_57' to 'attention_quantizd9N' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_58' to 'attention_quantizeaO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_59' to 'attention_quantizebO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_60' to 'attention_quantizecO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_61' to 'attention_quantizedO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_62' to 'attention_quantizeeO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'attention_quantized_final_outp_63' to 'attention_quantizefO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'dut_mul_58ns_56s_113_3_1' to 'dut_mul_58ns_56s_egO' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'dut_mul_58ns_56s_egO': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'attention'.
INFO: [HLS 200-111]  Elapsed time: 5.95 seconds; current allocated memory: 798.671 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'dut' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_in_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on port 'dut/strm_out_V_V' to 'ap_fifo'.
INFO: [RTGEN 206-500] Setting interface mode on function 'dut' to 'ap_ctrl_hs'.
INFO: [RTGEN 206-100] Finished creating RTL model for 'dut'.
INFO: [HLS 200-111]  Elapsed time: 2.73 seconds; current allocated memory: 808.849 MB.
INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were NOT satisfied.
INFO: [HLS 200-789] **** Estimated Fmax: 114.29 MHz
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_40s_42ns_bkb_MulnS_0'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_33s_29nscud_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_72s_40s_7dEe_MulnS_1'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_udiv_40ns_40neOg_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_72ns_61sfYi_div'
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_g8j_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'apply_rotary_pos_hbi_rom' using auto ROMs.
INFO: [RTMG 210-278] Implementing memory 'apply_rotary_pos_ibs_ram (RAM)' using block RAMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_47nsncg_MulnS_2'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_50ns_50nsocq_MulnS_3'
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mkbM_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_f_x_mlbW_rom' using auto ROMs.
INFO: [RTMG 210-279] Implementing memory 'exp_40_24_s_exp_xmb6_rom' using auto ROMs.
INFO: [RTMG 210-282] Generating pipelined core: 'dut_sdiv_56ns_40spcA_div'
INFO: [RTMG 210-282] Generating pipelined core: 'dut_mul_58ns_56s_egO_MulnS_4'
WARNING: [RTMG 210-274] Memory 'attention_ln_weigqcK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_ln_weigqcK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighrcU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighrcU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighsc4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighsc4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weightde' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weightde_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighudo' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighudo_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighvdy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighvdy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighwdI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighwdI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighxdS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighxdS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighyd2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighyd2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighzec' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighzec_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighAem' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighAem_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighBew' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighBew_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighCeG' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighCeG_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighDeQ' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighDeQ_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighEe0' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighEe0_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighFfa' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighFfa_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_q_weighGfk' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_q_weighGfk_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighHfu' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighHfu_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighIfE' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighIfE_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighJfO' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighJfO_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighKfY' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighKfY_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighLf8' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighLf8_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighMgi' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighMgi_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighNgs' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighNgs_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighOgC' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighOgC_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighPgM' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighPgM_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighQgW' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighQgW_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighRg6' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighRg6_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighShg' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighShg_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighThq' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighThq_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighUhA' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighUhA_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighVhK' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighVhK_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_k_weighWhU' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_k_weighWhU_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighXh4' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighXh4_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighYie' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighYie_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weighZio' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weighZio_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh0iy' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh0iy_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh1iI' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh1iI_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh2iS' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh2iS_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh3i2' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh3i2_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh4jc' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh4jc_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh5jm' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh5jm_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh6jw' is read-only, switch it to a ROM.
INFO: [RTMG 210-279] Implementing memory 'attention_v_weigh6jw_rom' using block ROMs.
WARNING: [RTMG 210-274] Memory 'attention_v_weigh7jG' is read-only, switch it to a ROM.
